[
    {
        "id": "when-it-comes-to-large-projects-think-architecture-first",
        "title": "When it Comes to Large Projects, Think Architecture First",
        "authorName": "Oleg Sadykhov",
        "authorTitle": "Principal",
        "authorImageUrl": "david-mitzel.png",
        "authorFullImageUrl": "david-mitzel-full.png",
        "shortDescription": "Large information technology programs are often multiyear initiatives with large teams and large budgets, but this approach does not guarantee success.",
        "industry": "insurance",
        "publishDate": "01/25/2017",
        "readTimeInMinutes": 10,
        "publishName": "Insurance Canada",
        "publishUrl": "https://archive.insurance-canada.ca/ebusiness/canada/2017/Xby2-Project-Architecture-1701.php",
        "content": "<p>Large information technology programs are often multi-year initiatives with large teams \u2013 more than 20 to 30 people, often much larger \u2013 and budgets in eight figures. Examples include an implementation of a core system such as policy administration, claims or billing, or a large data-analytics project. The problem with large initiatives is that delivering them in a reasonable amount of time requires large teams, and large teams present large challenges such as:<\/p>\r\n\r\n<p><ul><li>Communication: Smaller groups can communicate more efficiently and directly, but large teams require more communication and documentation that increase project overhead.<\/li>\r\n<li>Inefficient use of resources: Team members either wait for one another to accomplish tasks they depend on or inadvertently break each other's functionality, which introduces rework.<\/li>\r\n<li>Uneven skills: More-capable people can take on larger, more complex tasks, but it's very difficult to assemble a team of all-stars.<\/li>\r\n<li>Ramp-up and knowledge transfer: Building the initial team, addressing turnover and other team configuration changes create the need for new people to quickly get up to speed.<\/li><\/ul><\/p>\r\n\r\n<p>On large projects all of this becomes even more challenging due to the size and complexity of the solutions. Because issues with large teams are well known, many modern software development methodologies (especially agile) stipulate that teams should be small. For example, in Scrum, an agile methodology, teams are typically cross-functional groups of seven or so. In an ideal world, large initiatives would be broken down to create a set of small teams that works almost independently and efficiently, and ultimately assembles their work products into the overall solution. Of course, most everybody tries to break large teams into sub-teams (also called work streams, tracks and the like), but very rarely is it done well, where a variety of challenges, including communication and inefficient use of resources, are truly addressed.<\/p>\r\n\r\n<p>This is where architecture comes in \u2013 and it's not talking about the selection of Java versus .NET, or what integration technology to employ, or what types of servers and networks to use - though these topics are all important. Instead, it's about architecture as decomposition: how big systems must be broken down into smaller pieces that will lend themselves well to independent work by teams. These pieces must be autonomous and loosely coupled. The task is not easy, but it's important since without a good breakdown of systems into parts (variously called subsystems, modules, services or bounded contexts; subsystems for purposes of this article) there will be no good breakdown of large teams into efficient and independent sub-teams. It's natural for sub-teams to form around subsystems. The stakes are even higher than just the team breakdown. If the system is not well-architected, it will result in a complex and inflexible solution. Cost overruns and major failures are inevitable.<\/p>\r\n\r\n<p>Every project team that has dealt with big system implementations has hit the \"wall.\" Suddenly, development tasks that used to take a week start to take a month and it's not clear why. This is a direct result of increasing complexity and insufficient architecture. If the parts of the overall solution overlap, then the teams responsible for these components will end up performing similar tasks. This leads to duplication of effort, inconsistent solutions and similar defects that show up in various parts of the system that seem elusive and hard to eliminate. Project teams will under perform if the architecture is poor. Inefficient communication paths between teams will start dragging the initiative down. Teams will be mired in coordination and dependency nightmares and spend their valuable time adjusting to the work performed by others.<\/p>\r\n\r\n<h1>Creating Good Architecture<\/h1>\r\n\r\n<p>Let's take a simplified example to illustrate the challenges. Assume that the project is the implementation of a policy administration system where insurance policies are stored and maintained. Further assume that customer information is also stored and updated by the same system. In order to break our system down into subsystems, a logical place to start might be to separate the functionality related to policies from the functionality related to customers. In this way we've created a policy subsystem that will deal with policy information, and a customer subsystem that will deal with customers. This is an example of a good architecture that has subsystems that are autonomous and loosely coupled. This creates the ability to make and release changes to these subsystems independently of one another. In this example, policy and customer subsystems should now be able to grow and evolve independently of each other.<\/p>\r\n\r\n<p>A typical system consists of user interface components, business logic components and data. When breaking the system down into subsystems, consideration is given to each of the layers listed. In order to achieve the independence of the subsystems, all of the layers must be dealt with - UI, business and data, including their corresponding business components. Separation of the business components is probably the easiest task (though still not easy) and as such it's the only thing that is typically attempted. Take for example the many implementations of Service Oriented Architecture over the past several years, where there is an incorrect belief that good architecture results from exposing business components such as Web services, which implies loose coupling and autonomy.<\/p>\r\n\r\n<p>But if both the customer and the policy subsystems continue to share the same underlying database, then any work done on customer functionality subsystem can inadvertently break the policy subsystem. In this configuration, when one subsystem is released, the others must be tested as well. And when the database is down, then both subsystems are down. So where is the autonomy? The same can be said of the UI. It's natural for the same screen to present information about both policies and customers, but the way such screens are often built further entangles the two subsystems. The challenge of architecture is to keep the criteria of autonomy and the loose coupling of subsystems very clearly in mind, and tackle the difficult issues such as data separation and the disentanglement of user interfaces to accomplish it.<\/p>\r\n\r\n<h1>Data Replication<\/h1>\r\n\r\n<p>So what's the problem with breaking the data down and letting the subsystems own their data in order to avoid one large database that supports it all? The problem is that the policy subsystem legitimately needs customer information, and the customer subsystem may also need to know information about the policies the customer owns. Clearly, the data needs to be shared, and there are a couple of choices for accomplishing that:<\/p>\r\n\r\n<p><ul><li><b>Store the data in one place and provide access to it.<\/b> In this scenario the customer data is truly owned and only accessible by the customer subsystem. To share data, the customer subsystem can either expose the data via Web services, or provide UI screens or widgets for others to use in order to access the data.<\/li>\r\n<li><b>Allow the partial replication of the data.<\/b> In this scenario, the replication of some of the customer data elements gets created so the policy subsystem can store it in its own database.<\/li><\/ul><\/p>\r\n\r\n<p>The idea of storing data in one place initially looks advantageous, but after a deeper look, it presents several challenges. In many data intensive situations, centralized data access presents a performance challenge. Many off-the-shelf packages expect the data they need to be stored locally in the application database (this is called a replication scenario). Some of the popular enterprise subsystems will be difficult to scale, since everyone will need the same data. They also will be hard to change without affecting everyone. Additionally, downtime of one subsystem will lead to the downtime of others that depend on it. The bottom line is that data replication is an important tool to achieve the goals of autonomy and loose coupling of subsystems.<\/p>\r\n\r\n<p>There is no hard-and-fast rule to say when data should be centralized and when replication makes sense. To decide, think about the amount of data in play, how much data processing is done by the subsystem that owns the data, how impactful the downtime of one subsystem will be on another, and so on. There is no hard-and-fast rule to say when data should be centralized and when replication makes sense. Of course, the moment one starts down the path of data replication is the moment one needs a good framework in order to implement data ownership and the corresponding rules required for the subsystems. One such framework is master data management, which is a tried-and-true way to track owners of data and the rules that should be followed.<\/p>\r\n\r\n<h1>CQRS Aids Architecture<\/h1>\r\n\r\n<p>Another relatively recent framework that helps one think about separation between subsystems, data ownership, and much more is called the Command and Query Responsibility Separation. The idea is fairly simple: It advocates separation of data that supports modifications of the system from data that supports inquiries. According to CQRS, systems should have two parts - one for data modifications and the other for data inquiries - each with their own databases. Even without fully following CQRS, the concepts introduced by CQRS are helpful when working on and discussing architecture concepts such as:<\/p>\r\n\r\n<p><ul><li><b>Commands:<\/b> Requests to modify data (that is, to create a new customer). Their names should be in the present tense and sound like commands. Their processing involves data validation, execution of business rules and changing the data.<\/li>\r\n\r\n<li><b>Events:<\/b> Notifications about data changes (such as, new customer created). Names of events should use past tense to indicate that they're just a notification of a change that's already occurred. Events notify interested parties that a data change has been approved and recorded. They're typically used to trigger other processing or to update decentralized data replicas.<\/li>\r\n\r\n<li><b>Queries:<\/b> Read-only requests to access the for specific data record details. Here's how the CQRS approach can be applied to our example of policy and customer subsystems. Policy and customer subsystems will \"own\" their data and be responsible for processing commands to change their data. Upon successful changes of data, they will publish events notifying the other subsystems. To accomplish this, a new subsystem is created \u2013 the operational data store (ODS) - that will combine data from policy and customer components in a way that is optimized for inquiries. The ODS will change its data in response to events published by the policy and customer subsystems. Therefore, to build a solution that involves interactions with the policy and customer subsystems, the ODS will be used to fulfill inquiries, and when data changes are made commands will be sent to the respective subsystems (policy and customer) to process them appropriately.<\/li><\/ul><\/p>\r\n\r\n<p>In summary, software architects should consider newer approaches such as CQRS when working through the challenges of the decomposition of their systems. The goal is to achieve autonomy and loose coupling of any subsystems. If successful, it can pave the way for small, independent, and efficient teams that have a much better chance to deliver large and complex enterprise initiatives successfully.<\/p>",
		"tags": [
            "insurance",
            "architecture"
        ],
        "nextMindShareId": "the-key-to-small-project-management-keep-it-simple"
    }, 
    {
        "id": "the-key-to-small-project-management-keep-it-simple",
        "title": "The Key to (Small) Project Management: Keep it Simple",
        "authorName": "Jeff Sallans",
        "authorTitle": "Senior Consultant",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Can a methodology be slimmed down to something that fits the needs of a smaller project, but still provide enough structure to effectively manage it?",
        "industry": "insurance",
        "publishDate": "02/02/2017",
        "readTimeInMinutes": 7,
        "publishName": "Insurance Innovation Reporter",
        "publishUrl": "http://iireporter.com/the-key-to-small-project-management-keep-it-simple/",
        "content": "<q>There are ways to apply some of the best practices from larger project efforts to smaller projects, while not burdening smaller projects with unnecessary bureaucratic overhead.<\/q>\r\n\r\n<p>Today one sees many articles about standard practices and patterns for managing projects. But what if it\u2019s a small project or an ad hoc initiative that wasn\u2019t necessarily planned for? Can a standard methodology be slimmed down to something that fits the needs of a smaller project, but still provides enough structure to effectively manage it?<\/p>\r\n\r\n<p>When working on any sizable project, decisions are typically made for development and testing in the interest of resource and time trade-offs. On such projects, the questions are something like: \u201CShould we delay this project release to add this feature?\u201D or \u201CWe are pretty confident in this code, so should we decrease testing time in this area?\u201D  When it comes to smaller projects, why shouldn\u2019t these questions be applied to methodology and practices? The key is to identify what practices provide the most benefit, and more importantly, what practice can be simplified to use on a smaller project.<\/p>\r\n\r\n<p>A comparison of two larger and smaller projects might prove instructive. For this comparison, both are full-stack web development projects. One is a large insurance quoting project, and the other a small e-commerce project. The biggest difference between the two is their scale and timeline. To set the scene, here is a quick overview of the two project structures:<\/p>\r\n\r\n<p><b>Large Team Structure \u2013 Team of 18 \u2013 2-Year Timeline:<\/b><\/p>\r\n\r\n<p><ul><li>(2) Project Leads<\/li>\r\n<li>(2) Business Analysts (provides stakeholder sign-off)<\/li>\r\n<li>(4) Quality Analysts<\/li>\r\n<li>(2) Tech Leads<\/li>\r\n<li>(6-8) Developers<\/li><\/ul><\/p>\r\n\r\n<p><b>Small Team Structure \u2013 Team of nine \u2013 6-Month Timeline:<\/b><\/p>\r\n\r\n<p><ul><li>(3) Stakeholders<\/li>\r\n<br>Responsibilities include Business Analyst (provides stakeholder sign-off)<\/br>\r\n<br>Responsibilities include Quality Analyst<\/br>\r\n<li>(2) Part-time Project Leads<\/li>\r\n<br>Responsibilities include Tech Lead<\/br>\r\n<li>(2-4) Developers<\/li><\/ul><\/p> \r\n\r\n<h1>Estimated project durations:<\/h1>\r\n\r\n<p>Large project man-hours = (18 team members) x (24 months) = 432 working months<\/p>\r\n\r\n<p>Small project man-hours = (8 team members) x (6 months) = 42 working months<\/p>\r\n\r\n<p>The large project is approximately 10 times larger than the smaller project. Given this, how might the smaller team, working on the smaller project, simplify the project communication practices for effectiveness and efficiency? Here are some suggestions:<\/p>\r\n\r\n<h1>Daily Standup: General Team Communication<\/h1>\r\n\r\n<p>The purpose of a daily standup is to spread awareness about the team\u2019s work to keep everyone productive. In the larger project, the standup was between eight people and usually lasted 20 minutes. The biggest benefit of this process is coordinating between testers and developers.<\/p>\r\n\r\n<p>For the smaller project, the process was simplified by using email to report any standup details. However, this proved to be ineffective because emails couldn\u2019t duplicate the standup\u2019s quick back and forth discussion between team members. The compromise for the smaller project was to use a longer in-person meeting (four people and 30 minutes), which kept team members with multiple roles on the same page.<\/p>\r\n\r\n<h1>Design Reviews\/Code Reviews \u2013 Developer-to-Developer Communication<\/h1>\r\n\r\n<p>Design and code reviews are typically done between fellow developers. A design review occurs when a plan or structure is reviewed before it is implemented. This is important to help create well-organized code and to reduce time when making improvements in the future. A code review occurs when the code written is reviewed before testing to help catch bugs, and to help with code consistency.<\/p>\r\n\r\n<p>For both large and small projects, design reviews are very important, especially at the beginning of the project because there are more opportunities to see the benefits of more easily extending existing functionality. Code reviews can be simplified for smaller projects because less communication is needed between developers when each person writes large sections of code. In many small projects, a single developer will write all the code for a certain feature. This removes a lot of developer coordination, and the code inherently gains the same benefits of reduced bugs and code consistency, while reducing the number of code reviews.<\/p>\r\n\r\n<h1>Business Engagement \u2013 Developer-to-Business Analyst Communication<\/h1>\r\n\r\n<p>Developer and business analyst interaction is crucial to a project, big or small. For those unfamiliar with the term business analyst, it refers to someone that understands the behavior and actions that the program will be responsible for executing. A useful way to think about it is that the business analyst knows the destination, and the developer builds the road to get there. Projects often veer off course when the two fall out of sync.<\/p>\r\n\r\n<p>For larger projects, this is usually addressed by adding a person to the development team dedicated to the business analyst role. For many smaller projects, however, that is not a practical option. More often than not, most members of small project teams must wear multiple hats and serve multiple roles. To simplify things for this purpose, it\u2019s often helpful to use a Google Document or something similar to act as a proxy for someone who would normally play this role full time on a larger project.<\/p>\r\n\r\n<p>In practice, developers would add a question to the document when they encountered a question while writing the code. Every other day, a business member of the team would respond to the questions posted. If a developer\u2019s question changed or was otherwise resolved prior to a response, they would update the Google Document, thus avoiding unnecessary work for the business members of the team.<\/p>\r\n\r\n<p>This method works quite well for quick questions about schema or the legacy systems but can be lacking when discussing new features. In those cases, a conference call is probably more appropriate.<\/p>\r\n\r\n<h1>Bug Reports: Developer-to-Quality Analyst Communication<\/h1>\r\n\r\n<p>For small project teams, everything is magnified. Things have to happen faster with fewer people, so finding the right tools that support the small project team structure is critical. As with developer-to-analyst communications, Google Docs or something similar can be quite helpful in the QA process.<\/p>\r\n\r\n<p>For larger projects, JIRA or something similar is a great tool for managing the QA process, but smaller project teams don\u2019t have that kind of tool luxury. That\u2019s why Google Spreadsheet fits this small project situation better than anything bigger and bulkier.<\/p>\r\n\r\n<p>In the end, having everything on one spreadsheet speeds-up data entry and offers a more holistic view of the data\u2014for a small project team, that\u2019s worth its weight in gold.<\/p>\r\n\r\n<p>\u2026<\/p>\r\n\r\n<p>The lesson to be drawn here is that there are ways to apply some of the best practices from larger project efforts to smaller projects, while not burdening smaller projects with unnecessary bureaucratic overhead. The best practice of project communications can be adopted for this purpose, and those same principles can be applied to other project management practices.<\/p>\r\n\r\n<p>The key is looking at the current roles and communication practices of a smaller project team and asking these questions: Do the project team roles line up with the project team\u2019s communication practices? Or, is there a practice that seems unnecessary? If so, modify or eliminate it. Remember the benefit of a small project and a smaller project team is the ability to quickly change practices to suit the needs of the team. Use this to your advantage when simplifying your small project team processes to strike the balance that makes the most sense in order to meet the ultimate goal\u2014getting the project done.<\/p>",
		"tags": [
            "insurance",
            "project management"
        ],
        "nextMindShareId": "how-neglecting-non-functional-requirements-makes-systems-non-functional"
    }, 
    {
        "id": "how-neglecting-non-functional-requirements-makes-systems-non-functional",
        "title": "How Neglecting Non-Functional Requirements Makes Systems Non-Functional",
        "authorName": "Matt Flores",
        "authorTitle": "Architect",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "It's more often than not that the non-functional capabilities of a system determine its value and lifespan, and whether or not the company views it as a success.",
        "industry": "insurance",
        "publishDate": "03/17/2017",
        "readTimeInMinutes": 7,
        "publishName": "Insurance Innovation Reporter",
        "publishUrl": "http://iireporter.com/how-neglecting-non-functional-requirements-makes-systems-non-functional/",
        "content": "<q>Over the long term, it\u2019s more often than not that the non-functional capabilities of a system that determine its value and lifespan, and whether or not the company views it as a success, or just another sub-optimal delivery from IT.<\/q>\r\n\r\n<p>Whether partnering with a custom solutions provider, creating a home-brew application, or purchasing an out-of-the-box product, most companies will follow a standard approach for evaluating their needs for a new or updated system.  Commonly, the first step is to identify the core functional requirements that need to be satisfied. First and foremost, the solution must meet the base business and process needs. However, this is only half of the first step. These core functional requirements typically will only address how something works but not how well it works. To completely identify requirements includes considering the non-functional requirements (NFR) of the system. It can be argued that this second half of requirements is as important as the first, if not more so in some cases.<\/p>\r\n\r\n<p>Non-functional requirements differ from functional requirements in several important ways. Functional requirements describe how a system will work: what screens will users see, how the screens transition from one to the next, what data can be entered, how and when will the data be saved and retrieved, and so on. These are commonly seen through the lens of the primary users of the system, as they must have these requirements met in order to do their jobs.<\/p>\r\n\r\n<h1>Functionality and Quality<\/h1>\r\n\r\n<p>By contrast, non-functional requirements describe the quality of a system. This simple but broad description covers a number of attributes including performing well under normal <i>and<\/i> extreme usage patterns (performance and scalability), quickly adapting the application to changing or varying business rules (configurability), addressing productivity for novice and expert users (usability), and preventing unauthorized access or transfer of data (security). These sets of requirements can be seen and experienced by many stakeholders beyond just the primary end-users. Viewing it this way, it is clear that outlining and fulfilling only the functional requirements could lead to a half-baked solution.<\/p>\r\n\r\n<p>It is not uncommon or unexpected for a customer to be focused primarily on the functional requirements, as the visible and tangible assets can create the perception of a high-quality system. Often, non-functional requirements can be prioritized lower or treated as afterthoughts constrained by time, budget, and resources. This is a mistake.  Non-functional requirements have a significant impact not only on the architecture and design of a system, but also on the long-term total cost of ownership (TCO).<\/p>\r\n\r\n<p>To illustrate this, consider Amazon.com and the architecture, infrastructure, and hardware required to maintain a system serving over 150 million unique users per month.  Even when stripping the site down to the most basic features of e-commerce (search, add to cart, and checkout), having such an enormous user base demands constant up time, a highly responsive system, and an ordering process simple enough for your grandmother to send you a birthday present.  Hiccups in any of these areas could be costly.  In 2013, merely 30 minutes of down time cost Amazon nearly $2 million.  This is, of course, is an extreme example, and the engineers at Amazon have created a remarkably robust architecture to prevent disruptions, but the site would not function as well as it does without having considered the needs of the company beyond its front-facing capabilities.<\/p>\r\n\r\n<p>Given that non-functional requirements are not a new concept, one would hope that they have become so ubiquitous in software development that major applications should never experience such issues.  Unfortunately that is not the case.  In 2016, the launch of the highly anticipated mobile game Pokemon Go attracted over 50 million players in the first month after release.  The game gave players the chance to capture and power up fantasy creatures\u2014Pokemon\u2014in an augmented reality platform, an exciting experience for players familiar with the video game series. This massive userbase overwhelmed the game\u2019s servers often preventing users from logging in or reliably playing the game in the first few weeks. Additionally, features within the game, such as tracking nearby Pokemon, were slowly disabled and removed over time due to the volume of network traffic between the client and the servers. While features met their functional requirements well, it appears that non-functional requirements were not given adequate weight. Overall, this led to a poor user experience until the game stabilized.<\/p>\r\n\r\n<h1>The Bigger Picture<\/h1>\r\n\r\n<p>The importance non-functional requirements can be demonstrated to be more than stretch goals or afterthoughts by connecting their value to important business goals. Projected growth, potential features, geographic expansion, or the execution of a company\u2019s long-term strategy can be hindered by the dreaded \u201Csystem constraints\u201D born from neglecting NFRs.<\/p>\r\n\r\n<p>Given these observations, what are some considerations for specific non-functional requirements? Let\u2019s review two of the aforementioned non-functional requirements as a way to better understand see how they connect to business goals, and identify key areas of focus.<\/p>\r\n\r\n<p>A thoughtful product owner will consider how much and how quickly a system\u2019s user base will grow. Building a system only for the present or average load can limit growth and significantly limit business goals. Scalability non-functional requirements are as much a business issue as they are an IT issue. Though a solution can be quickly developed, tested, and deployed without considering how a system might function under increased load, addressing scalability concerns may be more difficult to solve on a live system than one still on the drawing board. When considering how much a system may need to scale, a product owner must consider when the system is under the most load, including what time of day and what time of year.  Perhaps most activity happens early in the morning or just after lunch. Maybe usage spikes significantly on weekends. Some industries, like retail, need to consider increased transactions leading up to major holidays. In any case, the system must be architected to handle peak loads as well as day-to-day traffic.<\/p>\r\n\r\n<p>How might systems handle spikes in demand? A straight forward tactic is to scale-up existing hardware with improved components thus increasing overall processing power. A system can also scale-out by adding additional hardware and servers to serve more concurrent users. However, a scalability bottleneck can occur in any layer of a system\u2019s architecture. Aside from the application running on its main hardware, less obvious areas to review include available network bandwidth, database responsiveness, and dependencies on external services and integrations.<\/p>\r\n\r\n<p>Aside from hardware concerns, a system must be easily usable by its core audience to be a success.  Usability as a non-functional requirement category entails too many parts to detail here. However, a key component that cannot be understated or overlooked is an understanding of what types of users will access the system. What about users who rarely access the application? Can such users stay familiar with the flow and nuances of the screens from session to session at that usage frequency? Are on-screen tips and a wizard-supported workflow helpful to guide users through a process? When accommodating novice or infrequent users, the application must have an intuitive and consistent layout to be accessible.<\/p>\r\n\r\n<p>Conversely, an expert user who works on the system daily may find this level of hand holding more annoying than helpful. For power users, the layout of screens can focus on completing as many tasks as possible in an efficient way.  This could mean that instead of a set of screens to enter one set of data, one screen with a different layout could allow for multiple entries at once with less on-screen help. Neither design here changes what data is stored from a user, thereby changing no functionally required fields, but the non-functional usability requirements must be assessed before the workflow is built to maximize the user experience.<\/p>\r\n\r\n<h1>Not Just How, But How Well<\/h1>\r\n\r\n<p>As solution providers increasingly deliver more functional, more stable, and more flexible software systems, the project leadership must consider not only how the product will work, but how well it will work.  To separate and neglect non-functional requirements\u2014or to prioritize them far below or after functional requirements\u2014is to tacitly approve building an inherently flawed and incomplete system. To meet the business\u2019 current and long-term goals, the product owners, architects, business analysts, and technical leaders must take a holistic approach to defining new features and updates.  Over the long term, it\u2019s more often than not that the non-functional capabilities of a system that determine its value and lifespan, and whether or not the company views it as a success, or just another sub-optimal delivery from IT.<\/p>",
		"tags": [
            "insurance",
            "requirements"
        ],
        "nextMindShareId": "architecting-an-agile-enterprise"
    }, 
    {
        "id": "architecting-an-agile-enterprise",
        "title": "Architecting an Agile Enterprise",
        "authorName": "Samir Ahmed",
        "authorTitle": "Principal",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "In the insurance industry the need to innovate is dire. The common thread weaving through the industry is the role of technology; it is expected to lead the charge.",
        "industry": "insurance",
        "publishDate": "04/10/2017",
        "readTimeInMinutes": 7,
        "publishName": "Insurance Technology Association",
        "publishUrl": "http://buyersguide.itapro.org/Article?area=&articleID=289&channel=Solution%20Partner%20Views",
        "content": "<p>In the insurance industry, be it life, health or P&C, the need to innovate is dire. Disruption is the word of the hour. Discussions around innovations in core systems, business processes, omnichannel experience for producers and policyholders, and digitization (e.g. e-application, e-signature, e-delivery, etc.) abound.<\/p>\r\n\r\n<p>The common thread weaving through all of these discussions is the role of technology: It is expected to lead the charge. And why shouldn't it? Technologies such as social networking, mobile computing, big data, neural networks, machine learning, and artificial intelligence, are all expected to disrupt the marketplace, and pay big dividends to those willing to make the bet. <i>Venture Scanner's Insurance Technology Market Overview and Innovation Quadrant for Q1 of 2017<\/i> shows them tracking 1,050 start-up companies in 14 categories across 54 countries with a combined $17 billion in funding.<\/p>\r\n\r\n<p>For carriers, often viewed as evolving slower than a speeding microorganism, the challenge is figuring out how to participate in the market disruption that presumably awaits their fate. According to the venerable Harvard Business Review (HBR), the answer lies in being agile. In the May 2016 edition, in an article titled \"Embracing Agile,\" the authors posit that the same agile innovation methods that have worked wonders for software development teams over the past 25 to 30 years are ripe for the picking as innovation methods of choice in a broad range of industries and business functions. In other words, agile approaches are no longer just for skunkworks projects; they are the way to innovate.<\/p>\r\n\r\n<p>With HBR's recommendation understood, the challenge for carriers is figuring out how to architect their enterprises to be (more) agile. To frame this portion of the discussion in the proper context, let's define a few key terms.<\/p>\r\n\r\n<p><ul><li>By enterprise, we mean the three-legged stool of people, process, and technology that supports any given business function, and more specifically in our current discussion, any given innovation project.<\/li>\r\n<li>Next, by agile, we are not referring to any of the specific agile software development methodologies such as Extreme Programming (XP), Scrum, Kanban, and their ilk. Rather, we're referring to the underlying values that make a given approach or methodology agile. In a manner of speaking, we're discussing not just \"doing agile,\" but also \"being agile.\"<\/li>\r\n<li>Finally, by architecture, we`re referring to the structure or structures of a system, which comprise elemental building blocks of the system, the externally visible properties of those building blocks, and the relationships among them. This is an adaptation and generalization of the definition of software architecture, as defined by Bass, Clements, and Kazman in the 2nd edition of Software Architecture in Practice, published by Addison-Wesley in 2003.<\/li><\/ul><\/p>\r\n\r\n<p>Since we`re borrowing a methods leaf from the software development community, we should address an issue many agile software developers would take with mixing in architecture. Agile software developers have an aversion to heavy and seemingly endless documentation of a system, which, in traditional methods, would be the outcome of an activity they would call \u201Cbig design up front\u201D or BDUF.<\/p>\r\n\r\n<p>Instead, they prefer an emergent design that`s the result of following the principles of \u201Ckeep[ing] it simple\u201D (KISS) and \u201Cyou aren`t going [to] need it\u201D (YAGNI), the latter a reference to attempts to build product features and capabilities that are not required currently, but are being anticipated as being required at some future point.<\/p>\r\n\r\n<p>Such disdain for architecture by agile software developers is misplaced. The reality is that in viewing architecture as defined herein, we find that every system has an architecture; it's only a matter of whether that architecture is intentional or accidental. Even the \"emergent design\" that agile software developers gravitate towards is the outcome of operating in some context that is pre-defined.<\/p>\r\n\r\n<p>To borrow an analogy from construction, when building a house, one can certainly start out by building a single story structure, but if one anticipates needing to build a second story at any point, it had better be factored into the foundation. Not doing so will require tearing down the structure and rebuilding with a stronger foundation. This is an example of a feature that will not emerge. On the other hand, adding an additional room to the first story could certainly be an emergent feature, issues of physical space notwithstanding.<\/p>\r\n\r\n<p>As to living the values of agility in order to be agile, in 2001, a group of individuals branding themselves as the Agile Alliance promulgated an Agile Manifesto as a way to codify the values of agility they adhered to. While these were in the context of software development, the premise of HBR's recommendation is that agile methods are broadly applicable, so it's worth discussing.<\/p>\r\n\r\n<p>The manifesto states, \"\u2026 we have come to value: individuals and interactions over processes and tools; working software over comprehensive documentation; customer collaboration over contract negotiation; [and] responding to change over following a plan. That is, while there is value in the items on the right, we value the items on the left more.\" Let's imagine how these values might be seen in action on a project team.<\/p>\r\n\r\n<p>In assembling a team, valuing individuals and interactions over processes and tools can manifest itself in several forms. First, team members should not be assigned to the project by managers. Instead, positions on the team should be filled much the same as any open position is filled in a company: through a process of applying for the position, interviewing candidates, and reaching a joint commitment to join the team between the project leads and the candidates. In doing so, the bar on required skills should be kept high, and not compromised due to the presence of other factors. The skills being sought should be a versatile set of skills, enabling team members to perform a variety of tasks. They should not be a narrow and specialized skill-set, constraining team members only to perform tasks within their specialized area of skill. The team should look more like a military special operations team, rather than a complete army.<\/p>\r\n\r\n<p>Second, in valuing a working product more than a specification of it, we should measure our progress regularly and often. The only measure of progress should be demonstrable features and capabilities. Descriptions and status reports will not suffice. The purpose of this is to see the progress in action directly against the desired final outcome. Measuring frequently enables the team to detect issues early, allowing the team to address them sooner rather than later.<\/p>\r\n\r\n<p>Third, valuing collaboration more than contract negotiation can have many manifestations as well. For instance, teams should be collocated, with access to spaces for face-to-face interactions and collaboration. Since most organizations have teams comprising individuals spread across different locations, this isn't always possible. In such cases, the team should invest in real-time collaboration tools such as web and video conferencing, shared editing and drafting tools, instant messaging, etc. Investing in such tools even for completely collocated teams has benefit. As basic as these might sound, many carriers have not yet made these investments.<\/p>\r\n\r\n<p>Another manifestation would be to have the innovative project be a full-time focus, at least for key team members. Having competing commitments and differing availability between team members prevents spontaneous collaboration, requiring interactions to be formalized, which slows things down at best, and prevents progress at worst. Finally, the team ought to be empowered to make decisions on the spot during the collaborative interactions. When such empowerment is lacking, decision point has to be taken back to the true power of authority, thus undermining the effectiveness of the collaborative process.<\/p>\r\n\r\n<p>Finally, in valuing responding to change more than following a plan, I'm reminded of a quote from President Eisenhower: \"In preparing for battle I have always found that plans are useless, but planning is indispensable.\" A manifestation of this is engaging in the planning process regularly, and frequently. Such checkpoints allow the team to adapt based on feedback it`s receiving. They also create an opportunity for the team to benefit from new information that might have become available. Thus, at every checkpoint, the plan going forward can be refined.<\/p>\r\n\r\n<p>The aforementioned are just a few practical examples from real-life experiences in executing agile projects that are manifestations of living the values of agility. While the practices are examples of \"doing agile\" it's important to tie them back to values of \"being agile,\" without which they end up being mindless habits whose benefit start diminishing over time. As these values spread to more areas of any organization, the enterprise will inevitably become more agile in the best possible sense.<\/p>",
		"tags": [
            "architecture",
            "agile"
        ],
        "nextMindShareId": "life-insurance-applications-a-better-way"
    },    
    {
        "id": "life-insurance-applications-a-better-way",
        "title": "Life Insurance Applications: A Better Way",
        "authorName": "Yunus Burhani",
        "authorTitle": "Senior Architect",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Life insurers are racing to become more innovative as a way to improve overall customer experience; specifically through the initial application process.",
        "industry": "insurance",
        "publishDate": "05/11/2017",
        "readTimeInMinutes": 5,
        "publishName": "Insurance-Canada",
        "publishUrl": "https://archive.insurance-canada.ca/poladmin/canada/2017/xby2-better-life-applications-1705.php",
        "content": "<p>Like the rest of the insurance industry, life insurers are racing to become more innovative as a way to improve the overall customer experience. While it's fair to say that life insurers are lagging behind the health and property and casualty verticals when it comes to innovation and customer engagement, there still are inroads being made. That said, there's room for a lot more improvement. One example of this, and one that touches a potential customer directly, is the initial application process for life insurance.<\/p>\r\n\r\n<p>Over the years life insurers have tried many approaches to enhance the buying experience for customers, but with little success. However, life insurance is still unique enough in terms of underwriting information that it requires potential customers to engage with an agent in order to find the right product for them. For better or worse, it is not as easy as buying auto insurance online \u2013 a process that is for the most part automated across the personal lines space and is widely accepted by consumers as the process for acquiring that insurance. However, a typical life insurance company sells a number of complex products, each with its own pros and cons that need to be carefully evaluated on an individual customer basis. In many respects it is more akin to investing money in financial sector products, which often requires engaging a broker, a certified financial planner, or some other financial expert.<\/p>\r\n\r\n<p>That's why the life insurance application process \u2013 and transitioning that to the underwriting process \u2013 continues to be an issue for most life insurance companies. Some of the issues that typically arise during the process are:<\/p>\r\n\r\n<p><ul><li>The application is not fully filled in by an agent.<\/li>\r\n<li>The insurer's web application is too strictly edited for required information so agents often try to avoid filling them out online and submits paper application.<\/li>\r\n<li>Not all the necessary underwriting requirements could be fulfilled because some application forms are not completely filled out.<\/li>\r\n<li>The scheduling of evidence vendors are not set at the time of the application, so it becomes difficult to reach out to a potential customer afterward.<\/li>\r\n<li>Applications are not signed electronically so insurers and consumers have to wait for wet signatures to occur.<\/li><\/ul><\/p>\r\n\r\n<p>These seem like relatively simple problems to overcome, but despite the perception they persist in the life insurance industry. So in a world growing increasingly accustomed to instant information and response gratification, life insurers run the risk of being perceived as backwards from a customer service and technology perspective, and with some justification.<\/p>\r\n\r\n<p>For example, while the quoting process for life insurance is pretty straightforward and does not require much data entry from a potential customer, once the potential customer agrees to the quote, the application process typically takes a long time and requires an extensive amount of data entry. Depending on the number of potentially insured's seeking coverage, the list of questions could easily go for an hour or two. This requires the customer(s) to take time out of their busy schedules in order to visit an agent's office, often for an extended period of time. Further, some of the questions on the application require the agent to physically see potential insured's, thus eliminating the option to have the conversation over the phone. A further complication can occur when some of the questions asked are HIPAA sensitive in nature, sometimes leading to awkward situations for potential insured's when they are required to answer such questions in front of others.<\/p>\r\n\r\n<p>Given all of that, the question is what can be done about this cumbersome application process? One possible approach would be a hybrid one that would allow the agent and the potential customer to each complete a part of the application. In this approach, an agent can initiate the quoting process over the phone, since it does not require lot of data to be entered. Once the customer agrees with the quote the agent can schedule a thirty-minute office interview with the customer to narrow down the possibility of life, health and annuity insurance products and better explain the best options for customer. During that same visit agent would initiate the application process electronically, and once they completed their portion send it electronically to the customer to complete the rest.<\/p>\r\n\r\n<p>As part of this self-serve approach, an evidence vendor scheduling system can be integrated allowing customers to schedule their exams appointments online. In the case where the insurer and\/or local regulation requires the agent to be present when the customer signs, this suggested approach still makes it easier as the agent does not have to spend the time asking all the involved parties medical questions individually. The parties could be given tablets or use their cell phone to answer those questions in a lobby just like what occurs in a doctors office today. Once the signature is executed \u2013 either in person or electronically \u2013 the executed application can be forwarded to the insurer to initiate the underwriting process.<\/p>\r\n\r\n<p>Such an approach would significantly streamline the application and underwriting processes for the life industry by providing customers a convenient way to apply for a policy without having to sit in front of an agent for a long time answering questions about their medical and financial histories. This application process also has the potential to speed up the new business and underwriting process for insurers, since they do not have to deal with incomplete applications, or applications coming in without a signature. They would also know that all the underwriting requirements have been ordered during the application process, and as a result would not have to bother with calling customers to schedule an appointment, or sending application information to evidence vendors so they can call the potential customers. Finally, since this streamlined application process does not allow agents to fill in all of the customer information (almost always erroneously), it will reduce the amount of paper applications received by insurers greatly, which will eliminate the burden on the New Business department to key in paper applications before any underwriting starts.<\/p>\r\n\r\n<p>There's an old saying that first impressions \u2013 good or bad ones \u2013 tend to stick with people. In today's rapidly changing insurance environment, life insurers should be motivated to put their best customer service foot forward by making it as easy and simple as possible for potential customers to navigate the application and underwriting process. Any life insurer who fails to do so could find themselves losing potential new customers to their competitors.<\/p>",
		"tags": [
            "application",
            "customer experience"
        ],
        "nextMindShareId": "the-bigger-they-are-the-tougher-the-talent"
    },   
    {
        "id": "the-bigger-they-are-the-tougher-the-talent",
        "title": "The Bigger They Are, The Tougher The Talent",
        "authorName": "David Packer",
        "authorTitle": "Principal",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Finding and keeping talented people for critical business technology programs and projects remains a difficult challenge across the insurance industry.",
        "industry": "insurance",
        "publishDate": "06/01/2017",
        "readTimeInMinutes": 7,
        "publishName": "HIT Leaders & News",
        "publishUrl": "https://us.hitleaders.news/the-bigger-they-are-the-tougher-the-talent/",
        "content": "<p>Finding and keeping talented people for critical business technology programs and projects remains a difficult challenge across the insurance industry. When it comes to large, transformational type efforts, the talent problem gets even more difficult.<\/p>\r\n\r\n<p>The projects can be multiyear, the risk to the organization can be high, the technology and process issues and problems can be daunting, and it can be difficult to keep talented people engaged on longer projects when they are in demand for other projects in other parts of the organization.<\/p>\r\n\r\n<p>That said, these large, transformational projects are now more common in the industry as insurers continue their efforts to modernize and innovate. Unfortunately, the effort required for finding the right talent is compromised in the name of quickly staffing these large projects. The compromised strategy often involves blindly staffing from vendor \u201Cstrategic partners,\u201D professional services firms and staff augmentation firms, often leading to increased cost and poor quality solutions.<\/p>\r\n\r\n\r\n<p>So is there a better way to go about staffing large projects with the right talent?<\/p>\r\n\r\n<p>In fact, there is a better approach, and it is based on a set of general principles that organizations often shortcut when staffing up. These principles are:<\/p>\r\n\r\n<p><ul><li><b>Quality over quantity.<\/b> The caliber of the people involved is the single most critical element to project success. Be patient and take the time needed to staff based on the quality of the people versus meeting arbitrary staffing goals that lead to filling the team quickly and compromising on quality.<\/li>\r\n\r\n<li><b>Project success trumps team member development.<\/b> The health and welfare of the project should not be compromised in the interest of giving less qualified employees \u201Csome experience doing this kind of stuff.\u201D Large, high-risk projects cannot afford to be training grounds.<\/li>\r\n\r\n<li><b>Recruiting and staffing is a continuous effort, not one and done.<\/b> Attrition, role rotation and performance issues will ensure that recruiting and staffing will be an ongoing process, so just plan for that eventuality up front. It makes for a healthier team environment anyway.<\/li>\r\n\r\n<li>Recruiting well takes time and resources; don\u2019t underestimate it.<\/b> For example, let\u2019s assume that it might take two interviews to qualify every candidate and that for every five candidates there is one candidate that is selected. If the project team needs 50 people, that equates to 500 interviews to staff a 50-person team, not to mention the effort that goes into the other essential aspects of recruiting, such as candidate screening, coordinating scheduling, interviewing and decision making. The overall project plan should account for a full-time resource to handle this process, and for several other team leads spending 20-30 percent of their allocated time on the interviewing process, at least at the outset of the project.<\/li>\r\n\r\n<li><b>Have an onboarding plan.<\/b> It is important to recognize onboarding as an essential part of the recruiting and staffing effort, and not an afterthought. Proper onboarding takes time and effort, but it goes a long way in increasing the overall effectiveness of individuals on the project and of the project team overall.<\/li><\/ul><\/p>\r\n\r\n<p>So let\u2019s break this down a bit. Just as it is with the technology and process portions of the project, recruiting for the overall project should have its plan, complete with ownership and accountability, for its success. The recruiting plan should consider the resources, time, effort and expense needed to staff the project initially and to rotate and replace team members continuously.<\/p>\r\n\r\n<p>A good place to start is at the top \u2013 start with the project leadership team before thinking about appropriate technical, project management or process skills. It\u2019s critical that team leads with strong skill sets are in place before the rest of the team is recruited, as the team leads will be vital to the rest of the recruiting process.<\/p>\r\n\r\n<p>Next, create actual job descriptions based on the project\u2019s needs. The descriptions should clearly define the skills and capabilities required for each role.<\/p>\r\n\r\n<p>Beyond the technical, it is imperative to consider the softer skills of candidates \u2013 communication, leadership, working with others, etc. While there is a need and place for specialist skill sets for particular focus areas, on balance it is a good practice to seek out more flexible people who are well-rounded and can adapt and grow as the project evolves.<\/p>\r\n\r\n<p>Once the team leads are in place, and the descriptions are created, it is time to create the rest of the recruiting team. The recruiting team will be responsible for the day-to-day screening, interviewing and placement for the rest of the project team.<\/p>\r\n\r\n<p>For a 50-person project team, an initial recruiting team of one or two people working full time on this task should suffice. However, this is a long-term task, so the recruiting team will persist throughout the lifecycle of the project. Plan accordingly.<\/p>\r\n\r\n<p>As in most worthwhile pursuits, time is the key element. Finding the right talent and resources takes time, so take that into account when planning.<\/p>\r\n\r\n<p>In the example previously given, staffing a team of 50 people can take a lot of time when being picky about who is selected. It will likely take hundreds, if not thousands, of person hours to find the right 50-75 (alternates for when attrition begins) for the project team.<\/p>\r\n\r\n<p>The same is true for the on-boarding process. With a larger project team, it is often more efficient and effective to onboard groups rather than individuals. A good approach is to set a regularly scheduled on-boarding day on a monthly basis for incoming team members. That session should include a general orientation of the project, processes and the overall team, and include any necessary training.<\/p>\r\n\r\n<p>Training should also include specific tools the team is using for collaboration and communication. And as talented as people may be, it will also take them time to ramp up to their specific roles in the project, so that needs to be built into the overall onboarding timeline.<\/p>\r\n\r\n<p>Additionally, finding the optimal mix of in-house and outside talent for any large project is often a delicate dance. Team chemistry versus overall team talent and competency should be given some consideration.<\/p>\r\n\r\n<p>However, training and \u201Cgrowing\u201D in-house resources should not come at the cost of the project. Similarly, satisfying contractual obligations with staff augmentation and consulting firms should not sway team member selection. Rather, the focus needs to stay on those resources that bring the most talent to the table and whose presence will improve the chances for project success the most.<\/p>\r\n\r\n<p>In this case, one great developer is more valuable than 10 average developers. And if organizational politics begin to get in the way of assembling the very best team for the effort, be sure to tie project responsibilities and accountabilities to those making the political waves \u2013 that usually calms the waters.<\/p>\r\n\r\n<p>The end goal in all of this is to put the best team together for that large project. Doing so immediately reduces the project and the organizational risks and increases the likelihood of project success, while allowing talented people to do what they do best.<\/p>",
		"tags": [
            "talent",
            "recruiting"
        ],
        "nextMindShareId": "healthcare-data-sharing-is-only-as-good-as-your-data-standards"
    },
    {
        "id": "healthcare-data-sharing-is-only-as-good-as-your-data-standards",
        "title": "Healthcare Data Sharing is Only as Good as Your Data Standards",
        "authorName": "Saifuddin Bhagat",
        "authorTitle": "Senior Consultant",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "While there is a well-established standards approach for medical treatment coding, healthcare data standards are noticeably lacking.",
        "industry": "healthcare",
        "publishDate": "06/08/2017",
        "readTimeInMinutes": 7,
        "publishName": "Becker's Health IT & CIO Review",
        "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/healthcare-data-sharing-is-only-as-good-as-your-data-standards.html",
        "content": "<p>Standards are a funny thing. In this era of insurance industry innovation, Insurtech startups, and rapid agile software development and distribution, many people blanch at the constriction that standards often represent.<\/p>\r\n\r\n<p>While there is a well-established standards approach for medical treatment coding, healthcare data standards are noticeably lacking. That is important because standards are the essential ingredient for optimizing advanced analytics and the information value derived from a well-designed standards structure.<\/p>\r\n\r\n<p>The healthcare insurance vertical, like most other verticals in the insurance industry, has struggled with pulling together the vast treasure troves of provider and patient information in their many and sundry data stores. Many reasons for this are familiar to all: older systems with poor data editing capabilities; proprietary data stores that require specific data formats but are incompatible with other systems; and operational data stores that create data quality degradation to name a few. However, many of these problems have a root cause based on the fact that most healthcare organizations do not have strong data standards as a part of their overall business and technology ecosystem.<\/p>\r\n\r\n<p>An example of this issue using a couple of pretty common healthcare data elements should prove illustrative: the difference between insurance eligibility and attribution enrollment. With a solid set of data standards in place, the first order of business is defining the data elements once and only once for use by all interested systems:<\/p>\r\n\r\n<p><ul><li>Eligibility is health insurance coverage that one has for a certain or fixed period<\/li>\r\n<li>Enrollment is signing up for coverage or program for a certain or fixed period, or derivation of a relationship for a certain or fixed period. A good way to think about enrollment is as a relationship, much like a marriage or friendship as it exists over some period.<\/li><\/ul><\/p>\r\n\r\n<p>While it might seem mundane, these standards-driven definition distinctions are important for creating analytical insights and actionable value from this data. Because one of the foundations of analytical insight is data sharing across systems and platforms, it is critical to know when a particular data element was created and when the data contained therein was last updated. The same holds true for any subsequent subsets of data. This means that there may be a number of \u201Cdate\u201D elements that are defined in a particular way. For instance, a \u201Cdate\u201D definition for when a file was created, versus a \u201Cdate\u201D definition for when that file is updated, and a subsequent file is created. While this sounds elementary, more often than these initial data standard markers are not present in healthcare technology systems. Here are a couple of examples why that is a problem:<\/p>\r\n\r\n<p><ul><li>Eligibility data needs to be shared when building an analytical claims population file that includes the subset of the claims population a given set of claims represents, and if the date spans are based on either service dates as opposed to paid dates. That is important as there is often a lag between when a patient is seen by a doctor, and when that service is billed.<\/li>\r\n<li>For enrollment, an attribution enrollment analytical file needs first to include the date described time span of the data necessary to derive the attribution and for what period the attribution is valid. An example of this is a patient seeing a doctor for the first time then making an attribution of the patient-doctor relationship valid for some period that includes the past, present, and future.<\/li><\/ul><\/p>\r\n\r\n<p>In both cases, a standards-based definition of the various date elements required is crucial for analytical data quality and insight. So how does one define data standards that optimize healthcare data sharing?<\/p>\r\n\r\n<p>To start, it is important to fix any communication problems among the various parties involved. For instance, providers usually hire an external vendor or buy a COTS product for analytical needs. However, the claims data needed for provider data analysis is usually provided by the payers. In this scenario, there are often two contracts - one between the payer and the provider for the claim, and the other between the provider and the analytical service vendor for the reports. More often than not, the two contracts have different communication protocols, different points of contact, and different delivery timelines. As a result, the provider often plays the role of the middleman, exchanging questions and answers between the payer and the analytics vendor. Many times the communication happens during the beginning of the contract but then fades into an ad hoc basis only. Adding to the problem, there is often no service level agreement in place for questions and verifications between the parties.<\/p>\r\n\r\n<img src=\".\/assets\/mind-share-img\/Saif_Picture_1.jpg\" \/>\r\n\r\n<p>To correct this common situation and make sure all of the parties are working synchronously, there needs to be some upfront collaboration between all the potential stakeholders before the creation and execution of the various contracts. The purpose of this collaboration is to ensure that each party understands the business goals and the analytical needs of the provider \u2013 both tactical and strategic. This collaboration will help the team understand what data is available; what will be provided to whom; and what will be required and expected to meet the data goals. This collaboration should then continue at regular intervals during the entire course of the contracts. This will provide a platform to ask questions and inform the group of any immediate or future changes to the data or process. There should be a medium set (i.e. Jira, MS Excel, or some other collaboration toolset) for asking questions and sharing relevant data.<\/p>\r\n\r\n<img src=\".\/assets\/mind-share-img\/Saif_Picture_2.jpg\" \/>\r\n\r\n<p>Once these high-level agreements are in place, it is time to get into the details of what will allow the parties involved to work in an efficient and effective manner on behalf of the provider. For example, all medical claims cannot be provided in one file. There is a clear distinguishing feature between facility, professional and Rx claims. These claims should be provided in separate files, but if they are provided in one file, there should be a clear indication of that. Also, since claims files are usually created on a monthly basis from the source, the data is often extracted based on service date. This results in an incomplete file because not all claims in that service month might have made it into the system. Hence, the easiest and safest choice is to extract data based on paid dates. This will always be a complete set with no duplication from month to month.<\/p>\r\n\r\n<p>Furthermore, a primary file should always have a supporting file (e.g., Provider_Rx_022016.txt should also have Provider_Rx_022016_support.txt). The support file has information such as when the file was created and its corresponding eligibility file. Similarly, an attribution file will have look-back dates, validity date, etc. Another possible option would be that all files come bundled in a zip format with a summary file. Of course, it is understandable that for privacy purposes claims related to STD, mental illness, or drug abuse are not shared. However, not sharing such data might lead to incorrect analytical output. A solution for this is to effectively mask the member information and provide the rest so that it can be included in the analytics. Finally, it is important to create a mandatory and industry specific list of columns populated with relevant data before sharing. A good example of this is that a claims file should always contain the service date, paid date, member id, POS, ICD code, servicing provider, etc. An eligibility file should contain an effective date, end date, member id, subscriber id, coverage info, etc.<\/p>\r\n\r\n<p>All of the above can be thought of as part of a data standards approach that will lead to efficient and effective data sharing. Once these elements and standards are in place, providers should be able to create insightful and actionable data analytics that provide more effective patient care and more efficient financial management.<\/p>",
		"tags": [
            "data",
            "standards"
        ],
        "nextMindShareId": "why-clinical-document-architecture-doesnt-solve-data-quality-issues"
    },
    {
        "id": "why-clinical-document-architecture-doesnt-solve-data-quality-issues",
        "title": "Why Clinical Document Architecture Doesn't Solve Data Quality Issues",
        "authorName": "Mohammed Hussain",
        "authorTitle": "Senior Developer",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Clinical Document Architecture (CDA), for all its promises of better-organized and better quality data analytics, has some problems.",
        "industry": "healthcare",
        "publishDate": "07/05/2017",
        "readTimeInMinutes": 7,
        "publishName": "HealthData Management",
        "publishUrl": "https://www.healthdatamanagement.com/opinion/why-clinical-document-architecture-doesnt-solve-data-quality-issues",
        "content": "<p>Clinical Document Architecture, for all its promises of better-organized and better-quality data, is not perfect. Its weakness is due, in part, to what it is able to do\u2014exchange data.<\/p>\r\n\r\n<p>To be sure, it is no secret that healthcare data has a quality issue, whether it\u2019s technical or otherwise. In fact, from a developer\u2019s perspective, health data is really at the mercy of those who treat patients and enter data into their records.<\/p>\r\n\r\n<p>However, it is possible to give doctors feedback on the data that they are entering to improve its usefulness. Why do they need such feedback? A big reason is the loose definition of Clinical Document Architecture (CDA), which is able to place results in many fields in the typical Electronic Medical Records (EMR).<\/p>\r\n\r\n<p>The CDA is a standard that provides the ability to place data in records, enabling clinician notes to be transferred as free text entry. For better or worse, that leaves developers at the whim of the data that a human has manually entered into an EMR program. It is difficult to tell a system what to do when the word \u201Cwheelchair\u201D is entered in a field for the height of a patient, or the word \u201Cchildhood\u201D is used for recording the date of a procedure in a surgical history questionnaire.<\/p>\r\n\r\n<p>EMR vendors do not take the time to educate doctors and other medical staff properly on how to enter data, especially after systems are running and operational. EMR systems typically do not run any analytics on information entered\u2014usually, they just do edit checks to ensure the data is valid. However, the systems do not ensure that entered data is actually accurate and useful.<\/p>\r\n\r\n<p>To extend the previous example, an entry such as \u201Cwheelchair\u201D is technically valid for height, weight, blood pressure, allergy, favorite soft drink and smoking habits, among other things. This scope of variation is bad for data analytics efforts, because it mixes invalid data with valid data.<\/p>\r\n\r\n<p>An analytics mindset needs to have a desire to drive down healthcare costs by allowing better risk calculations while, at the same time, helping doctors and other medical staff provide better healthcare by identifying which patients need treatment, and how that treatment needs to be organized.<\/p>\r\n\r\n<p>Physicians and other medical staff are increasingly receptive to feedback offered about how to improve data entry. In many cases, they just need the proper education. This means that with the proper analytics approach, the benefits of better data entry can provide tangible and actionable benefits for providers, patients and payers.<\/p>\r\n\r\n<p>And this is where software architecture can help out. CDA is an XML-based structure designed to contain any number of Continuity Care Documents (CCD). The documents are used for tracking patient data. However, while they both use XML as their document structure standard, the use of that XML for these purposes has not been stringently defined.<\/p>\r\n\r\n<p>Here is are examples of how two EHRs handle the \"effective time\" field:<\/p>\r\n\r\n<p><b>EMR1<\/b><br \/>&lt;effectiveTime value=&quot;2015-06-03&quot;&gt;<br \/>&lt;low&gt;&lt;\/low&gt;<br \/>&lt;high&gt;&lt;\/high&gt;<br \/>&lt;\/effectiveTime&gt;<\/p><p><b>EMR2<\/b><br \/>&lt;effectiveTime&gt;<br \/>&lt;low&gt;&lt;\/low&gt;<br \/>&lt;high&gt;&lt;\/high&gt;<br \/>&lt;\/effectiveTime&gt;<\/p><p>Those examples reflect the same patient data generated by two different EMR systems, and it is clear that the data is just different enough to be a programmatic nightmare. While the document structure is common between source systems, the location of values is not. In the example, the effective time is located somewhere in the 'effectiveTime' XML tag, but that is all the standard guarantees. To resolve this requires customization for each EMR, and potentially for each practice.<\/p>\r\n\r\n<p>Furthermore, there is currently nothing in the third-party software market that could truly and properly parse a CCD for the information needed to resolve this kind of issue. Although plenty of products can create and send CCDs, almost nothing can parse them out into a model with which a data analyst can work.<\/p>\r\n\r\n<p>One way to circumvent this is by parsing CCDs into a tabular format so that they can be used in an analytics program. Building a custom CCD parser that creates a configuration file that tells the parser exactly where to get its information from is a good place to start.<\/p>\r\n\r\n<p>To solve the problems posed by the example above, the parser created two configuration files. When parsing the data from EMR1, the parser pulls in that configuration and knows that to get the effective date; it needs to utilize the value attribute of the main 'effectiveDate' tag. For EMR2, the parser gets the effective date from the literal value of the child 'low' tag of the 'effectiveDate' tag.<\/p>\r\n\r\n<p>The key is the ability to rely on the fact that even though each EMR does it differently, a single EMR still does it consistently. That means for EMR1, one would always expect to find the effective date in that value attribute. The parser also accounts for different EMR versions, so if a specific EMR has multiple releases, a configuration can be created for each specific version. The parser can also allow for customization per medical practice, so if different practices use the same EMR version, but one adds more data than another, configurations can be simply added to pull that additional data.<\/p>\r\n\r\n<p>The payoff from this approach is the ability to collapse many different CCDs into a single set of tables upon which analytics can be performed. The parsing solution that was built was designed to be scalable. Clinics receive CCDs in real time, as patient encounter records are created. They are processed in a batch format and parsed in bulk every night by queuing them up and pushing them through the parser before being released to the data mart. However, the code could just as easily be repurposed to queue up records run in real time as loads increase.<\/p>\r\n\r\n<p>This is but one, albeit an elegantly effective one, approach for dealing with some of the inherent problems of a Clinical Data Architecture running headlong into real-world data entry in the medical world. It is an approach that begins to resolve the discord between the theoretical and the practical, and as a developer, it does not get much better than that.<\/p>",
		"tags": [
            "data",
            "quality"
        ],
        "nextMindShareId": "non-core-insurance-system-modernization-matters-too-part-one-assessment"
    },
	{
        "id": "non-core-insurance-system-modernization-matters-too-part-one-assessment",
        "title": "Non-Core Insurance System Modernization Matters Too: Part One - Assessment",
        "authorName": "Priyanga Manoharan",
        "authorTitle": "Senior Architect",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Many insurers neglect the non-core systems they have that provide much of their day-to-day operational capability, struggling with how to approach and scale their efforts.",
        "industry": "insurance",
        "publishDate": "07/12/2017",
        "readTimeInMinutes": 5,
        "publishName": "Insurance Innovation Reporter",
        "publishUrl": "http://iireporter.com/non-core-insurance-system-modernization-matters-too-part-one-assessment/",
        "content": "<p>Core systems modernization efforts\u2014defined here as pertaining to the policy, claims and billing functions\u2014have dominated the industry over the past several years. Insurers continue to pour millions of dollars and significant human resources into the substantial work required to modernize legacy systems. At stake is an insurer\u2019s ability to stay competitive in its current markets and the potential to compete in new and emerging markets by employing systems that are more adaptive, responsive to changing market conditions and focused on the customer experience.  However, in all of that activity, the need to modernize non-core systems similarly remains. Many insurers have struggled with how to approach and scale their efforts for systems that don\u2019t have the bulk or sizzle of the newer core systems. This three-part article will suggest an approach for dealing with non-core systems in a way that matches the resources and efforts required to the potential benefits derived.<\/p>\r\n\r\n<p>The assessment process for non-core systems is similar to the process for core systems but with some tweaks to contextualize the approach for the size and scale of the project effort. In either case, the assessment begins with a thorough analysis of the current and future state business challenges and opportunities, the solution provider space and the functional parameters of the systems that might be considered. The tweak here is that while this might be an elaborate process that includes a specialized software product to manage the assessment and analysis, smaller non-core systems don\u2019t necessarily require that kind of investment. A thorough analysis can still be done with more moderate tools.  With non-core systems, the processes are simpler and don\u2019t require as detailed or as lengthy of analysis.<\/p>\r\n\r\n<p>For many insurers, the move to a modern non-core system might mean a move from a homegrown platform with homegrown processes.  It\u2019s important to keep in mind that while the systems may be lacking in modern touches, those who have been accustomed to working on them may show some reluctance in letting them go. These tend to bias the requirements for the new system toward acting and behaving like the legacy system, potentially adding time and training to the overall process. The senior management team can also significantly impact the evaluation and solution selection process. For example, a company might have a culture of working a certain way or of being reticent to change. If the leadership can make the decision to shift the company culture, it can significantly help with finding the best solution fit.<\/p>\r\n\r\n<p>The next step is a review of the relevant vendor space. Here again, there are some tweaks to the standard assessment process because of the non-core nature of the systems being evaluated:<\/p>\r\n\r\n<p><ul><li>The solution provider space for non-core systems is typically smaller, so there will likely be fewer options to consider. Technology, hosting, integration and reporting are a few factors that may help to narrow the field.<\/li>\r\n<li>Besides from a few exceptions, the solution providers are also typically not large companies themselves. Many have somewhere between 20-50 employees in the organization. This can be good and bad news.<\/li>\r\n<li>The good news is that smaller solution providers mean easier access to the provider\u2019s leadership. This can lead to quicker review and decision times.<\/li>\r\n<li>The bad news is that smaller solution providers, while willing and cooperative, don\u2019t typically have the bandwidth for a steady stream of change requests for their system.<\/li>\r\n<li>When it comes to non-core systems, there are also international providers in the mix. These companies typically have U.S. clients and support multiple currencies.<\/li><\/ul><\/p>\r\n\r\n<p>Like core systems analysis, both functional and non-functional requirements are analyzed and scored, solution providers perform in-person demonstration where possible, and references are provided and contacted. The tweak with references for non-core systems is that it\u2019s often easier\u2014and better for decision-making\u2014to visit a client of the provider to see their solution in action. The smaller functional footprint of most of these systems means that the users of them are knowledgeable about them and are often willing to talk about all of the positives and negatives about the system. Another important aspect of the assessment is any solution provider\u2019s ability to deliver. It\u2019s not unusual for smaller solution providers to over-commit themselves. Keep the following in mind:<\/p>\r\n\r\n<p><ul><li>Functional proof of concepts and technology deep dives are needed to ensure that what is \u201Csold\u201D actually exists. There is a larger chance of overselling with smaller solution providers than with the larger ones.<\/li>\r\n<li>Ask about in-flight implementations and how they\u2019re being resourced. Many smaller providers typically don\u2019t have the bandwidth to support multiple implementations at the same time, so the timeline of when they\u2019re available for the next implementation is key.<\/li>\r\n<li>Build timing flexibility into the analysis and proposed project plan. The project may need to be delayed until a time when the provider\u2019s teams are available.<\/li>\r\n<li>Request dedicated resources and be sure to carefully review the depth and breadth of experience of those resources.<\/li><\/ul><\/p>\r\n\r\n<p>Additionally, most non-core systems rely heavily on integration with feeder systems and other data sources. Be sure to carefully assess the relative maturity of the integration points and protocols of any system under consideration. As opposed to larger core systems, the sophistication of the integration architecture can vary widely in smaller non-core systems. Request specific documents that depict the architectural structure of any potential solutions as that\u2019s a great way to assess the architectural strengths and weaknesses of the overall platform. Finally, it\u2019s important to understand that many non-core systems do not have robust reporting capabilities\u2014sometimes that\u2019s intentional. Many smaller solution providers will work with the insurer to customize the kind of reporting required and to identify the required data integrations. This can be time-consuming, so it needs to be accounted for in the overall project plan once a solution is selected.<\/p>\r\n\r\n<p>In part two of this article, the focus will shift from assessments analysis to contracts and implementations.<\/p>",
		"tags": [
            "systems",
            "assessment"
        ],
        "nextMindShareId": "a-short-guide-to-emerging-website-architectures"
    },
	{
        "id": "a-short-guide-to-emerging-website-architectures",
        "title": "A Short Guide to Emerging Website Architectures",
        "authorName": "Jeff Sallans",
        "authorTitle": "Senior Consultant",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Front-end experience was previously limited to static websites that were dull for the customer and tedious for the architect or developer to maintain. That has all changed.",
        "industry": "insurance",
        "publishDate": "08/02/2017",
        "readTimeInMinutes": 5,
        "publishName": "Becker's Health IT and CIO Review",
        "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/a-short-guide-to-emerging-website-architectures.html",
        "content": "<p>It\u2019s no secret that mobile applications and games have upped the ante on the kinds of interactive experiences that customers have come to expect.<\/p>\r\n\r\n<p>Several years ago that front- end experience was limited to relatively static websites that, while rendering useful information and some connectivity, were dull for the customer and tedious for the architect or developer to maintain and update. That has all changed over the past few years, and today\u2019s organizations face intense pressure to create websites and mobile apps that are not only useful but also are intuitive, engaging and interactive. These kinds of experiences for customers \u2013 and potential customers \u2013 are becoming core to many organizations\u2019 customer service and retention approach, as well as to their branding and messaging appeal. The experiences that a consumer has with an organization\u2019s technological front door are quickly becoming one of the key aspects of acquiring and keeping a customer. With this change in mind a new architecture has been created called MVVM (Model-view-viewmodel):<\/p>\r\n\r\n<img src=\"sallans-image-1.jpg\" \/>\r\n\r\n<p>MVVM makes the website\u2019s user experience comparable to that of a mobile application by increasing the speed of navigation and business logic by ten times. This increase is possible because MVVM moves the business logic that previously was coded on the server to the front-end, as shown below. With the majority of the codebase moving to the front-end, architectural tools are being developed to organize large front-end code bases to help maintain code quality. The following illustration highlights the way in which MVVM enables improved user experiences:<\/p>\r\n\r\n<img src=\"sallans-image-2.jpg\" \/>\r\n\r\n<p>The rest of this article focuses on two of the architectural frameworks currently available \u2013 Angular from Google and React from Facebook \u2013 as a way to highlight these front-end technology trends.<\/p>\r\n\r\n<p>The good news is that both Angular and React produce great front-end experiences for customers. However, both frameworks accomplish that differently. That\u2019s important to project managers, tech leads, architects and developers who may be responsible for putting together the best UIX team for their organization.<\/p>\r\n\r\n<p>The Angular framework is better organized for development, despite requiring more coding by the architects and developers. Google has prefabricated the architecture so that most of the work of naming and defining architectural elements for consistency is already done. While there are tradeoffs when it comes to coding versus having to create and define a set of naming conventions, Angular is a more practical and ready-to-use framework in general.<\/p>\r\n\r\n<p>The React framework is less prefabricated, so that means it is better suited for talented developers and architects who don\u2019t mind coding the architectural infrastructure. That work enables more design flexibility and granularity when using React, along with advantages in testing and overall performance. For instance, using React allows the decoupling of the business logic layer from its - or some other subsequent - UI framework. React accomplishes testing through its virtual document object module that helps to make user experience modeling and testing much easier to perform. That said, React is still more of a \u201Cframework of a framework\u201D that requires some up-front work than Angular. The following illustration visualizes the capabilities and differences of these new front-ends, and what Angular and React bring to the party respectively:<\/p>\r\n\r\n<img src=\"sallans-image-2.jpg\" \/>\r\n\r\n<p>In both cases, there are additional considerations--not the least of which is a learning curve if these frameworks are new to the organization. Both frameworks require practice and patience when it comes to coding and testing to find the best fit for the organization. Both frameworks also require updated infrastructure that might not be in place. Potential new tools and libraries for managing smaller and more numerous bits of JavaScript are but one example.<\/p>\r\n\r\n<p>In the end, most organizations will only need one of these frameworks. If there are already strong architectural skills in place, then React might be the better choice. The depth and breadth of its user experience capabilities, along with a more efficient and effective experience modeling and testing approach, makes React a stronger framework overall. It provides all the necessary tools and capabilities required to create responsive, flexible, and hopefully, retentive front-end customer experiences.<\/p>\r\n",
		"tags": [
            "architecture",
            "framework"
        ],
        "nextMindShareId": "non-core-system-modernization-matters-too-part-two-contracts-and-implementation"
    },
	{
        "id": "non-core-system-modernization-matters-too-part-two-contracts-and-implementation",
        "title": "Non-Core Insurance System Modernization Matters Too: Part Two - Contracts & Implementation",
        "authorName": "Priyanga Manoharan",
        "authorTitle": "Senior Architect",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Implementations for non-core systems follow many of the same best practices as for core systems, but with a few tweaks for scale and scope.",
        "industry": "insurance",
        "publishDate": "08/09/2017",
        "readTimeInMinutes": 5,
        "publishName": "Insurance Innovation Reporter",
        "publishUrl": "http://iireporter.com/non-core-insurance-system-modernization-matters-too-part-two-contracts-implementation/",
        "content": "<p>Part one of this article was a discussion regarding an approach for assessing and analyzing non-core systems (just about anything besides the policy, claims and billing functions) that suggested ways to scale the effort to match the potential benefits derived from the system. This article will focus on two of the other elements necessary for modernizing non-core systems: contracts and implementation. The premise of this installment is to provide a framework that insurers can use to treat their non-core system acquisitions and implementations as if they were core systems, but adjusted for scale and scope.<\/p>\r\n\r\n<p>When it comes to contract negotiations with many of the smaller solution providers that sell and support non-core systems, the good news is that the negotiations are often conducted with the owner or other senior members of the company. This usually means that decisions and changes can be discussed and made more quickly than with larger solution providers. It also means there may be fewer layers of lawyers and procurement managers involved. However, non-core solution providers often have their challenges and nuances, so among other things, it\u2019s important to:<\/p>\r\n\r\n<p><ul><li>Take extra care with the contracts to ensure that appropriate service level agreements have been defined. Many smaller providers have limited resources and can (and will) over-commit to what they can do and how quickly they can do it to close a deal.<\/li>\r\n<li>If the solution provider is hosting or is using a third-party hosting service, then an annual security review ought to be included in the contract beyond the initial review. Cyber liability coverage should be considered as well, either through the solution provided via an addendum in the contract or through an independent insurer specializing in such coverage.<\/li>\r\n<li>The contract should include a copy of the code that can be placed in escrow. This is particularly important for smaller solution providers, who may or may not have the financial wherewithal to stay in business for as long as the insurer uses their product(s). However, obtaining the code doesn\u2019t always mean that standing up an environment to house and protect the code is easy or feasible, so the time it might take to do that should be part of the overall project plan.<\/li>\r\n<li>Including a clearly defined list of functionality and expectations in the contract can also help mitigate the possibility of overselling by any of the vendors. Although it may take a bit of time, it is worth the investment so that the vendor is contractually bound to deliver the functionality.<\/li><\/ul><\/p>\r\n\r\n<p>Implementations for non-core systems follow many of the same best practices as for core systems but with a few tweaks for scale and scope. Practices like leveraging the functionality of the non-core system out of the box to avoid customization and scope creep and avoiding the mistake of comparing the new functionality to any of the incumbent systems (often homegrown) remain valid.  That said, there are some wrinkles to be aware of when implementing non-core systems as follows:<\/p>\r\n\r\n<p><ul><li>Smaller solution providers with multiple clients don\u2019t typically have a deep bench to handle the flow of change requests that always occur once testing begins. Implementation timelines should take into account that the vendor development team is on the critical path.<\/li>\r\n<li>Don\u2019t fall into the trap of attempting to make the non-core system compensate for functionality gaps and shortcomings in the core systems. This is especially important when it comes to data gaps or inconsistent data.<\/li>\r\n<li>Take into account the community using the non-core system. Many will be transitioning directly from older non-core systems, but others may be part of the group already exposed to modern core systems. This comes with the burden of ensuring that the user experience is a comparable one. Wherever possible, have the non-core system users use the new system and core system users stick to their core systems.<\/li>\r\n<li>As a result of not being a core system, reporting and analytics are usually under-developed in many non-core systems. Although the total user group might be small, the impact of processes can be significant if reporting and analytics structures need to be modified for the new system. For example, a premium audit group managing $200M in annual premium can typically recover 1-2 percent per year in premium leakage by using data analytics. If that capability is not immediately available in the new system then the cost it takes to replicate that functionality is more than just the time and resource burn for the technical effort.<\/li>\r\n<li>Similarly, if moving from a paper process to an automated one in the modern non-core system, then expectations have to be created and managed about what functions will be introduced over what period. It can be a fundamental process shift from a reactive analytics and reporting mode (usually paper-based) to a proactive one (usually automated). With the implementation of a modern non-core system, new data points are available, and companies should carefully consider their reporting options.<\/li><\/ul><\/p>\r\n\r\n<p>Finally, and as with modern core systems, good solution providers will take the time to understand the client\u2019s needs and processes prior to any development. This will eliminate the round-peg-into-the-square-hole issue that can occur. If the solution provider is not starting there, then it\u2019s incumbent on the client to begin that process to ensure that the capabilities are mapped to the appropriate business processes. Also, initial releases should be quick and contain as much out of the box as possible. Adjusting or updating the system as it matures with feedback from its actual use will be much more effective than attempting to build a \u201Cperfect\u201D system in a whiteboard setting.<\/p>\r\n\r\n<p>Following this suggested approach should benefit the assessment and implementation of any non-core system. While these systems are often in the background and don\u2019t get the same kind of attention and fanfare as modern core systems, they are essential to the ultimate success of any insurer. Part three of this article will focus on post-implementation best practices, including sustaining and maintaining non-core systems in the most efficient and effective ways.<\/p>",
		"tags": [
            "systems",
            "implementation"
        ],
        "nextMindShareId": "avoiding-communication-breakdowns"
    },
	{
        "id": "avoiding-communication-breakdowns",
        "title": "Avoiding Communication Breakdowns",
        "authorName": "Hatim Kader",
        "authorTitle": "Project Quarterback",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "There's an adage that the 3 most important elements in real estate are all the same: location. The same is true for consulting, but replace location with communication.",
        "industry": "insurance",
        "publishDate": "08/17/2017",
        "readTimeInMinutes": 5,
        "publishName": "Insurance-Canada",
        "publishUrl": "https://www.insurance-canada.ca/2017/08/17/xby2-communication-breakdowns/",
        "content": "<p>There\u2019s an old real-estate adage that claims the three most important elements in any real estate sale are all the same one: <i>location, location, location<\/i>.  The same can be said for the practice of consulting by replacing the word \u201Clocation\u201D with the word that represents the most important element of consulting: <b>communication.<\/b><\/p>\r\n\r\n<p>Yes, the technology can be challenging, and yes, understanding the business goals and problems is vitally important, and yes, identifying the best-fit solution with the highest probability of success is key.  But none of these elements mean anything if they are not effectively communicated to the client, project team, solution provider, etc. Many projects have foundered as a result of inefficient and ineffective communications to the key stakeholders.<\/p>\r\n\r\n<p>Communication can take many forms, and it is such a vast and deep subject that after many years of practicing effective communication that I find myself learning and re-learning some of its basics time and time again. This is not to say that effective communication requires a Psychology or Marketing degree either. It just requires some common sense, and more often than not, some persistence.  For instance, anybody who has been a parent, a manager of people, or even in IT, knows that it\u2019s much easier to gain support for an idea or action if the party being communicated to believes that they helped to create said idea or action.<\/p>\r\n\r\n<p>Put another way, it is much easier if the other party believes it to be <i>their<\/i> idea. This is not to suggest that deception, coercion, or even subterfuge be employed.  Rather, it is to suggest that understanding the other party, their perspectives and building a working relationship that\u2019s based on mutual respect and trust helps to create the environment that yields successful outcomes.<\/p>\r\n\r\n<p>In the real world of consulting and project management, it takes some experience to recognize when and where there\u2019s a potential for communication issues. Many times the problem or issue is stark, and the eventual solution is evident; however, there could be other enterprise environmental constraints that influence the decision-making. Often things may seem clear on the surface, but just below the surface, there are obstacles to communicating effectively any proposed solution.  It\u2019s important to work to understand such things. For example, an organization might have some underlying experiences that make committing to a particular solution more difficult.  These experiences could be some of the following:<\/p>\r\n\r\n<p><ul><li>The organization is cautious or averse to solutions they haven\u2019t experienced, like cloud-based applications or storage solutions.<\/li>\r\n<li>There may be push back over concerns of initial and ongoing costs.<\/li>\r\n<li>There may have been prior bad experiences with a particular type of product (ex: COTS solution) or a particular provider.<\/li>\r\n<li>There may be cultural or technical resistance to a proposed solution due to the potential impact on long-held practices and processes.<\/li>\r\n<li>There may be political infighting that prevents progress from occurring.<\/li><\/ul><\/p>\r\n\r\n<p>Where such enterprise environmental constraints might be in play, a quick jump to the conclusion often carries the risk of a good solution proposal being rejected due to the real or imagined constraints. These could lead to lost opportunities for major transformational initiatives with high direct and indirect costs to the organization as a result of misunderstanding and miscommunication of the necessity for and benefits of the proposed solution. Good consultants should make note of such enterprise environmental constraints. As part of their overall strategy for understanding business problems and evaluating best-fit solutions, consultants should also devise strategies for getting their enterprise stakeholders to grasp the problem and assess the proposed solution options.<\/p>\r\n\r\n<p>To avoid this unpleasant but all too common consulting mistake, there are a number of communication approaches that I\u2019ve employed over the years that can help:<\/p>\r\n\r\n<p><ul><li>The deliberative approach. This approach allows time for the problem and its shortcomings to be understood, usually through persistent and understandable communication. It helps the stakeholders to grasp the problems involved fully.  While it may take a little longer, the benefit of this approach is that the stakeholders will discuss solutions naturally and often of their own accord.<\/li>\r\n<li>The team exercise approach. This method involves engaging the stakeholders in exercises that acknowledge the current state, analyze the gaps that need to be remediated to reach the desired future state and discuss potential solutions \u2013 technology-based or otherwise.<\/li>\r\n<li>The anticipation approach. This approach involves more than just anticipating questions, a common practice for good consultants.  Rather, this approach requires a deep understanding of the enterprise environmental constraints \u2013 technology, resources, budget and revenue, talent, priorities, market pressures, etc. \u2013 and preparing and presenting multiple options that challenge those limitations and\/or address those constraints and their relative pros and cons.<\/li><\/ul><\/p>\r\n\r\n<p>These common sense approaches have been around for quite some time, but in the heat of the project battle they are often forgotten or overlooked. The importance of effective communication between all key stakeholders is on the critical path for any successful business technology initiative.  However, even with experience using these techniques as a guide effective communication can be difficult to achieve and maintain.  In my own experience, there are any number of variables that can impact successful communication.  Despite my best efforts as a consultant, I am not always as successful as I\u2019d like to be when establishing communication protocols with clients.  It\u2019s fair to say that sometimes it works better than others, and sometimes a favored approach needs to be abandoned for something else that might prove to be more effective.  The key is to understand the audience, the environment, the constraints, and then to tailor a communication approach that is the best fit for the combination of elements.  The final, most important key is to keep learning.<\/p>",
		"tags": [
            "communication",
            "consulting"
        ],
        "nextMindShareId": "non-core-insurance-system-modernization-matters-too-part-three-sustainability"
    },
	{
        "id": "non-core-insurance-system-modernization-matters-too-part-three-sustainability",
        "title": "Non-Core Insurance System Modernization Matters Too: Part Three - Sustainability",
        "authorName": "Priyanga Manoharan",
        "authorTitle": "Senior Architect",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "The problem with maintaining non-core systems after production implementation is that they never get the priority and resource dedication that core systems typically get.",
        "industry": "insurance",
        "publishDate": "09/18/2017",
        "readTimeInMinutes": 5,
        "publishName": "Insurance Innovation Reporter",
        "publishUrl": "http://iireporter.com/non-core-insurance-system-modernization-matters-too-part-three-sustainability/",
        "content": "<p>Parts one and two of this series focused on assessing and implementing non-core systems (defined as just about anything other than policy, claims, and billing systems) that suggested ways to prioritize and scale the efforts required when modernizing non-core systems. This, the third part of the series, focuses on how to overcome the challenges inherent in supporting and maintaining non-core systems post production. As with the first two parts of this article, the premise remains on suggesting a framework that insurers can use to treat their non-core systems as if they were core systems, but adjusted for scale and scope.<\/p>\r\n\r\n<p>The primary problem with maintaining non-core systems after their production implementation is that they are non-core systems and because of that, will never get the priority and resource dedication that core systems typically get. With core systems, there are often large teams of dedicated resources that are committed to the effort for years due to the strategic, operational, and financial importance of the system or systems. This makes perfect sense in an insurance landscape where modern core systems are the table stakes for many of the other innovations and initiatives insurers want and need to take on.<\/p>\r\n\r\n<p>With non-core systems, however, the project teams are not nearly as large and are often disbanded and shifted to higher priority projects just as soon as the non-core system is implemented in production. More often than not the business group is left managing and supporting the system with a skeleton support staff that may not be dedicated to that effort, and may not even include any IT resources.  This often creates a cascade of problems:<\/p>\r\n\r\n<p><ul><li>A business group left to their own devices to manage the newly installed system has other day-to-day priorities, and this reality will inevitably slow down the overall adoption and maturation of the new system.<\/li>\r\n<li>Modernized non-core systems typically introduce new processes and functions to business areas. To get to production, many customers defer some of these enhancements to hasten the delivery of the new system. That creates a post-production backlog of potential enhancements and features that nobody has the time to investigate.<\/li>\r\n<li>That backlog can lead to the system being sub-optimized and therefore unable to deliver the efficiencies it promised. The danger is that since non-core systems generally have a long lifespan, people will just come to accept the system as is, without it reaching its full potential.<\/li>\r\n<li>Since neither IT nor the business has the time and resources to maintain the system, insurers will turn to the vendor for that support. This often makes the problem worse. The vendor may, in fact, work diligently to resolve any issues, but without the proper internal attention required to separate technical problems from process problems, many vendors simply adapt the system to incorporate existing bad processes.<\/li>\r\n<li>This pretty typical cascade of events often results in an overall negative view of the new non-core system by most involved, and that can lead, quite unfortunately, to the insurer never achieving the potential improvements and innovations in effectiveness and efficiencies that modern systems can bring to any company.<\/li><\/ul><\/p>\r\n\r\n<p>That said, there are a few simple steps that insurers can take to avoid the scenarios above for their non-core systems modernization efforts. Employing a little foresight, insurers should plan at the outset for a temporary allocation of business and IT resources post implementation. These resources should be used to form two small teams\u2014one for routine support (upgrades, patches, performance, etc.) and one for focusing on the process and functional impacts of the new system. Depending on the size of the system and the insurer, they may be as small as 2-3 people per team.<\/p>\r\n\r\n<p><b>Roles of Support and Impact Teams<\/b><p>\r\n\r\n<p>While the support team helps to settle the system into production, the impact team should begin the analysis of identifying the process changes that remain, quickly remedying any initial pain points, and prioritizing the enhancements backlog. The impact team can quickly gather the data they need to optimize the system by interviewing new users of the system to identify process opportunities, and by working with the vendor to make sure they\u2019ve fully implemented the latest features of the system. The focus of the impact team is on process improvement first, followed by any technical system changes required to enable the process changes.<\/p>\r\n\r\n<p>One of the biggest benefits of most new systems is the creation and availability of data not previously available. One of the key roles of the impact team is the identification of opportunities where any new data can be put to good use. That\u2019s often in the form of business process improvement and regulatory efficiencies, but there are many other areas where access to new data elements could prove useful and valuable.<\/p>\r\n\r\n<p>Both the impact and support teams should work toward creating a prioritized list of such opportunities. That list should then become part of an ongoing support plan that packages system improvement initiatives into regularly scheduled releases. The releases should be scheduled on a quarterly basis (as opposed to core systems where releases are often deferred for years due to the complexity and the disruption inherent in them) to take advantage of the value of performing incremental improvements on a consistent basis.<\/p>\r\n\r\n<p>Finally, this ongoing support work can be done by a small team, sometimes even ad hoc, that comes together for a couple of weeks each quarter to implement the scheduled release. Resources from the original impact and support teams might be used for this purpose, but it\u2019s not mandatory.<\/p>\r\n\r\n<p>In fact, it\u2019s often the case with non-core systems that after a year or two the system settles in and any functional or technical releases\u2014from the vendor or the insurer\u2014become fewer and farther between. Once any potential enhancements reach the point of diminishing return, many non-core systems can be put into a simple maintain mode until the next modernization cycle begins.<\/p>\r\n\r\n<p>Non-core systems modernization should be an essential part of any insurer\u2019s short and long-term technology plans. With the proper planning and execution, any insurer\u2019s non-core systems should be humming as well as any of their core systems\u2014perhaps even better.<\/p>",
		"tags": [
            "systems",
            "sustainability"
        ],
        "nextMindShareId": "automating-a-little-can-help-a-lot"
    },
	{
        "id": "automating-a-little-can-help-a-lot",
        "title": "Automating a Little Can Help a Lot",
        "authorName": "David Mitzel",
        "authorTitle": "Architect & Senior Developer",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Reliance on information has become the way that hospitals operate and creating, organizing and optimizing information on a real time basis is an operational imperative.",
        "industry": "healthcare",
        "publishDate": "09/01/2017",
        "readTimeInMinutes": 5,
        "publishName": "HIT Leaders & News",
        "publishUrl": "https://us.hitleaders.news/automating-a-little-can-help-a-lot/",
        "content": "<p>Hospitals are nothing if not busy places. Doctors, nurses, patients, families, and support staff make up the human fabric of the daily life at any hospital system. And undergirding all of these stakeholders and activities is another sort of lifeblood for the hospital \u2013 data and information. The reliance on information has become the way that hospitals operate, and creating, organizing, and optimizing information on a real time basis is an operational imperative for any hospital system. One of the problems, however, is that hospitals need and use a fair amount of information that they do not create. This article focuses on one of those information streams \u2013 the data sent to hospitals from insurance companies \u2013 and suggests some best practice approaches for quickly automating the file load process as part of any hospital\u2019s overall data quality improvement efforts.<\/p>\r\n\r\n\r\n<p>The initial process is likely familiar to anybody in a hospital working with data from insurance companies. There are multiple file loads \u2013 from dozens to hundreds to thousands \u2013 on a daily and monthly basis, sent from a multitude of insurance companies. Somebody, often from the IT group, is tasked with reviewing the files, determining what should and shouldn\u2019t be loaded, firing off the necessary load processes, validating the results, and following up on any files that weren\u2019t received this month or didn\u2019t load successfully. The entire process is laborious and time-consuming, and can easily result in errors. It screams out for an automated solution.<\/p>\r\n\r\n<p>That said, there are challenges that any automated solution needs to address. Chief among the challenges is deciding what can and should be automated, and what can and should be left for operator intervention and decision-making. While automating the entire process is the goal, in some cases, experience and intuition cannot be automated, so a process should be created that allows for an operator to make a judgment on something where appropriate. Another challenge is determining how to handle the multitude of file format and data errors that inevitably occur on a daily basis. Some of the errors are often due to developer mistakes, while others are transmission problems, and still, others are from files that should not be loaded at all, either because they are duplicates, or because they aren\u2019t relevant.<\/p>\r\n\r\n<p>The first step in the automation process is to identify what human operators already know and codify that through enhanced metadata. Operator decisions such as file action (load, archive, ignore, etc.), load process, load frequency, identifying file formats, and many more can all be easily expressed in metadata. From there, a standard and automatic way of responding to data and load errors and exceptions should be determined. This is not just a technical requirement, but also requires the input of those members of the team responsible for communication back to the insurance companies. All of that becomes input for the requirements for developing an automated system that handles incoming files from any and all insurance companies.<\/p>\r\n\r\n<p>Automating file handling in this way will significantly reduce the time required of operators as a part of the process. The whole process orientation is flipped from being manual labor dependent \u2013 complete with such human delays as conflicting priorities, run on meetings, and sick days \u2013 to a process where operator intervention is entirely exception based. Going forward, rather than manual monitoring of a file load system, operators are instead notified via an electronic ticket from any number of collaborative software tools available. This kind of automation approach also has the important side benefit of allowing valuable business and IT resources to be repurposed for much more important tasks \u2013 tasks that have more of a direct impact on patient and employee well being.<\/p>\r\n\r\n<p>From a technical perspective, automating the file load process can drastically reduce the number of file processing errors by reducing operator intervention. Also, the creation of more robust metadata to define the files took what was information that was housed in people\u2019s heads and placed it in a systems structure, thus reducing organizational risk. Finally, one of the most important technical benefits is that the automation structure provides a place to perform data validation on each and every file, thereby beginning the process of improving any hospital\u2019s data quality quotient over time. That is a non-trivial benefit that pays short and long term dividends for any hospital hoping to improve their data quality for the purpose of implementing an analytics approach to information.<\/p>\r\n\r\n<p>There is one additional benefit worth mentioning. Many hospitals struggle with breaking the cultural barriers of doing things differently. It can be difficult to abandon something that works \u2013 even if it is cumbersome \u2013 for something that requires less human intervention. Some people will view the automation of formerly manual tasks as a diminishment in the value of the job they perform for the hospital. That, of course, is not true, and the key is to allow people to do things that are more important for, and valued by the hospital. This relatively simple automation process is a great step in that direction and has the potential to significantly improve the validity and integrity of data received from insurance companies. It can also be used as a good teaching moment for hospitals working toward modernization and better data usage and insights: a little automation can go a long way.<\/p>",
        "tags": [
            "data",
            "automation"
        ],
        "nextMindShareId": "emr-data-integration-still-a-long-way-to-go"
    },
	{
        "id": "emr-data-integration-still-a-long-way-to-go",
        "title": "EMR Data Integration: Still a Long Way to Go",
        "authorName": "Bill Sun",
        "authorTitle": "Senior Developer",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "For all its benefits - realized and potential - the process of aggregating and consuming EMR information is still problematic at best",
        "industry": "healthcare",
        "publishDate": "10/01/2017",
        "readTimeInMinutes": 7,
        "publishName": "HIT Leaders & News",
        "publishUrl": "https://us.hitleaders.news/emr-data-integration-still-a-long-way-to-go/",
        "content": "<p>The advent of Electronic Medical Records (EMR) has been a boon to the operational and analytical abilities of healthcare analytics firms and medical providers, allowing them to more accurately document, track and create the analytics required to better care for patients. Likewise, patients have benefited from the ability to track and monitor their care needs across time, allowing proactive actions to take place when identified and recommended by their healthcare providers. One of the core goals of an EMR-based medical practice is to improve the overall quality of care over time across their particular patient population. This is welcome news all around, but it ultimately entirely depends on the effective and efficient creation, organization, normalization and distribution of medical practice and patient-related data. And that\u2019s where the trouble begins.<\/p>\r\n\r\n<p>For all its benefits\u2014realized and potential\u2014the process of aggregating and consuming EMR (and its offspring the Electronic Health Records or EHR) information is still problematic at best. To date, there is no single technical standard for creating, formatting and distributing EMR data between healthcare analytics firms and EMR vendors. The net result of that is the sub-optimization of the accuracy and usefulness of EMR data. That\u2019s not only an operational issue for healthcare analytics firms but also a serious quality of care issue for patients.<\/p>\r\n\r\n\r\n<p>One of the benefits of EMR data is that it can be converted into insightful and actionable analytic data, both on an individual patient level and on a patient population level. However, the current lack of standards and therefore consistency amongst EMR vendors and healthcare analytics firms continues to be a challenge to producing high-quality analytic information. That means that for most healthcare analytics firms the conversion of EMR data to analytical insights is still a cumbersome one.<\/p>\r\n\r\n<p>Most healthcare analytics firms consume EMR data from multiple vendors in this space. The EMR vendors typically extract and aggregate data from various available sources that they then send to healthcare analytics firms that have contracted with to receive the data on a regular or ad hoc basis. However, the lack of standards by which the EMR vendors create, format and transmit that data makes it very difficult for healthcare analytics firms to consume it efficiently.<\/p>\r\n\r\n<p>For example, things like Continuity of Care Documents (CCD)\u2014a potential valuable data element\u2014are often sent to healthcare analytics firms with missing or incomplete data, or with \u201Cstandard\u201D data fields in one place from one EMR vendor, and in another place from a different EMR vendor. Also, the data is not usually well edited or normalized, so initial input errors are seldom corrected before the data is redistributed for consumption. This process is further complicated by the fact that EMR vendors each use different transmission methods. Some use a web service approach, others use a file transfer protocol hybrid, while others create proprietary transmission channels. This all adds up to burdensome technical complexity and overhead for any healthcare analytics firm trying to consume the data and turn it into useful analytical insights.<\/p>\r\n\r\n<p>The Clinical Document Architecture (CDA) is one of the ways the industry has tried to achieve standardized data, but to date these efforts have fallen short. For healthcare analytics firms needing to consume this data, that leaves few options. Most use consultants or other technical expertise to create front-end data consumption processes that translate and configure EMR data into something more useful to their data purposes. And there are ways to do that, even as it remains incumbent on healthcare analytics firms and other consumers of the data to do so. From a variety of lessons learned in this area, a few steps stand out as critical.<\/p>\r\n\r\n<p><ul><li>First, with EMR vendors providing data in a number of formats, the practice of data profiling is essential. More often than not there are too many assumptions made on the healthcare analytics firm side about what type of data is being sent and how that data will be formatted. Is it test or live data? Will data be sent via the same channel and in the same format as the last time? Has the data been put into a format that can be used for integration on the healthcare analytics firm end? A good practice is to create a pre-integration process that requires the EMR vendor to send live data samples before any regular transmissions commence. This will go a long way toward ensuring that all of the necessary data elements are included and that the data will be ready for integration when it is received.<\/li>\r\n<li>Second, once the above is in place, an automated routine should be created that addresses any potential integration issues before they reach the data store by categorizing incoming EMR data along a data quality rubric. The quality check should result in the creation of a data feedback report that can be used by healthcare analytics firms and vendors alike to improve their data input and formatting accuracy.<\/li>\r\n<li>Third, any integration process should be built in such a way that it\u2019s flexible enough to handle similar\u2014but not the same\u2014data types from different EMR vendors. For example, CCDs from different EMR vendors can be formatted completely different, even though there is a standard for CCDs. That means it\u2019s critical for healthcare analytics firms to build their integration platforms with as much flexibility as possible to handle the same but different \u201Cstandard format\u201D of CCDs.<\/li><\/ul><\/p> \r\n\r\n<p>Taken as a whole, these and other similar steps\u2014including the standardization of EMR data distribution channels by EMR vendors\u2014will improve the quality and usability of EMR data for healthcare analytics firms. However, these steps are also an acknowledgment that the data quality problems associated with EMR data are likely to persist into the foreseeable future.<\/p>\r\n\r\n<p>At present, there are not any overriding market, regulatory or operational forces strong enough to motivate EMR vendors, healthcare analytics firms, insurers or any other stakeholders to invest the time and resources necessary to improve things dramatically. Although, a recent effort to improve the accuracy and the quality of EMR data as part of a process to score medical providers\u2019 eligibility for incentives and benefits for the delivery of proactive patient care holds some promise, but it\u2019ll likely be a long time in coming.<\/p>\r\n\r\n<p>In the interim, it remains incumbent on EMR vendors and healthcare analytics firms to put the necessary technologies and associated processes in place to improve the overall quality, integrity and integration of EMR data into their operational analytics.<\/p>",
        "tags": [
            "data",
            "integration"
        ],
        "nextMindShareId": "modern-business-intelligence-for-a-modern-world"
    },
	{
        "id": "modern-business-intelligence-for-a-modern-world",
        "title": "Modern Business Intelligence for a Modern World",
        "authorName": "Zahid Ansari",
        "authorTitle": "Architect",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Modern BI is about business enablement, consumable dashboards, visual data discovery and producing insightful analytics that are both timely and actionable.",
        "industry": "insurance",
        "publishDate": "11/15/2017",
        "readTimeInMinutes": 7,
        "publishName": "Insurance Innovation Reporter",
        "publishUrl": "http://iireporter.com/modern-business-intelligence-for-a-modern-world/",
        "content": "<q>Modern BI is about business enablement, consumable dashboards, visual data discovery, self-service, leveraging the data capabilities of traditional BI, and producing insightful analytics that are both timely and actionable.<\/q>\r\n\r\n<p>Much like core systems transformations and mobile applications efforts in the insurance industry, the business intelligence trend in the industry is all about putting the customer at the center of the universe. The rapid advent and adoption of what is known as modern business intelligence (BI) and analytics is proof positive of that.  Put simply, the modern BI approach shifts the focus and effort from an IT-centric initiative where reports are produced with complex tools that require specific product knowledge, to a customer centric approach where reports and analysis can be generated quickly by business analysts or others without an IT background. This shift, already well under way, has implications in and across insurance verticals that should allow organizations to more quickly harness and leverage data and information for actionable insights.<\/p>\r\n\r\n<p>Like most industry initiatives, however, conceptualizing modern BI is one thing, while successfully building and using a modern BI platform is quite another.  It\u2019s not a well-kept secret that many insurers have expended copious amounts of time, resources, and money on BI and data initiatives that have not proved successful. The reasons why are many, but a short list that includes poor data organization and quality, complex and expensive tools, competing IT priorities, and soft business sponsorship\/ownership are usually right at the top. These BI efforts have created data fatigue in many insurers, so any modern BI approach must begin with focusing on the ways these traditional risk factors are overcome.<\/p>\r\n\r\n<p>First and foremost, modern BI efforts are business driven and as such are not as dependent on the IT department. The business owns the effort, and commits the resources necessary to make some data inroads. Second, modern BI efforts can be business driven because the tools (such as Tableau and Power BI to name just two) are more intuitive and user friendly. The tools require little to no technical training, and within days (as opposed to months and years) most business analysts can create rudimentary dashboards that can be built upon for insights. Third, since IT supports rather than implements the BI effort, issues like competing IT priorities and long delays for IT generated reports goes away almost overnight. And fourth, since the new tools are agile in nature\u2014favoring quick development and incremental improvements along the way\u2014it naturally leads both the business and IT down the path of working together in a more agile way.<\/p>\r\n\r\n<p>Adopting a modern BI approach still has some challenges. Just as with traditional BI efforts, any successful and effective BI initiative still very much comes down to the data.  No modern BI tool will add any appreciable value to any organization if the data being used is locked in legacy data stores and formats. The good news there is that many insurers have been working on data organization and quality for some years now, and if at this point the data isn\u2019t perfect, it\u2019s still probably in much better shape than it was. Assuming an insurer has made some strides in improving their overall data and information ecosystem, there are some things to keep in mind when transitioning from a traditional BI orientation to a modern BI orientation.<\/p>\r\n\r\n<p>Since a modern BI platform is designed for IT-supported (as opposed to IT-produced) analytic content development, it\u2019s important that the appropriate communications channels and expectations are created between IT and those on the business side who will be developing the analytics. Both sides need to understand their roles and responsibilities clearly, as that will vastly improve the probability for a successful transition from a traditional to modern BI mindset.  One of the keys to the business level usability of modern BI platforms is that they have self-contained architectures. This is what allows business users to execute the full analytic life cycle\u2014from data access to collaborative sharing of insights\u2014and why they are quickly becoming so popular within the business functional areas of insurers.<\/p>\r\n\r\n<p>That said, modern BI platforms do have limitations that this new class of business users needs to keep in mind. Chief among these limitations is scalability. For reporting and visual discovery purposes modern BI tools are providing great value. However, when it comes to advanced analytics where advanced data technology skills and larger amounts of data are needed, there should be an evaluation process to determine whether or not to keep the advanced analytics capabilities within the business function, or move such initiatives to a more centralized technology environment.<\/p>\r\n\r\n<h1>A Way to Leverage Existing Investments<\/h1>\r\n\r\n<p>Adopting a modern BI approach also serves as a way for insurers to leverage some of the investments already expended on the traditional BI and data approach.  One of the frustrations of traditional BI was low user adoption. Since business users couldn\u2019t use the complex tools required and as a result had to request time and effort from IT, many just moved on after a few bad experiences. Now, however, business users can use modern BI tools with little or no training, so there\u2019s an opportunity to rebuild some goodwill while producing actionable and useful analytics.  From a broader operational perspective, modern BI can be used as a way to improve data and analytics governance across an enterprise, by extending the traditional BI governance and data quality processes into the modern BI world.<\/p>\r\n\r\n<p>The point of all of this is that insurers who are serious about their analytics capabilities should take adopting a modern BI approach seriously. Modern BI is about business enablement, consumable dashboards, visual data discovery, self-service, leveraging the data capabilities of traditional BI, and producing insightful analytics that are both timely and actionable. For any insurer interested in leveraging the investments they\u2019ve already made in traditional data and BI initiatives, and in putting the power of insightful data into the hands of their frontline service providers, modern BI is worth a look.<\/p>",
        "tags": [
            "data",
            "business intelligence"
        ],
        "nextMindShareId": "health-information-exchanges-the-future-is-now-for-life-insurers"
    },
	{
        "id": "health-information-exchanges-the-future-is-now-for-life-insurers",
        "title": "Health Information Exchanges: The Future is Now for Life Insurers",
        "authorName": "Yunus Burhani",
        "authorTitle": "Senior Architect",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Life insurance still involves a complex process to assess an individual's health. What has changed are the tools, techniques and information to perform such assessments.",
        "industry": "insurance",
        "publishDate": "11/30/2017",
        "readTimeInMinutes": 5,
        "publishName": "Becker's Health IT & CIO Review",
        "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/health-information-exchanges-the-future-is-now-for-life-insurers.html",
        "content": "<p>At its core, life insurance underwriting hasn\u2019t changed much. It still involves a complex process to assess an individual\u2019s health and longevity risks. What has changed, and what continues to evolve, are the tools, techniques, and information available to perform such assessments.<\/p>\r\n\r\n<p>An application for a traditional life insurance policy still requires detailed information about any past medical conditions, including any procedures performed, the doctors and hospitals involved, the medications prescribed and taken, etc. However, these applications are often partially or incorrectly completed and submitted because they still rely, at least in part, on the applicant\u2019s memory and forthrightness. It\u2019s not unusual for applicants to have difficulty recalling the specifics of procedures or ailments they may have had five to 10 years ago. And of course, certain coverage\/risk amounts (generally more than $2M) may also require life insurers to get an applicant\u2019s full medical record from any or all of their physicians. These documents are usually provided as a written statement from a physician that then has to be read and interpreted by life insurance underwriters.<\/p>\r\n\r\n<p>In short, it\u2019s still a process that is open to misinterpretations, assumptions, errors and omissions, and at the same time, it lengthens the time it takes to underwrite an insurance policy.<\/p>\r\n\r\n<p>However, advances in the healthcare industry and the formation of the Health Information Exchange (HIE) provide the means to improve the life underwriting process significantly. One of the key advancements in the health industry is the usage of the Electronic Medical Record (EMR). Physicians uniformly use diagnosis and procedure codes to record the condition or ailment, and the procedures performed to address medical ailments and procedures. These codes are also used to create claims for health insurers for payments to physicians. The EMR and claims are then submitted to the HIEs. An HIE typically contains EMRs for patients that are not just from a single patient-physician relationship, but rather from a patient to many other relationships other physicians, hospitals, medical practices and the like. Most HIEs will also have the pharmaceutical records detailing the history of the medications taken by the patient.<\/p>\r\n\r\n<p>Since most any of the exchanges contains vast amounts of data, an AI-approach could be used to mine the required data to determine the existence of any key diseases, illnesses or other issues for underwriting. Life insurers could more reasonably rely on the diagnosis and procedure codes contained in the HIE to determine a more specific medical history for any applicant. Done well, this has the potential to greatly improve the speed and accuracy of the life insurance underwriting process.<\/p>\r\n\r\n<p>Many of the current HIEs are funded by physicians and providers and as such are always struggling to survive on a limited budget. If life insurers started to use this data, it would provide another revenue option for the survival of HIEs making it a win-win solution.<\/p>\r\n\r\n<p>The best way to start using the data in these HIEs would be to leverage evidence vendors who are used by life insurers to provide medical histories using traditional paper-based and in-person methods. One of the services these evidence vendors provide is calling the insureds to ascertain a detailed medical history along with urine and blood tests. Of course, this service is highly dependent on the insureds availability. A better alternative could be first to check the HIE data to see if there is information available for that insured and use it accordingly. This could speed up the process of underwriting considerably.<\/p>\r\n\r\n<p>And for life insurers, such a process could be essential for appealing to a new demographic of life insurance customers. It is clear that the most recent generations\u2014Gen X, Y and Millennial\u2014are less interested in insurance than their parents and grandparents, and part of the reason for that is the onerous process new life insurance applicants must submit to. These generations will not abide by the \u201Cold\u201D rules of life insurance underwriting that often includes in-person interviews and in-home medical exams. That is anathema to the sensibilities of these new generations, and they will, apparently, forego life insurance rather than comply with these outdated (in their view) rules.<\/p>\r\n\r\n<p>Instead, these new generations are much more comfortable with non-personal touch for acquiring services of any kind. For life insurers, that means creating services using AI, telematics, automated call and support services, and more robust customer portals and mobile applications.<\/p>\r\n\r\n<p>The wider adoption and use of HIEs is a step in that direction. However for that to occur, life insurers, providers and physicians need to collaborate more effectively, including building a consensus of vision for what and how the HIE ecosystem should evolve. For the life insurance industry, the future really is now.<\/p>",
        "tags": [
            "underwriting",
            "data"
        ],
        "nextMindShareId": "building-great-technical-solutions-requires-more-than-technical-prowess"
    },
	{
        "id": "building-great-technical-solutions-requires-more-than-technical-prowess",
        "title": "Building Great Technical Solutions Requires More Than Technical Prowess",
        "authorName": "Hatim Kader",
        "authorTitle": "Project Quarterback",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "The underlying key to success in leveraging technology is the balance of business needs and the correct technological solutions to meet them.",
        "industry": "insurance",
        "publishDate": "12/12/2017",
        "readTimeInMinutes": 7,
        "publishName": "Health IT & CIO Review",
        "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/building-great-technical-solutions-requires-more-than-technical-prowess.html",
        "content": "<p>In this era of hyper-competitiveness, businesses are increasingly looking at leveraging technology for that added edge or the distinguishing factor that sets them apart from their competition.<\/p>\r\n\r\n<p>Be it an innovative use of population health analytics by a health insurer or a care provider, or the use of artificial intelligence by the auto industry, in every sphere technological breakthroughs are shaping or creating new business opportunities.<\/p>\r\n\r\n<p>In all these shiny examples, the underlying key to success is the right balance of understanding the business needs and applying the right technological solution to meet or exceed those needs. This underlying factor applies to every business and technology customer interface. Whether it is the development of that bleeding-edge application that creates new models such as autonomous driving or the routine application of technological improvements to any business's operational systems, such as modernizing an insurer's claims system. No matter the initiative, achieving this balance requires that the business and technologists can communicate with each other and comprehend each other.<\/p>\r\n\r\n<p>However, more often than not, challenges emerge because of the interfacing parties - i.e., business and technologists - may not be speaking a common language. It's not that IT practitioners are not aware of such language barriers, yet it's still very easy to miss the signs when such barriers start emerging in interactions between IT and business stakeholders. For example, it's easy to imagine an IT practitioner using the word \"metadata\" in his\/her communication with the end-user or customer, and not realizing that even that concept could be very foreign to someone in their audience. This may lead to the parties ending up disconnected, and eventually impacting the effectiveness of that excellent technology in solving the end customer's critical business need.<\/p>\r\n\r\n<p>So how do you deal with this situation? The key is to step into their shoes, to grasp the view from their vantage point as a way to tailor the interactions appropriately.<\/p>\r\n\r\n<p>A non-IT example can help illustrate this point. Imagine you are a person with no prior experience with law enforcement or the judicial system, and you are sitting in a courtroom as a first-time juror. You are listening to \"opening statements,\u201D and the \"prosecutor\" calling upon witnesses. There is also a \"defense counsel\" interjecting with phrases like \"objection,\u201D \"hearsay, and not admissible.\" The judge is also using some technical language to \"overrule\" or \"sustain\" something the attorneys have said, and you find yourself sometimes struggling to understand the proceedings fully. You may wonder why these fine people - the judge, the prosecutor and the defense attorney - need your help to decide this case, and yet they are not helping themselves by making it easy for you to grasp the proceedings effectively. Why can't they use common sense language? What do they mean by \"hearsay,\" and why is it \"not admissible?\u201D<\/p>\r\n\r\n<p>Well, that's precisely how the business people might feel in their interactions with IT practitioners where the IT practitioner - an accomplished individual - may be presenting an elegant solution and explaining it using common industry technical terminologies. However, the business people may not be able to connect with everything being presented, and thereby are not able to partner effectively in the development or application of that just-right solution.<\/p>\r\n\r\n<p>Of course, it would be beneficial if all the stakeholders were equally tech-savvy, as in the case that a juror may be familiar with the law, or the judicial system, or law enforcement terms more generally. But that does not always happen, and here the IT practitioner needs to make that effort in ensuring they first comprehend the business audience \u2013 try to see the world from their vantage point \u2013 and communicates using simple and commonly understandable language \u2013 language that would be readily comprehensible by that business audience.<\/p>\r\n\r\n<p>This is not \u201Cdumbing\u201D down anything; instead, it\u2019s assimilating the business's perspective to see it through the same lenses. We used an example here to illustrate how the use of technical jargon by some or the lack of understanding of technical jargon by others, can impair outcomes. But the point is not just about using terminologies appropriately or simplification of communication only, preferably it is about pursuing effectiveness of technical solutions and creating positive impacts on the business, which requires business and IT to team up effectively and that in turn requires them to fully and completely understand each other.<\/p>\r\n\r\n<p>The key to all of the above is to employ skills that will allow one to see and understand something from another's perspective. Things like listening deeply and attentively, having an honest and open dialogue without preconceived notions of what an outcome should be, understanding that the other person's ideas opinions are just as important to them as yours are to you, and yes, even exercising a little empathy towards the other party when and where appropriate.<\/p>\r\n\r\n<p>In a nutshell, we are talking about emotional intelligence or EQ. \u201CYour EQ is the level of your ability to understand other people, what motivates them and how to work cooperatively with them\u201D as stated by the noted Harvard theorist Howard Gardner.<\/p>\r\n\r\n<p>IT practitioners usually possess good IQs, but they must also nurture and mature EQ skills that are required to understand better, empathize and communicate or negotiate with their business customers. These skills need to be employed in all interactions between IT and business customers, be it simple conversations, or moderating sessions to discuss end-user needs, or presenting solutions or results.<\/p>\r\n\r\n<p>People do possess some or all of these skill sets at varying degrees, but don\u2019t always excel in them \u2013 especially in the IT realm \u2013 and it takes commitment and practice to master and wield them effectively. But done well, blending and leveraging EQ and IQ prowess will lead to bonding (in a workplace context), which leads to trust, which fosters better communication and understanding, and eventually manifests into the development of an effective solution that provides significant positive impact to the business. Now that sounds like it's worth the practice to perfect.<\/p>",
        "tags": [
            "communication",
            "technology"
        ],
        "nextMindShareId": "a-moneyball-approach-to-insurance-it-recruiting"
    },	
	{
        "id": "a-moneyball-approach-to-insurance-it-recruiting",
        "title": "A 'Moneyball' Approach to Insurance IT Recruiting",
        "authorName": "Jeff Sallans",
        "authorTitle": "Senior Consultant",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Among the many challenges facing the insurance industry, none might be more critical - and problematic - than finding and keeping strong technical talent.",
        "industry": "insurance",
        "publishDate": "01/02/2018",
        "readTimeInMinutes": 7,
        "publishName": "DigitalInsurance",
        "publishUrl": "https://www.dig-in.com/opinion/a-moneyball-approach-to-insurance-it-recruiting?brief=00000159-faf0-d111-af7d-fbfd9f0a0000",
        "content": "<p>Among the many challenges facing the insurance industry, none might be more critical\u2014and problematic\u2014than finding and keeping strong technical talent. And this is not a new problem. It has slowly but surely been creeping up on the industry for decades, as technology companies, the lure of startups and the industry\u2019s reputation for living behind the technology curve have all created disincentives for young technical talent to take a look at the industry.<\/p>\r\n\r\n<p>So what\u2019s the answer? In part, it's focusing on the assessment process as a way to find talent that others might overlook by measuring skill\u2014real and potential\u2014over experience. And much like the popular baseball book and movie <i>Moneyball<\/i>, the key to better assessing talent and outcomes is to know the right questions to ask.<\/p>\r\n\r\n<p><ul>This <i>Moneyball<\/i> approach promises several benefits:\r\n<li>First, it could provide the industry with an influx of strong IT talent that could help to move technology capabilities forward.<\/li>\r\n<li>Second, it could\u2014over time\u2014improve the quality of systems and processes in the industry, leading to better customer service overall and improved profitability.<\/li>\r\n<li>Third, it could enhance the efficiency of IT departments as they become restocked with fewer but more capable people.<\/li>\r\n<li>Fourth, it could start to break the perception of the industry as a backwater for technology innovation.<\/li><\/ul><\/p>\r\n\r\n<p>The key to accomplishing all of this is by improving the way in which technical talent is assessed. However, asking the right questions is something that seems easy, but actually isn\u2019t. It can only be learned through the bitter experience of asking many of the wrong questions, which leads to bad hires.<\/p>\r\n\r\n<p>One of the keys to asking the right questions is to ask the questions that would likely arise while somebody was working in that position. To get a holistic view of the experiences and potential of each candidate, the interview questions should be organized into three categories that align with the desired position.<\/p>\r\n\r\n\r\n<p>In the following examples, the positions used are an IT developer, a technical lead, or a junior architect. Through the above-mentioned trial and error, the question categories that have been developed over time for the interview include technical skills, previous successes\/failures and soft skills.<\/p>\r\n\r\n<h1>Technical skills<\/h1>\r\n\r\n<p>The assessment objective behind the Technical Skills category is to determine with some reasonableness of certainty that the candidate is or will be capable of developing quality software within a large project scope. Therefore, the specific questions will not focus on knowing <i>this<\/i> language or <i>that<\/i> configuration tool, but rather on a thoroughness of understanding of best practice coding patterns, testing, and integration techniques, and designing software for scalability and maintainability.<\/p>\r\n\r\n<p>This approach generally leads to an insightful dialog and avoids simply checking yes and no boxes for specific skills. The focus is on the long-term probability of success for any given interviewee. The analogy to the <i>Moneyball<\/i> approach is that this qualitative approach is trying to get at how an interviewee will perform over a full season, given enough at-bats, pitching starts or other applicable metrics.<\/p>\r\n\r\n<h1>Previous success\/failures<\/h1>\r\n\r\n<p>This approach is continued for the two other main categories of the interview. For the Previous Success\/Failures category, the whole point here is that evaluating somebody based upon their experiences to date and extrapolating their development into some future point is usually a better success indicator than evaluating somebody on the specific skills listed on their resume.<\/p>\r\n\r\n<p>So the questions in this category focus on asking the candidate to tell a particular project story from their experience, why they thought it turned out the way it did, what they might do differently next time, and what they learned from the experience. This is a good opportunity to have the candidate use a whiteboard or some other medium that allows them to illustrate and elaborate. There are great insights in this category for a candidate\u2019s thinking and logic process, the technical and operational complexity of their experiences, and their organizational and communication skills.<\/p>\r\n\r\n<h1>Soft skills<\/h1>\r\n\r\n<p>For the final category, the previous discussion on the candidate\u2019s experiences can be used as input to evaluate their Soft Skills. Asking questions that center on <i>why<\/i> something works the way it does, or <i>why<\/i> certain decisions were made, are much more valuable than questions about <i>how<\/i> something works.<\/p>\r\n\r\n<p>For example, answering the question of how to maintain software over time is a very different answer than why it's important to do so. Answering the why question leads to broader thinking about the value of software to a company, its relation to operational processes in the business, etc. There are no real right or wrong answers, and it again becomes an opportunity to have a conversation about a variety of things. The focus is mainly on communications, and the goal is to ascertain a candidate's ability to:<\/p>\r\n\r\n<p><ul><li>Understand technical conversations<\/li>\r\n<li>Meaningfully contribute to technical conversations<\/li>\r\n<li>Summarize the technical details for others<\/li>\r\n<li>Participate in overall team dynamics<\/li><\/ul><\/p>\r\n\r\n<p>The premise posited here is that the insurance industry has a technical talent issue and that the suggested approach herein is one way to begin to address that issue. This approach is of course only one way, and other approaches may be equally effective.<\/p>\r\n\r\n<p>The larger point is that the industry's overall approach to talent assessment needs to get better, and not just by a little bit. The consequences of not doing so are problematic and would leave the industry exposed to continuous innovation and technology deficit as compared to other sectors. That's why the industry needs to be proactive rather than reactive about talent assessment, and judging someone on their current skills even if they haven't had the experience to prove them out entirely is one way to do that. As the statistical analyst opined to his baseball general manager boss in <i>Moneyball<\/i>: \"Your goal shouldn't be to buy players. Your goal should be to buy wins. To buy wins, you need to buy runs.\" We need to score more runs.<\/p>",
        "tags": [
            "talent",
            "recruiting"
        ],
        "nextMindShareId": "if-its-not-mobile-dont-bother"
    },
	{
        "id": "if-its-not-mobile-dont-bother",
        "title": "If It's Not Mobile, Don't Bother",
        "authorName": "Tyler Sullivan",
        "authorTitle": "Marketing Consultant",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "The customer experience expectations of millennials, with a specific focus towards mobile technologies, should be a focal point across the insurance industry.",
        "industry": "insurance",
        "publishDate": "01/04/2018",
        "readTimeInMinutes": 5,
        "publishName": "Insurance-Canada",
        "publishUrl": "https://www.insurance-canada.ca/2018/01/04/not-mobile-dont-bother/",
        "content": "<p>A plethora of articles have been written on how the insurance industry should engage with younger generations of potential customers. Whether millennial, Gen X or Y or anything in-between, the industry has rightly recognized a shift in the research and buying patterns of younger consumers. As a millennial, this change has everything to do with growing up as near digital natives and thus having a different kind of relationship with technology and time than prior generations might have had. For most of the millennial generation then, that means that if it\u2019s not possible to use a mobile device to manage a relationship with an insurer, then we\u2019d just as soon move on, thank you very much.<\/p>\r\n\r\n<p>Now before devolving into stereotypical clich\u00E9s about what a millennial may or may not want, and why or why not that may be, it\u2019s important to understand that just like every prior generation, the millennial generation has their way of relating to the world around them. Objectively speaking, those ways are neither good nor bad; they\u2019re just the way the millennial perspective has evolved. So let\u2019s get specific. My generation has grown up in a world where researching and purchasing things (meals, books, cars, houses) can be accomplished online exclusively via mobile devices.  It should come as a shock to exactly nobody in the insurance industry that the same expectation for researching and purchasing would hold true for insurance. However, since that is not entirely possible at this point, it should also come as no surprise that most millennials tend to just skip the whole insurance thing, at least for now. This is due to many other influences as well, but online accessibility and ease of access remain the key prohibitors in millennials actively seeking insurance. That said, there is an opportunity here. As the millennial generation gets on with their lives, insurers who develop viable mobile interfaces along with targeted products and services can reap the financial rewards of selling into the most prominent demographic since the baby boomers. That\u2019s no small opportunity, and it should be one that insurers are stepping all over themselves to leverage.<\/p>\r\n\r\n<p>One way that the insurance industry might leverage that opportunity is to make investments now to begin to turn themselves into digital businesses. That doesn\u2019t mean that they won\u2019t have buildings or offices, but it does mean that they\u2019ll take a digital\/mobile-first approach to their next generations of customers.<\/p>\r\n\r\n<p>For those insurers who are interested in going down this road, here are the basic millennial requirements:<\/p>\r\n\r\n<p><ul>\r\n<li>Researching and purchasing insurance happens online and can be completed in full by the customer.<\/li>\r\n<li>Any subsequent amendments, alterations or customizations happen online with text chat to answer any questions.<\/li>\r\n<li>Claim reporting happens online in real-time, with AI and robotic technologies to assist.<\/li>\r\n<li>Through the whole process, self-research and text chatting with a live person on the other end are fine, but wasting time on unproductive and inefficient phone calls should never be required.<\/li><\/ul><\/p>\r\n<p>This, of course, is a major change for the industry. But from a millennial perspective, is this a bigger change than what the retail, music, book, or countless other industries have gone through? Or should millennials infer that the insurance industry is just less interested in having them as customers than other industries are? Yes, it is all a matter perspective. For the millennial generation, technology exists to make life \u2013 and everything that\u2019s part of it \u2013 easier and more manageable. If a given technology, or industry, doesn\u2019t achieve that then millennials are loath to have anything to do with it unless absolutely necessary, and even then grudgingly.<\/p>\r\n\r\n<p>And for the millennial generation\u2014and this is important to the insurance industry\u2014technology is directly related to time. What does that mean? That means that the millennial generation has a relationship with time\u2014managing it, wasting it, accumulating it\u2014that is quite different from prior generations. The millennial generation has grown up in an era of high-speed networking, social technologies, and for better or worse, near instant gratification. This means that any technology or service that can save us time is a technology or service that we want in our lives. Yes, this is part of the clich\u00E9d list of complaints that our parents and grandparents have of this generation, along with every kid gets a trophy, etc. However, that\u2019s how we were wired, and that\u2019s no different in the abstract than our parent\u2019s generation being wired for rock and roll and protests, and our grandparent\u2019s generation being wired for the greatest generation and suburban bliss. It\u2019s just part of the millennial experience set, and the insurance industry would be well advised to recognize it and make plans to deal with it sooner rather than later.<\/p>\r\n\r\n<p>So no thank you. The millennial generation is not interested in waiting on hold to report a claim, or waiting for days for an insurance policy or product to be completed, or filling out seemingly endless and redundant forms only to be told that our information can\u2019t be located by the branch office, or to have a nurse come into our home for an intrusive exam for a life insurance application. Like electricity, this generation will flow to the paths of least resistance and will look for other ways to get the insurance type products needed \u2013 hello Insurtech startups \u2013 rather than dealing with an industry that doesn\u2019t care to understand our needs. The demographic reality is that the millennial generation will need insurance products as it starts businesses and families, and purchases homes and cars. The demand will be there, so the supply just needs to get aligned in a way that is more palatable and consumable for the millennial generation.<\/p>\r\n\r\n<p>So drop us a text insurance industry, and let us know how you\u2019re doing. Oh wait, your old system can\u2019t text\u2014we have a problem.<\/p>",
        "tags": [
            "mobile",
            "customer experience"
        ],
        "nextMindShareId": "how-providers-can-get-better-results-from-data-efforts"
    },
     {
        "id": "how-providers-can-get-better-results-from-data-efforts",
        "title": "How Providers Can Get Better Results From Data Efforts",
        "authorName": "Mohammed Hussain",
        "authorTitle": "Senior Developer",
        "authorImageUrl": "david-mitzel.png",
        "shortDescription": "Information in the healthcare industry has seen project successes and failures as insurers race to develope ways to extract actionable information from their data stores.",
        "industry": "healthcare",
        "publishDate": "02/12/2018",
        "readTimeInMinutes": 7,
        "publishName": "HealthData Management",
        "publishUrl": "https://www.healthdatamanagement.com/opinion/how-providers-can-get-better-results-from-analytics-and-data-warehouses",
        "content": "<p>The primacy of information to the healthcare industry over the last several years has seen project successes and failures, as health insurers and other health stakeholders race to develop ways to extract actionable information from their data stores.<\/p>\r\n\r\n<p>While healthcare data and analytics holds promise to bring increased effectiveness and efficiency to the industry, the route to achieving the things that these analytics promise\u2014proactive healthcare, better customer service, process improvements, better claims modeling, increased profitability and more\u2014has been littered with failed attempts. That begs the question of whether there is an approach to healthcare data and analytics that offers some reasonable hope of success.<\/p>\r\n\r\n<p>Through trial and error, a successful pattern for implementing analytics solutions has emerged that is centered around conferring closely with the business customer. By having business analysts and data scientists actually coding their own solutions and employing IT just to standardize, optimize, scale and automate those solutions, there is a correspondingly much higher success rate in delivering those solutions. The idea behind this was and is\u2014because the business had built the original prototype, regardless of how inelegant it may have been, they had a clear understanding of what they should be expecting.<\/p>\r\n\r\n<p>To examine this approach more closely, let\u2019s suppose there are two different tracks of work that were conducted in widely different ways by two different teams. Each track was defined by business and developed by IT, but one was successfully implemented by IT, and the other track was plagued with bugs and ultimately shelved by business pending further analysis.<\/p>\r\n\r\n<p>Briefly going through each track in a simple retrospective and outlining the best practices around how to conduct and coordinate projects between IT and business should illustrate the difference in approaches, and therefore the difference in potential success. The focus here is on healthcare data and analytics and how IT can help facilitate actionable analytics in a continuous delivery and reporting cycle.<\/p>\r\n\r\n<p>Let\u2019s explore two short contrasting case studies that outline how this suggested approach could be applied and its benefits:<\/p>\r\n\r\n<h1>Person matching<\/h1>\r\n<p>The project goal was to develop a master person index using the conformance layer of an Enterprise Data Warehouse project. The driver and business case for the project was to have all outgoing extracts use the same person matching logic to generate data, in the current state (at the time) each individual extract had its own custom matching algorithm and the development of a new track required re-coding this matching in each component.<\/p>\r\n\r\n<p>There were two primary functional requirements. The existing extracts must use this new generic matching logic, and any new system or process must be able to easily integrate with this matched person data.<\/p>\r\n\r\n<p>For non-functional requirements, The major consideration in architecting a solution was to ensure that the core logic of the matching algorithm was easily modifiable so that the algorithm could be refined over time as more was learned on how to properly match disparate person records.<\/p>\r\n\r\n<p>The end status of this work track was that it did not pass acceptance testing. The final product was shelved pending an analysis by the business. In the analysis of this project, there were several lessons learned, but the core issues were that the business resources did not have enough involvement in the design process, and IT did not push enough for that involvement.<\/p>\r\n\r\n<p>The business resources had essentially outlined the problem domain and requested that IT provide a solution, and IT complied by doing an analysis, determining an architecture and developing a solution. The problem was that the business resources had no clear idea (aside from design artifacts) as to what the final deliverable should look like. And because the analytics team could not conceptualize the solution well, the integration was never fully accepted, and the business shelved the project awaiting further research and development.<\/p>\r\n\r\n<p>This track of work only spanned a single sprint before it was pushed back to planning and leadership, but the retrospective results challenged IT\u2019s ability to work properly with the business. It was clear that a change in approach would be required for the next project.<\/p>\r\n\r\n<h1>Dimensional model<\/h1>\r\n<p>The analytics team wanted a dimensional model built out of the conformed data in the enterprise data warehouse. The final result should be a framework on which dimensions and fact tables can be quickly modeled and then moved to production. The primary use case was to build Claims and Membership facts across over 16 dimensions.<\/p>\r\n\r\n<p>The functional requirements of the project was that the solution would be business defined fact tables and their associated dimension tables. For non-functional requirements, the design must support new business needs. New facts and dimensions that are identified should be easily added to the existing integration.<\/p>\r\n\r\n<p>The end result of this work track was successful Quality Assurance (QA) and User Acceptance Testing (UAT) and a limited launch to the analytics team for a \u201Cbeta acceptance.\u201D Additional facts and dimensions were added in the following months as the data scientist performed further discovery.<\/p>\r\n\r\n<p>This track of work followed a different requirements gathering phase. This need was communicated upfront to the business resources assigned to ensure more participation and accountability for the systems that were being built by IT. The business analyst assigned to this work was responsible for creating drafts of the data models and coding the final expected output for each fact and dimension. The models were then refined by the architect and technical leads, and the developers had a business-built and accepted model along with the expected output to test against.<\/p>\r\n\r\n<p>Another success factor was limiting the number of high impact decisions. There were still issues and redesigns done during the development and QA phases, but the overall design stayed the same, and the solution was delivered to the business on time and met their expectations.<\/p>\r\n\r\n<p>This method of providing requirements and defining the final results so that IT can set proper expectations was ultimately successful. Developers are very good at (and primarily responsible) for delivering well-structured and performing solutions to business problems, but often lack the proper intuitions to dissect and disseminate the business problem thoroughly.<\/p>\r\n\r\n<p>Of course, business involvement in a development process is not a new concept\u2014the Agile methodology actually encourages and depends on this\u2014leading to a modus operandi designed to increase business involvement to remove a lot of the guesswork that IT tends to do in these situations.<\/p>\r\n\r\n<p>While not foolproof, the suggested approach significantly increased the likelihood of project success. The business analysts intuitively knew how the data should be queried and used and what the output should look like. Once they provided that to IT, all the assumptions and guesswork was eliminated, and IT could focus on the things it does best by preparing the new dimensional model for production. This should serve as a sharp reminder that the best business technology outcomes happen when business and IT resources find the right way to work together.<\/p>",
        "tags": [
            "data",
            "analytics"
        ],
        "nextMindShareId": "when-it-comes-to-large-projects-think-architecture-first"
    } 
   ]