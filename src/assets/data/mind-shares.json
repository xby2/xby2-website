[
  {
    "id": "xby2-llc-20th-anniversary",
    "title": "X by 2 Celebrates Twenty Years",
    "authorImageUrl": "20th-anniversary.jpg",
    "shortDescription": "X by 2 recently crossed another significant milestone, our 20th Anniversary as a company. Thank you to all of our team members and clients that made it all possible.",
    "isFeatured": false,
    "isPublication": true,
    "authorName": "Company News",
    "authorTitle": "Press Release",
    "industry": "consulting",
    "publishDate": "07/30/2018",
    "readTimeInMinutes": 5,
    "publishUrl": "https://xby2.com/insights/xby2-llc-20th-anniversary",
    "content": "<p>X by 2 was founded in 1998 by three colleagues who left the traditional consulting ranks with an idea to deliver business technology services following a different set of principles.  Initially focusing on internet-based design and commerce, the firm evolved to focus on the practice of software and data architecture, program and project leadership, and business technology assessments and strategies.</p><p>The X by 2 difference, honed over many years of partnering with clients, is based on creativeness, persistence, and always doing what is in the best interest of its clients.  That difference has set X by 2 apart as an innovative and impactful consulting firm that has consistently delivered value to its clients in the P&C, Life, and Healthcare industries.</p><h2 class='text-left'>Reflections</h2><p>Reflecting on the journey, David Packer, X by 2’s President, recalls, “Our aim has always been to enable X by 2’s clients to achieve their strategic and operational business goals.”  A lot has changed in business technology over the past twenty years, but according to Packer “our different approach created the opportunity for X by 2 to serve some of the most forward-thinking insurance and healthcare organizations in North America over the last 20 years. While our day-to-day architecture practice has evolved, our core principles remain the same: objectivity, being results driven, quality over quantity, and creating client self-sufficiency remain the bedrocks of our company.”</p><h2 class='text-left'>In Appreciation</h2></hr><p>Accordingly, we would like to sincerely thank our clients and team members over the years on our twentieth anniversary as a software architecture consultancy.  This milestone would not have been possible without the trust and support of our sixty-plus clients who have engaged X by 2 on over two hundred and fifty projects.  Here’s to the next twenty years.</p><img src='/assets/mind-share-img/anniversary-image-1.jpg' style='max-width: 80%;' /><p>Learn more about X by 2 <a href='xby2.com/services'>Services, Expertise</a>, and <a href='xby2.com/careers'>Careers</a> at <a href='xby2.com'>xby2.com</a></p>",
    "tags": ["publication"],
    "nextMindShareId": "why-worry-about-business-architecture"
  },
  {
    "id": "why-worry-about-business-architecture",
    "title": "Why Worry About Business Architecture?",
    "authorName": "Priyanga Manoharan",
    "authorTitle": "Architect",
    "authorImageUrl": "pri-manoharan.jpg",
    "shortDescription": "The importance of effective business architecture in organizations is a way to, among other things, optimize processes and create standards that enable business technology success.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "07/09/2018",
    "readTimeInMinutes": 7,
    "publishName": "Becker's Health IT & CIO Report",
    "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/why-worry-about-business-architecture.html",
    "content": "<p>Many companies are engaging in large transformation initiatives with the goal of not only modernizing their technology but also creating a more efficient workflow that is powered with better technology to tackle new challenges and growth.</p><p>In many of these cases, there is a clear need for a modernized technology architecture as the systems requiring modernization are antiquated. With a significant technology shift, the need for architectural guidance is clear, and as a result, many projects are staffed with various technology architects to ensure alignment with the established standards. Many organizations realize the need for enterprise-wide guidance and have created architecture groups or departments with the sole purpose of providing enterprise-level technology guidance. While this is all well and good, many organizations fall short when it comes to a similar architectural discipline on the business side.</p><p>This article argues for the importance of effective business architecture in organizations as a way to, among other things, optimize processes and create standards that enable business technology success.</p><h1>What is Business Architecture?</h1><p>Before deciding on whether valuable and limited resources and energy should be dedicated to creating business architecture, a few definitions are in order. Similar to any technology architecture, business architecture provides guidance and a general structure from which to operate.</p><p>The first step is to establish a vision for the business. This goes deeper than any strategic goals related to markets, profits or efficiencies, by defining the set of underlying tactical goals required to enable the higher-level goals. A goal of increasing profit by a certain percent annually doesn't provide enough guidance to create a comprehensive achievement plan.</p><p>A better approach might be to break this goal down into a percentage reduction in cost combined with a percentage increase in revenue. The point is that higher-level goals need to be broken down into levels of abstraction such that they are tactical enough to start creating actionable plans for and around.</p><h1>Optimizing Processes</h1><p>Once the vision has been established, and tactical plans have been created, the next step is to review the business processes. In many if not most cases, older systems have influenced the business processes. Over time many organizational methods have evolved to include steps manual or otherwise that work around technology limitations.</p><p>For example, the lack of a real-time lookup in a system may have resulted in a two-step overnight process - the traditional overnight batch process. Even modernizing portions of the platform might enable real-time lookups to make it a one-step instantaneous process. From there, further examination of the processes and an understanding of the various steps and business needs involved could help identify additional points for efficiency or even automation.</p><p>A key component of examining the processes is understanding and determining where the process pain points are, and how much time is spent on them. This provides the focus required to help eliminate the process pain points, allowing for a better experience for those using the system. Also, prioritizing and subsequently addressing the most inefficient aspects of a process can provide a better return on resource and financial investment.</p><p>One of the best practices of any business architecture is using observed evidence to ensure that the scope of any process improvements directly impacts the business in a positive way.<p><h1>Establishing Standards</h1><p>Standards and other best practices are also valuable sources to incorporate into any business technology solution where applicable. The first benefit is in not having to reinvent the wheel. There also may be many models or standards that can be leveraged that provide great process and technology options.</p><p>An even larger benefit is that the standards offer a point of comparison to the current processes. This allows organizations and the business and technical resources that support them to better understand the value of their current processes. It is possible that the existing standards are too generic and don't quite fit a new or existing niche market of the organization. Either way, process validation or invalidation occurs, thus adding data points to making better decisions in the near and long-term.</p><p>Adopting standards also help when implementing new solutions by making it to easier to implement the out-of-the-box features and functionality, if desired. Having a robust set of standards also means that IT does not spend its time on migrating ineffective or broken processes as part of upgrades and implementations, opening up scarce IT bandwidth. That added benefit allows business, and IT resources to focus on building and implementing value-added features, adding a competitive edge to the organization.</p><p>The difference lies in making a focused effort to take advantage of new features and functions, rather than just approaching them opportunistically as most organizations currently do.</p><h1>Change Management</h1><p>Many organizations struggle with change. That 's why a change management approach is an essential component of any business architecture. The change management impact of all of the business processes optimizations as a result of a new implementation can be daunting.</p><p>For better or worse, many organizations use the change management argument for keeping things the way they are, as handling and training on all that change can be a mammoth effort in its own right. That said, there is a counter-argument for that, however, and that is that there has to be a balance.</p><p>Organizations who avoid the pain of change often encounter additional implementation costs in keeping a product, feature or process working as it always has. This can, of course, increase the cost and complexity of changing when that day of reckoning finally occurs. Additionally, the cost of ownership in both implementing a custom feature or functionality and maintaining that feature or functionality needs to be balanced against the impact and cost of training the end users.</p><p>In many cases, a phased approach can be beneficial, but for most organizations, the bottom line is that sometimes change is inevitable.</p><p>All of this and more is why taking a more proactive approach to business architecture can help organizations not only improve their technology but also help them to modernize their business processes, thus maximizing the return in their investments in large transformation projects.</p>",
    "tags": ["business architecture", "modernize"],
    "nextMindShareId": "how-healthcare-organizations-can-pay-off-technical-debt",
    "nextMindShareTitle": "How Healthcare Organizations Can Pay Off Technical Debt"
  },
  {
    "id": "how-healthcare-organizations-can-pay-off-technical-debt",
    "title": "How Healthcare Organizations Can 'Pay Off' Technical Debt",
    "authorName": "Rob McCurley",
    "authorTitle": "Architect",
    "authorImageUrl": "rob-mccurley.jpg",
    "shortDescription": "By understanding what technical debt is, how it arises, and how to manage it, healthcare organizations can reduce the burden of their technical debt and use it to their benefit.",
    "isFeatured": false,
    "industry": "healthcare",
    "publishDate": "07/09/2018",
    "readTimeInMinutes": 10,
    "publishName": "Health Data Management",
    "publishUrl": "https://www.healthdatamanagement.com/opinion/how-healthcare-organizations-can-pay-off-technical-debt",
    "content": "<p>Technical Debt also known as Design Debt or Code Debt is a concept in software development that reflects the implied cost of additional rework caused by choosing an easy solution now instead of using a better approach that would take longer.</p><p>Most healthcare organizations have a business technology ecosystem that has grown and advanced over years and decades. These are combinations of older and newer systems, legacy and modern data stores, and integration points and processes that have gotten more complex over time. Often, these issues prevent healthcare organizations from upgrading or replacing systems promptly and cost time, money and resources when leadership does decide to upgrade or replace systems.</p><p>The accrual over time of all of these technical issues is called technical debt. For much of human history, debt has generally been considered as something best avoided. But modern times have brought the realization that borrowing can be a beneficial financial tool. So it is with technical debt. By understanding what technical debt is, how it arises, and how to manage it, healthcare organizations can reduce the burden of their technical debt and use it to their benefit.</p><p>Ward Cunningham, an early contributor to object-oriented and agile programming, defined technical debt as the amount of beneficial work avoided or deferred to deliver a business solution earlier or to reduce the cost of the system.</p><p>In the case of technical debt, the  'principal ' becomes the amount of work left undone, and the  'interest ' is the additional work caused over time by this unfinished work. Among the most common sources of technical debt are:</p><p><ul><li> 'Quick and dirty ' coding is created instead of modular, structured code</li><li>Applications use ad-hoc data sources or create new and poorly structured sources, rather than taking the time to implement new, well-designed sources</li><li>Functionality is  'bolted on ' to existing applications without rewriting them</li><li>Functionality is deployed using non-standard technologies</li><li>Documentation is not created to spend more time on coding</li><li>Tests are not created or run to spend more time on coding</li><li>Deferred technology and software version upgrades</li><ul></p><p>Technical debt can be acquired in either a reckless or prudent manner. Reckless technical debt occurs when design or coding shortcuts are taken inadvertently, most often through inexperience or laziness. Prudent technical debt occurs when beneficial work is deferred deliberately for a known benefit, such as omitting a less-useful feature or implementing a workable but less elegant solution to deliver a solution earlier.</p><p>Whether reckless or prudent, each instance of technical debt created during development and maintenance adds to a virtual balance sheet of principal maintained within the organization 's technical infrastructure. This principal represents the backlog of work required for the organization to get full functionality and value from its systems. But often more significant is the interest that the technical debt accrues from its inception.</p><p>This interest manifests itself in healthcare organizations in myriad ways. Some of the most common healthcare examples are systems and applications that are expensive (budget and resources) to maintain, difficulty implementing and integrating newer systems, increased security exposures and risks, and an overall increase in the risk of system failures. This interest is paid daily over many years until the technical debt is retired by implementing the originally deferred functionality.</p><p>So how can a healthcare organization eliminate technical debt? The bad news is that for all practical purposes, it can 't. The existing balance of technical debt will typically be financially prohibitive to remediate as a project. And the normal course of IT work will introduce new debt continuously. But the good news is that an organization can reap significant benefits from actively managing its technical debt portfolio. To do this, an organization must do two things: address the current balance of technical debt, and improve processes to minimize the accrual of new technical debt.</p><p>To address current technical debt, key activities include:</p> <p><ul><li>Creating a technical debt catalog to identify areas of debt and ranking their impact on existing processes and practices. This is the  'balance sheet ' for technical debt.</li><li>Using the catalog as a backlog, pay-down of technical debt by funneling items of  'principal ' into the organization 's existing software delivery streams.</li><li>Reviewing and updating the catalog regularly.</li></ul></p><p>The technical debt catalog can be implemented without any special tools a table in a text document or spreadsheet makes a satisfactory catalog and can be maintained with little overhead. To populate the catalog initially, gather input from all areas of IT not just development areas about where they see friction within the technical infrastructure. Eliminate problems caused by broken processes or human error, leaving those that are truly technical debt. Assign an owner to each item and determine a priority or severity for it. Don 't spend too much time trying to make the initial list complete or over-documenting each case.</p><p>The technical debt catalog by itself won 't bring value unless it 's incorporated into the healthcare organization 's software development and delivery processes. To do this, it's necessary to assign at least a rough size and complexity to each item in the catalog to determine the best avenue to remedy it.</p><p>Items of debt that require a small amount of work and are isolated to a single system can typically be merged into regular maintenance and update delivery cycle for that system. Each system support group should mine the technical debt catalog for items that can be placed into their system enhancement backlog, where they can be prioritized alongside business enhancements and regulatory changes. These debt items are then removed from the catalog as they are addressed.</p><p>Items of debt too large to address through regular system enhancement processes should be addressed within projects. Debt items can sometimes be added to current or planned projects if they fit naturally into the project scope and don't have an excessive impact on timeline or budget.</p><p>However, if it can't be broken down into smaller pieces some items of technical debt will require a dedicated project to address. Large items of technical debt like this are often the hardest to retire. Healthcare organizations are reluctant to allocate precious project resources to what is seen as 'clean up' work. But a valid business case can be made by highlighting the real cost of the  'interest payments ' required by a large piece of technical debt.</p><p>In addition to tackling existing technical debt, healthcare organizations must look at how new debt is created. Not all technical debt can or should be avoided. It can be prudent to accept certain amounts of technical debt when time-to-market takes priority over completeness. But this decision should be made with full consideration of the interest the technical debt is likely to accrue. Reckless technical debt is avoidable, however, and can be reduced through developer training and quality control processes such as architectural reviews.</p><p>Healthcare organizations that tackle technical debt can optimize their effort if they follow a few guiding principles:</p><p><ul><li>Keep the debt catalog simple to understand and maintain. Aside from giving each item a name, description and rough size, it 's typically sufficient to assign a responsible individual or department, along with the date the item was added to the list.</li><li>Technical debt should not be viewed as the problem of one particular group or department - it needs to be a shared responsibility between IT and business areas.</li><li>Assign responsibility for the catalog to a team that participates in the software and project initiation processes. An Enterprise Architecture team is often a good choice.</li><li>Review and update the list no less than twice a year.</li><li>Don't restrict the technical debt catalog to software items only. Technical debt can accrue in infrastructure as well, for example where failover or backup capabilities are omitted from initial installation.</li><li>Above all, technical debt cannot be delegated to  'when we have spare time ' priority - there is never spare time.</li></ul></p><p>Like financial debt, technical debt is not a pleasant issue to grapple with and resolve. It is, however, a critically important issue that ignored can lead to harsh consequences like inefficient systems, costly and laborious maintenance cycles, and, in the worst case scenarios, the inability to migrate to modern system platforms. That said, it is possible for healthcare organizations to minimize technical debt 's negative impacts with a little proactive planning and follow-through.</p>",
    "tags": ["technical debt", "healthcare"],
    "nextMindShareId": "four-ways-to-build-effective-agile-teams",
    "nextMindShareTitle": "Four Ways To Build Effective Agile Teams"
  },
  {
    "id": "four-ways-to-build-effective-agile-teams",
    "title": "4 Ways to Build Effective Agile Teams",
    "authorName": "Darius Cooper",
    "authorTitle": "Architect",
    "authorImageUrl": "darius-cooper.jpg",
    "shortDescription": "The objective of this approach is to leverage the benefits of an agile approach to software development and blend those benefits with the advantages of a centralized group of specialists.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "07/05/2018",
    "readTimeInMinutes": 5,
    "publishName": "Digital Insurance",
    "publishUrl": "https://www.dig-in.com/opinion/4-ways-to-build-effective-agile-teams?brief=00000159-faf0-d111-af7d-fbfd9f0a0000",
    "content": "<p>In part one of this series, the focus was on the advantages and disadvantages of creating a separate and distinct team for any project 's integration work, as opposed to having integration work as part of a holistic agile team. That begs the question: How can one remain true to agile, while also bringing in the technical depth and the  'flexible coupling ' that come from a separate team?</p><p>A productive approach is to have each sprint team focus on its integrations but to also have integration specialists as members of that team to bring their knowledge of the technology and the standards necessary to ensure a robust architecture. Technical specialists who have seen similar types of integrations before, and who are familiar with standard approaches and patterns, can help immensely. It means that subsequent project teams do not have to reinvent the wheel to tackle problems for which there already are good patterns and tools. In the context of this example, the integrations specialists should join the core agile team, and work as part of the single team.</p><h1>Center of Excellence</h1> <p>A good way to approach this is to establish an Integrations Center of Excellence, staffed with integration specialists. One role of this team is to choose the tools, provide the advice and create the organizational standard around integrations. However, purely  'staff ' teams can sometimes become ivory towers, imposing their ways upon  'line ' teams. The best specialists learn by doing.</p><p>In an agile environment, they should actually apply their knowledge to situations that are very concrete and specific to the organization. They need to base their work on the day-to-day work of the teams that are creating the actual integrations, as opposed to creating arbitrary and abstract one-size-fits-all standards.</p><h1>Embedded Experts</h1> <p>Another best practice for this approach and a good way to keep the integration specialists grounded while also ensuring buy-in from both sides is to embed some specialists from the Center of Excellence into the sprint teams developing the specific integrations. That way they become members of the sprint team. Another benefit is that this is in keeping with an agile approach to an organizational structure: instead of creating another silo of specialists, form the teams tactically, pulling in specialists as needed.</p><p>In this way, the Center of Excellence becomes a fluid organization rather than a sizeable static team, but is also the coordinator of a more comprehensive 'Interest Group.' Only a few members are in the separate team in the Center of Excellence at any point. They may be creating organization-wide standards for integrations, or choosing specialized tools, or developing tools that can be useful to individual teams. Meanwhile, integration specialists are spread across the project or program, working in various development teams.</p><h1>Guilds</h1> <p>Another idea that works well with this structure is the idea of a guild, whose members belong to various and different teams, but who do similar work in those teams.</p><p>For example, the Quality Assurance resources or the Business Analysts from across the organization may be members of guilds, where they meet to discuss common problems faced and the solutions devised and implemented. Another benefit is that an  'Integrations Guild ' becomes an effective mechanism for sharing ideas and knowledge about the special problems faced within and amongst the teams. It also helps to create an informal network of co-workers who can call on each other for help, resulting in shared knowledge, less re-invention of the same solutions, quicker learning curves for new members and higher productivity.</p><p>The Center of Excellence can help coordinate the activities of the guild and to create common approaches and standards, but the day-to-day integration work is done by members who belong to various other development teams.</p><h1>Phased Approaches</h1> <p>In the early design and development stages of a project, there is usually a greater need for specialists who have experience with certain types of issues, and who can create project-specific patterns and approaches.</p><p>That need for specialists is reduced as the project progresses and the project team tackles a more extensive variety of problems with the patterns put in place by the specialists. At the start of any project, the specialists may be intimately involved with the project team in developing the initial integrations. Once the project is underway, however, and with established approaches and patterns of work, the specialists can shift their time and energy to helping keep things on track, introducing new technologies as appropriate, and with the inevitable tough problem that will crop up from time-to-time.</p><p>In this way, the direct involvement of the Center of Excellence will be gradually reduced. As the project moves along, other developers may find themselves interested in the specific technologies they are working with, and as a result, they may wish to join the guild, creating a valuable pipeline for future integration specialists.</p><h1>Blending Goals</h1><p>In summary, the objective of this approach is to leverage the benefits of an agile approach to software development and blend those benefits with the advantages of a centralized group of specialists, in this case, integration specialists. To that end, the approach aims to blend both long-term and short-term goals. It leverages specialist knowledge, while also creating accountability on the part of the specialists for the delivery of well-architected software solution. It also enables a core, but more refreshed team of specialists to look at the bigger picture, across the organization, while at the same time keeping them grounded in the requirements of real projects and, thus, close to the action. This creates a blend of strategic vision and tactical implementation, and in the process sets a context for success.</p>",
    "tags": ["Agile", "integration"],
    "nextMindShareId": "living-in-a-devops-world-part-two",
    "nextMindShareTitle": "Living In A Dev-Ops World (Part Two)"
  },
  {
    "id": "living-in-a-devops-world-part-two",
    "title": "Living in a DevOps World (Part 2)",
    "authorName": "Dave Farinelli",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "dave-farinelli.jpg",
    "shortDescription": "DevOps was created to more seamlessly connect the development and operations processes, making code migration, testing and promotion to production a more efficient process.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "07/03/2018",
    "readTimeInMinutes": 7,
    "publishName": "DevOps.com",
    "publishUrl": "https://devops.com/living-in-a-devops-world-part-2/",
    "content": "<p>Part one of this article focused on some of the more behind-the-scenes benefits of an Agile DevOps approach. In part two the focus turns to some of the other traditional problems that a well-executed DevOps approach can address, and how doing so can benefit an organization in more ways than just a technical perspective.</p><p>By way of quick review, DevOps was born out of the Lean and Agile software development methodologies when it became clear that, while those methodologies did indeed speed up the development process, a bottleneck still occurred when push came to shove and new code had to be moved to quality assurance and production environments.</p> <p>DevOps was created to more seamlessly connect the development and operations processes, making code migration, testing and promotion to production a more efficient process. To accomplish this, the DevOps approach had to find solutions for some of the issues that caused operational delays, and create new ways to organize, implement and continuously optimize the operations process.</p> <h1>Overproduction/Overprocessing</h1> <p>For those who have been in development and/or operations for any length of time, it quickly becomes clear that there is a multitude of operational safety checks that serve to protect a production environment. While that is vitally important, it was also clear that there had grown an  'over ' problem around many operational procedures, and in many cases that manifested itself in the development process. That includes overproduction, when making or requesting more than was needed from requirements and/or operations perspective to clear arbitrary operations process hurdles.</p> <p>Alternatively, overprocessing, when development and operations resources do more work (as opposed to just enough, as Lean and Agile would suggest) than required to smooth the transition of code and functions from development to operations. This created waste regarding time, resources and budgets that were not proportional to the benefits derived from following the operations process.</p> <h1>Motion and Transportation</h1> <p>Similarly, DevOps also sought to solve the operational problems of both motion and transportation. That is, the amount of excess work required to deliver new code to meet the operational requirements for code migration. The friction caused by such requirements slowed the motion and momentum of the development process. The same is true of transportation, or the difficulty in moving code between environments such as testing, quality assurance and production.</p> <p>In both cases, development and project momentum was sacrificed for what often turned out to be a series of artificial hurdles that had long since become less effective or even obsolete parts of the operations process.</p> <h1>Correction and Inventory</h1> <p>In most instances, all of the above resulted in the final maladies of the pre-DevOps development and operational ways. The first was the number of in-flight corrections required when timelines were squeezed, and the rush was on to get to production. Unfortunately, this went hand in hand with the ultimate problem of good code being sacrificed for expedient delivery, often resulting in inadequate functionality, system outages and, in the end, lost market opportunity and revenue.</p> <h1>3 Keys to DevOps Success</h1> <p>Any successful DevOps implementation must address three critical factors in this order: culture, organization and tools.</p> <h1>Culture</h1> <p>It 's critically important to connect an organization 's values to the DevOps process. Valuing quality, timeliness and organizational alignment of goals and objectives is the first step toward DevOps success. Such cultural values translate directly into a DevOps organization.</p> <p>Providing empowerment and accountability to DevOps team members helps to build ownership among the team, and trust from their customers in the rest of the organization. It also helps to provide a physical environment that fosters collaboration, teamwork and continued learning. Co-working spaces and collaboration tools such as Slack are a good start. Attending external conferences to broaden perspectives and to bring new ideas back to the team is often beneficial. From there, brown bag lunch sessions where ideas and experiences can be shared, frequent post-mortems on implementations to hone best practices, and even internal mini-conferences where several departments come together for a day to discuss DevOps practices are all effective ways to build a strong DevOps culture.</p> <h1>Organization</h1> <p>Any good DevOps organization is two-sided; that is it has to work from the top down and from the bottom up at the same time.</p> <p>The top-down part is in the ability to  'see the system ' from a macro level, allowing for process understanding and insights from a business workflow perspective. This helps to identify the pain points and bottlenecks in the current process that can be optimized through the DevOps process.</p> <p>Once that 's accomplished, the bottom-up work begins. Identifying things such as inconsistencies in code deployment environments that cause delivery issues, elimination of manual and custom built deployment processes and quarantining inefficient and poorly written code until it can be redone or eliminated are all part of optimizing the time, quality, resources and success factors for deploying production systems on schedule. It 's also important here to continually audit the current processes with an eye toward eliminating the processes that are no longer required or useful but have been kept in place out of the fear of   'breaking something we don 't understand. ' If nobody understands it, then it shouldn 't be in production software.</p> <h1>Automation Tools</h1> <p>The final factor for DevOps success is to have the right toolset.</p> <p><b>Communication</b>: Any DevOps team requires the ability to quickly and directly communicate with other team members sans meetings. For this purpose, tools such Slack (real-time chat), Skype (video chat), and Confluence (for storing persistent information) are pretty good options.</p> <p><b>Planning, Monitoring & Consistency</b>: For the team 's planning needs, a tool such as Trello that can provide Kanban board functionality is worth a look. For issue tracking and monitoring of any system 's overall health, tools such as Jira and NewRelic respectively provide some good functionality. Likewise, consistency is vital in a DevOps world, and using automation to ensure that all systems are configured as desired across different environments is a crucial best practice. For this, a tool such as Ansible is worth a review.</p> <p><b>Integration & Deployment</b>: For continuous integration of systems in development and as a way to tighten the feedback loop for developers to determine if the central build used for deployment to production is working as intended, the Jenkins toolset might be a good fit. And finally, when it comes making any deployment process as painless as possible, a tool such as Docker that can handle created containers for an application that includes all dependencies, reducing the complexity of deployment to multiple environments, is a solid way to go.</p> <p>The point of all of this is to create an environment culturally, technically and physically where DevOps can succeed, grow and thrive. Organizations that can create an effective and efficient DevOps environment have also created a competitive advantage for themselves.</p>",
    "tags": ["devops", "agile"],
    "nextMindShareId": "living-in-a-devops-world-part-one",
    "nextMindShareTitle": "Living In A Dev-Ops World (Part One)"
  },
  {
    "id": "living-in-a-devops-world-part-one",
    "title": "Living in a DevOps World (Part 1)",
    "authorName": "Dave Farinelli",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "dave-farinelli.jpg",
    "shortDescription": "The first step on the journey to effective DevOps is the recognition that the concept is the result of the rise of the Lean and Agile software development methodologies.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "06/05/2018",
    "readTimeInMinutes": 5,
    "publishName": "DevOps.com",
    "publishUrl": "https://devops.com/living-in-a-devops-world-part-1/",
    "content": "<p>The concept of DevOps has grown and evolved into a conceptual and working model for more effective and efficient software development and implementation. That said, there are some differences of opinion on the real-world value of any DevOps approach to date, and on the best way to create and implement a real-world DevOps environment. This two-part article will focus on what an agile DevOps approach is meant to address, and what it is not meant to address.</p><p>DevOps sits at the nexus of three essential business technology functions: software development, quality assurance and operations. A short and concise definition of DevOps proposed in 2015 seems as appropriate as any:</p> <q>DevOps is a set of practices intended to reduce the time between committing a change to a system and the change being placed into regular production while ensuring high quality.</q> <p>The definition was suggested in the book,  'DevOps: A Software Architect 's Perspective, ' and the authors have hit upon the essence of the practice. The key, of course, is how to put that concept into practice.</p> <p>Perhaps the first step on the journey to effective DevOps is the recognition that the concept is the result of the rise of the Lean and Agile software development methodologies. Those methodologies, among other things, emphasize the following:</p>  <p><ul><li>A focus on customer value.</li> <li>The elimination of waste.</li> <li>Reduced cycle time (accomplishing work faster, releasing faster).</li> <li>Shared learning.</li> <li>Avoiding batching (don 't do things until required).</li> <li>Theory of constraints (break things up, focus on individual issues).</li> <li>Continuous integration, testing and delivery.</li> <li>Faster time to market.</li>  <h1>DevOps in Practice</h1> <p>Adherence to the principles above meant that something had to be invented to accomplish them and that something was DevOps. Over time, an effective DevOps practice should address any number of business technology pain points. The following short list of those pain points and the DevOps response should prove instructive.</p> <h1>System Downtime</h1> <p>The developers ' curse since systems were developed, system outages are inevitable as long as systems are designed, tested and implemented even with increased automation by imperfect beings. DevOps acknowledges that by changing the focus from trying to create applications that never fail to designing systems that can recover quickly, thus decreasing aggregate systems outage time over the life cycle of any application or system.</p> <h1>Stagnation</h1> <p>This was a staple of traditional systems development and is most closely associated with the waterfall methodology for systems development. After requirements were created, the development team would be locked away for weeks, months or, in some cases, years before emerging with  'fully ' working software that inevitably no longer satisfied rapidly evolving business requirements. DevOps is designed to fit hand-in-glove with the Agile practice of short windows of incremental changes instead of long release cycles, putting working software in the hands of customers as quickly as possible.</p> <h1>Team Conflict</h1> <p>Having been borne from the cultural combination of Agile and Lean, DevOps has taken on the problem of functional silos that are often erected between development, operations and the business customers. It follows the methodological approaches of collaboration and teamwork first to understand what others know and to leverage the best of it to solve business problems more rapidly. There is also a cultural bent toward experimentation, continual learning and constant improvement. This leads to blameless post-mortems, where instead of finger pointing when something goes wrong there is collaborative discussion and learning to correct and prevent the problem from occurring again.</p> <h1>Knowledge Silos</h1> <p>Functional silos have led to compartmentalized knowledge. If the old game was that knowledge is power, the new game in the DevOps world is that knowledge is freely exchanged as an enabler to solving business problems. DevOps addresses the problem of information being lost in translation between the development and operations functions by eliminating the functional barricades and making knowledge sharing the highest form of collaboration.</p> <h1>Inefficiency</h1> Waiting for things to happen used to be a standard operating procedure in the pre-DevOps world. Project plans were created and managed to account for the time it might take for new code to be moved into a testing, quality or even production environment. This was a momentum killer for projects and at times a morale killer for developers waiting to see what changes they might need to make to their code set. The combined Agile and DevOps approach has rewritten the traditional approach to code migration, smoothing and eliminating wait times so that projects flow more seamlessly from start to finish. It also has the benefit of keeping business resources testers, approvers, etc. more engaged as a result of a constant flow of new functions and features to test and use. And lest this be viewed as simply a way to keep the technical stuff moving along, it 's important to remember that there is a financial aspect to this as well. Reducing speed to market with new functionality, reducing or eliminating idle hands be they technical or business and delighting customers with a steady stream of enhancements and features all go directly to an organization 's top and bottom lines.</p> <p>That, after all, is in many ways what the DevOps approach is all about. All of these critical areas become the means to accomplish it. Part two of this article will focus on some more of the benefits of a DevOps approach, and how to achieve them.</p>",
    "tags": ["devops", "agile"],
    "nextMindShareId": "how-to-integrate-systems-more-efficiently",
    "nextMindShareTitle": "How to Integrate Systems More Efficiently"
  },
  {
    "id": "how-to-integrate-systems-more-efficiently",
    "title": "How to Integrate Systems More Efficiently",
    "authorName": "Darius Cooper",
    "authorTitle": "Architect",
    "authorImageUrl": "darius-cooper.jpg",
    "shortDescription": "Any large software program has tentacles that stretch across the entire insurance organization. These dependencies are often undocumented and overlooked.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "06/01/2018",
    "readTimeInMinutes": 3,
    "publishName": "Digital Insurance",
    "publishUrl": "https://www.dig-in.com/opinion/how-to-integrate-systems-more-efficiently?brief=00000159-faf0-d111-af7d-fbfd9f0a0000",
    "content": "<p>As many insurers have discovered, the risks entailed in core system modernization are not what they used to be. There are still plenty of risks, of course, but in many cases these have less to do with new software implementation and more to do with systems integration.</p><p>Any large software program has tentacles that stretch across the entire insurance organization. These dependencies are often undocumented and overlooked until such time when a new system is rolled out and promptly begins to break other systems in unexpected ways. To correct and prevent this, the integration effort may require as much work as the new core system itself.</p><p>How then to achieve these integrations in the most efficient manner possible?</p><p>One byproduct of all the core system modernization efforts underway is that many specialized integration tools are now available. And, to take advantage of them, many insurers have established an 'integration team' that groups related technical specialists together.</p><p>This approach, however, can cut against the business view of the project. From a business standpoint, what the user sees on a front-end screen is the visual representation of one or more business processes or user stories, and this is where some integration is often required. But this requires great coordination and communication. To avoid this, the agile approach emphasizes treating the user-story as a cohesive unit.<p><p>But a good systems integration approach is to combine these strategies. This keeps the integration work close to the other work being done for the user story, while also bringing specialized knowledge to bear.</p><p>Let me explain in a little more detail.</p><p>When a team works independently on a project, the structure of the software they produce will often reflect that separation. This is sometimes called Conway 's Law: that software structure often reflects organizational structure.</p><p>Likewise, when the team working on a user-story does integration work, there is a risk that the integration software will also reflect that team 's orientation and assumptions.</p><p>Having a separate team develop the integration software can lead to more independently structured and universally applicable integration.</p><p>However, this approach also comes with a risk: The generic solutions created by a more independent team can fail to fully support the different user stories and require costly rewrites.</p><p>Instead of creating a separate integrations team, one way to avoid both types of problems is to treat the integration software as an extension of the business process software with an adaptable layer between one system and another. In effect, this approach creates an integration layer between the core system and the insurer 's other systems, even though the software is developed by the core team. This helps to mitigate the adverse effects of Conway's Law. And the dedicated integration team can provide a single point of contact with the rest of the organization, thus reducing the risk of any communications falling through the cracks.</p><p>In a follow-on article, I 'll explore further how the two approaches can be successfully wedded.</p>",
    "tags": ["modernization", "integration"],
    "nextMindShareId": "analogical-storytelling-approach-to-technical-communication-barriers",
    "nextMindShareTitle": "Analogical Storytelling Approach to Technical Communication Barriers"
  },
  {
    "id": "analogical-storytelling-approach-to-technical-communication-barriers",
    "title": "Analogical Storytelling Approach to Technical Communication Barriers",
    "authorName": "Mohammed Hussain",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "mohammed-hussain.jpg",
    "shortDescription": "Data technologists are often challenged with effectively communicating complex technical information to the non-technical side of the business.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "05/07/2018",
    "readTimeInMinutes": 5,
    "publishName": "Becker's Health IT & CIO Report",
    "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/analogical-storytelling-approach-to-technical-communication-barriers.html",
    "content": "<p>Data technologists are often challenged with effectively communicating complex technical information to the non-technical side of the business to gain approval for the funding, the resources, the priorities, or all the above for any given data strategy.</p><p>This situation is common practice, but being able to conceptually explain the strategy is where good storytelling comes into play. To better elaborate, consider the following story.</p><p><i>Suppose you are in the year 300 BC and you need to create a way to collect, store and organize data. How do you do it? Well as a first step, you'll likely learn the language used to communicate and gather information, particularly in its written format. With this knowledge in hand, you 'll be ready to start writing. To accomplish this, you 'll acquire some papyrus or parchment and some ink (made from the local materials available) and begin recording information. When you run out of page space on one page, you'll start to create more pages. When the number of pages gets large enough that it becomes difficult to find a particular page quickly, you 'll give each page a mark or a number to make it easier and faster to access the information you want. And when the amount of marked or numbered pages becomes too cumbersome to balance on your lap or to spread out on a table, you 'll create a book by binding the pages together.</p><p>As you create and acquire books, you find that you have to start organizing books by subject to make it easier to store and retrieve. The math information goes in the math book, and the math book is placed with other math books. Likewise, the biology information is placed in the biology book, the geography in the geography book, and so on. When you have amassed too many books to put on the table or the floor, you 'll build a bookshelf to hold them all. When you run out of bookshelf space, you 'll build another bookshelf to hold all of your books. As the number of books you 've acquired increases, you begin to organize them by sub-topics within subjects to make them easier to locate and access.</p><p>Up to this point you 've done an effective job of organizing your two bookshelves worth of books - so far so good. But what happens when you have a hundred bookshelves full of books? As the number of similar subjects grows, organizing simply by subject and even sub-topic within the subject is difficult, so you decide to organize not only by subject but also by author. However, at one hundred bookcases and growing, it can still take a while to locate a specific book, so what do you do? You decide to number each of the one hundred bookshelves and create a separate filing system that tells you which book is on which shelf. To help further, you also label the individual shelves so that your filing system includes not only the bookshelf number but also the individual shelf any book is on. You could expand your filing further and maybe add a different filing system that tells you where books are by genre. Or maybe reading level. Or whatever other organization you can come up with so long as it makes the task of locating the information you want as efficient and effective as it can be. Congratulations are now in order, as you 've become one of the creators The Library of Alexandria c. 300 BC and your data is stored and easy to find.</i></p><p>If you followed that story you now have a basic understanding of a simple data strategy along with the rationale for why it was required, and some of the insights surrounding the intuition behind why it was built the way it was built.</p><p>To connect this back to modern software, think of the Book as the data entity or record, the Bookshelf as the data structure representing and storing the data (an array or a list maybe), the collection of bookshelves as representing your Database, and the filing system as the Database Index. We can expand this analogy even further to say that in your library you can check out a book so that someone cannot change it while you're reading it, or that two people cannot change it at the same time. And, as your library grows it will require regular updates. For example, if you add a new book to your library, your filing system needs to be updated to record all of the particulars of the new book. That will protect against the complications of running out of shelf space and having to move all of the books the filing system will remain accurate.</p><p>In my experience, a quality storytelling approach has proven to be an effective way to convey the technical ideas involved and to get some interesting discussions going as a byproduct. By telling a non-technical but relatable story, a technologist can help explain what data is and how it's generated, as well as what it means to create a data strategy and why it's both necessary and important.</p><p>And while it might not come naturally to many technologists, the happy fact is that stories and storytelling is a part of who we all are, the technical and non-technical alike. It's a part of human history and the human experience, uniquely so in fact, and as such provides an instantly recognizable format. Of course, if it were as simple as it sounded everybody would do it. The truth is that like anything else it requires practice and persistence, but once mastered, storytelling can provide huge dividends in communications and relationship building. The other requirement is finding the right story for the right technical concepts presented, which also requires practice and persistence.</p><p>The introductory story takes between five and ten minutes to tell. The overall idea is that the story represents a relatively succinct and relatable way for understanding something very complex essentially giving non-technical listeners the conceptual building blocks to help understand complex problems. This method of formulating a creative analogy for a technical problem is foundational to any successful consulting career. It is a shockingly successful way of getting buy-in from business users by helping them understand a topic to the level that a new developer would, for example, without the explicit understanding of the underlying technical details.</p><p>The art of analogical storytelling accomplishes a few things: first and foremost it captures the audience 's attention - everyone likes to hear a story. Second, it uses a common and easily understood format that can be visualized by each listener, increasing the possibility of retention and understanding. And finally, it can be seamlessly connected back to its technical counterpart.</p><p>Good stories are worth their weight in gold, and like any other skill, it is something that requires practice and forethought. So the next time you're struggling to explain something to someone, try telling a story.</p>",
    "tags": ["technical communication", "data"],
    "nextMindShareId": "new-life-insurance-delivering-a-next-gen-experience-to-the-next-gen-applicant",
    "nextMindShareTitle": "New Life Insurance: Delivering a Next-Gen Experience to the Next-Gen Applicant"
  },
  {
    "id": "new-life-insurance-delivering-a-next-gen-experience-to-the-next-gen-applicant",
    "title": "New Life Insurance: Delivering a Next-Gen Experience to the Next-Gen Applicant",
    "authorName": "Samir Ahmed",
    "authorTitle": "Principal",
    "authorImageUrl": "samir-ahmed.jpg",
    "shortDescription": "Delivering the complete next-gen customer experience requires assembling simple technology building blocks into a sophisticated and well-engineered solution.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "04/11/2018",
    "readTimeInMinutes": 7,
    "publishName": "Insurance-Canada",
    "publishUrl": "https://www.insurance-canada.ca/2018/04/11/xby2-next-gen-experience/",
    "content": "<p>The life insurance industry has been abuzz with improving the customer experience for the past few years, particularly in the new business and underwriting processes. It might seem like a tired topic to revisit in 2018, but the number of carriers that have yet to (fully) implement capabilities such as end-to-end electronic application, end-to-end eSignature, straight-through policy issuance, and eDelivery of policy, just to name a few, indicates otherwise. What is behind the low adoption rates for such capabilities? And more importantly, what can be done to drive higher adoption?</p><h1>Current Applicant Experience</h1> <p>The current life insurance applicant is likely to be a Gen X or Millennial - and will soon be a Post-Millenial. Her buying experience leaves her frustrated and dissatisfied. As she does whenever she is faced with any new purchase, she starts on the internet. She quickly finds a website that offers life insurance quotes. This is where her experience first starts to deviate from her expectations. Instead of providing a quote, the website facilitates a phone call with an agent. Disappointed, the applicant provides her phone number and indicates a convenient call time. The agent calls and offers to visit her at her house and walk her through everything. The applicant is a bit taken aback. She wasn 't expecting to have to work with someone to get a life insurance quote, let alone invite that person into her home, or go to his or her office. In her view of the world, this is not how things get done.</p><p>When the agent arrives at her home, he starts by asking her to fill out an insurance  'Needs Assessment ' questionnaire that is seven pages long. It all looks simple and straight-forward, yet, as she begins filling it out, she notices that the questions become progressively more intrusive. It starts with demographic information, such as her name, address, and occupation. That is followed by detailed questions about her monthly budget, including all her income and all her expenses broken down by category. Finally, it rounds out the assessment by asking about assets, liabilities, financial goals, and expectations for final expenses, debts and income replacement. Using the collected information, the agent prepares a few proposals, walks the applicant through them while answering her questions along the way. He also gives her a 41-page packet containing the insurance application and several associated forms. He asks the applicant to review the proposals and to fill out the application packet based on the plan she likes. He offers to return in a few days to take care of signatures and payment and to collect the paperwork for submission to the carrier.</p><p>Her experience continues in this manner, inclusive of documentation follow-ups, family medical histories of which she knows little, a visit from a nurse, and it culminates with the agent telling her that the 41-page application packet is ready for evaluation and that she 'll hear something in the next 90 days or so. To the applicant, the 21st century this is not.</p><h1>The Next-Gen Experience</h1><p>These sentiments held by the biggest pool of potential life insurance buyers are well known in the life insurance industry; it 's not a surprise to anyone.</p><p>It 's equally well known that what these applicants desire instead is an experience that flows like this:</p><p><ul><li>24/7 self-service on devices of their choosing with a seamless transition from one (e.g., mobile app) to another (e.g., desktop web browser);</li><li>interactive questionnaires presenting a few questions at a time and tailored based on answers already provided;</li><li>being asked the breadth of information needed during the application process (not as follow-ups during the evaluation process);</li><li>comparison of products, coverages, premiums, etc.;</li><li>review of the final application packet before signing it;</li><li>electronic signature;</li><li>electronic payment at the time of signature;</li><li>an immediate decision, with an explanation in cases when the application requires further evaluation, followed by regular notification of its status; and</li><li>electronically delivered documents, including the issued policy.</li></ul><p>When both the source of frustration and the pathway to delight are known, why the low adoption rate on capabilities that matter most to this newest generation of consumers? The short answer is that while there is a simple to understand and implement technology solution for each element of the desired experience, delivering the complete experience requires assembling simple technology building blocks into a sophisticated and well-engineered solution. That might sound simple, but it is not easy and is what bedevils the industry.</p><p>The technology components that need to be assembled consist of the following:</p><p><ul><li>A modern user interface development framework that supports web, tablet and mobile access.</li><li>A reflexive question engine that can determine what questions to ask based on answers already provided.</li><BLOCKQUOTE><li>It requires codification of all application evaluation rules, including New Business, Compliance and Underwriting</li></BLOCKQUOTE><li>A document generation system to present electronically completed application packets for review.</li><li>eSignature and ePayment</li><li>A system integration platform that facilitates:</li><BLOCKQUOTE><li>Real-time communication with information sources, e.g., MIB, Rx history, MVR, etc.</li><li>Real-time appointment scheduling with evidence providers, e.g., paramedical exam, labs, tele-interview, etc.</li></BLOCKQUOTE><li>An underwriting rules engine, to codify underwriting rules, and provide real-time risk assessment with stratification by statistical confidence intervals.</li><li>ePolicy and eDelivery</li><li>Policyholder portal for receiving application status, viewing the issued policy and securely communicating with the agent and the insurer.</li><li>Agent portal to view the status of submitted applications, and securely communicate with applicants and the insurer.</li></ul></p> <h1>Conceptual Solution</h1><p>The following diagram illustrates a logical assembly of the necessary technology components into a conceptual future state for a typical life insurer:</p><img src='/assets/mind-share-img/samir-image-1.jpg' /> <p>Such a conceptual solution might seem daunting. Fortunately, the software architecture discipline provides a proven approach for accomplishing all of the above and more, which is to conceptualize the target state, acknowledge the current state, identify gaps between the two, outline a roadmap for closing the gaps, and then chip away at the solution one capability at a time. Care must be taken to ensure that each building block that adds functionality and capability on the back-end also enhances the front-end experience of next-generation customers. Given proper prioritization of resources and budgets, all of this can be accomplished in two to three years.</p><p>Think big, start small, and move fast is the call of the hour. It 's not rocket science, but it does take considerable focus and persistence - something the industry has been demanding of its applicants for decades.</p>",
    "tags": ["customer experience", "application"],
    "nextMindShareId": "the-purpose-driven-project-approach",
    "nextMindShareTitle": "The Purpose-Driven Project Approach"
  },
  {
    "id": "the-purpose-driven-project-approach",
    "title": "The Purpose-Driven Project Approach",
    "authorName": "Jason Brown",
    "authorTitle": "Developer",
    "authorImageUrl": "jason-brown.jpg",
    "shortDescription": "One of the issues with the current technology tsunami is that it can be difficult for those working on IT initiatives to gain a fuller understanding of a project.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "04/02/2018",
    "readTimeInMinutes": 7,
    "publishName": "Becker's Health IT & CIO Report",
    "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/the-purpose-driven-project-approach.html",
    "content": "<p>Modern Information Technology work is all about outcomes.</p><p>Market pressures, competitive differentiators, customer service expectations and new capabilities are just a few of the reasons that insurers are sprinting to enable new technologies and processes that support their strategic goals and objectives. For people consulting and working in IT divisions that usually means long hours, shifting priorities and making decisions as quickly as possible with the information available. That 's all well and good, but a recent experience has suggested an alternative approach.</p><p>One of the issues with the current technology tsunami is that it can be difficult for those working on IT initiatives to gain a fuller understanding of why a particular technology project is being put together the way it is, as opposed to just having a rudimentary understanding of what the technology is supposed to be doing. That's an important distinction and one that deserves further consideration, as this article will attempt to do.</p><p>For most developers, architects, data administrators and others on IT projects, it can be very easy to fall into the trap of only understanding how something is supposed to work in a rush to get new capabilities into production. One sure sign of this can be the introduction of new tools and frameworks as part of a new project that is duplicative of the tools and frameworks already in use. This leads to technology tunnel vision, with IT resources having an even narrower understanding of a new project, focusing only on their piece of the technology pie for the effort.</p><p>Again, this is understandable given the short-term pressures of delivering, but it can be detrimental over the longer term in the context of developing well-rounded IT people who understand more than what something is supposed to do. The problems with this kind of approach are many and well documented, but a short list includes the likelihood of repeating patterns and mistakes from previous projects, IT people who are too narrowly skilled and focused on only what they 've done in the past, and an unwillingness to be open to new technologies and approaches that might be beneficial to any insurer. And a good deal of this can be attributed to focusing on the execution-driven issues (what and how), rather than the purpose-driven (why) business technology issues.</p><p>That said, there are a couple of ways to begin to break this cycle, but both will take a company 's commitment to altering at least some of its IT project approach. One is to allow the time on a new project to focus on the why, or purpose-driven questions: why that architecture, why that platform, why that technology stack, why that database approach, etc. And when time doesn 't allow such an approach (which might be almost always), an alternative and perhaps more practical approach is to allow the purpose-driven questions (as opposed to the execution-driven questions) to be considered on subsequent project rework, maintenance and upgrade cycles. There 's a risk to that, but there 's also a risk of being late to the market with new technology capabilities, so those tradeoffs must be considered.</p><p>Of course being part of a team considering the purpose-driven questions when implementing a project from the ground-up can provide valuable experience for IT developers and others. Among them are:</p><p><ul><li>Ingraining the importance of considering the purpose-driven questions before starting on any subsequent IT project</li><li>Providing time to evaluate technology and process patterns for the best fit for any particular project</li><li>Offering time to dissect previous decisions made about architecture and database approaches</li> <li>Giving IT people an opportunity to discover and discuss the things they don't know about technology or project</li></ul></p><p>As suggested earlier, recent experience on a project provided a valuable opportunity for asking such purpose-driven questions. The application had the same architecture and technology stack as the one from an immediately prior assignment. This provided a good opportunity to look at another application using the same technology stack, and evaluate more closely the why, instead of merely accepting the architecture and copying it blindly in the new project. Essentially the prior project could be used as a reference architecture comparison point leading to asking why it was designed as it was and why a specific approach was chosen for the project.</p><p>This proved invaluable from an educational and a qualitative perspective and forced the kind of thinking and re-evaluations to occur that often don't happen in the first go-round of IT projects. In this latest project, the opportunity to consider the purpose-driven questions proved essential for professional development and skills growth, and the ability to carry the lessons forward into future projects.</p><p>And from a more practical perspective, having more than just the architect understand why certain decisions have been made is a good hedge against sudden staff changes and critical knowledge walking out the door. When that happens, the use of ineffective patterns in the race just to complete the project at whatever the cost can quickly dilute the deliverables for any project. That said, the experience of being able to ask purpose-driven questions allowed for the reconsideration of the architectural approach rather than just accepting what was already in place. Asking the fundamental why questions also opened the doors to better problem-solving skills.</p><p>After all, architecture or technology that works well for one project may not be the right thing for the next project. It may seem obvious in hindsight, but seeing and understanding the patterns and architectures - along with the reasoning - behind something can easily and quickly get lost if there isn 't enough time to carefully think about an approach and ask questions about why things were done the way they were done. For this project, some of those why questions were things like:</p><p><ul><li>Why was this application designed the way it was?</li> <li>Is this application solving the problem it was created to address, and why is it or is it not doing it well enough?</li><li>Why was the architecture deployed chosen over others?</li><li>Is there a better approach or why should this application remain in place?</li></ul></p><p>Determinedly answering these questions has a direct impact on the overall quality of the project and its deliverables. In all, the purpose-driven approach can lead to benefits such as:</p><p><ul><li><b>Technology Agility</b>: knowing when and when not to use specific patterns</li><li><b>Maintainability</b>: hedging against IT staff changes by spreading knowledge</li><li><b>Upgradability</b>: designing and implementing with upgrades in mind</li><li><b>Integration</b>: understanding ahead of time prevents problems later</li><li><b>Skills Development</b>: overall IT staff gets better</li></ul></p><p>These and other benefits seem like as good as reasons as any to ask and answer the why questions. And while that might take a little more time up front, it will inevitably take several times longer to answer them after the project is in production.</p>",
    "tags": ["purpose-driven", "approach"],
    "nextMindShareId": "using-analytics-for-healthier-encounters",
    "nextMindShareTitle": "Using Analytics For Healthier Encounters"
  },
  {
    "id": "using-analytics-for-healthier-encounters",
    "title": "Using Analytics For Healthier Encounters",
    "authorName": "Oleg Issers",
    "authorTitle": "Architect",
    "authorImageUrl": "oleg-issers.jpg",
    "shortDescription": "To ensure data tells a comprehensive story, insurers are focusing on their claims analytics data, and particularly the idea of a Healthcare Encounter.",
    "isFeatured": false,
    "industry": "healthcare",
    "publishDate": "03/23/2018",
    "readTimeInMinutes": 7,
    "publishName": "Health IT Outcomes",
    "publishUrl": "https://www.healthitoutcomes.com/doc/using-analytics-for-healthier-encounters-0001",
    "content": "<p>It 's no secret that a new business model has been emerging for the healthcare industry over the last several years. From its early days at the end of the previous century, the Managed Care model has slowly but surely been making its way into every nook and cranny of an industry desperately in need of reinventing itself. The reasons for this are many, but at its core, the promise of a healthcare delivery system organized to manage cost, utilization and the quality of healthcare services is just too promising to ignore.</p><p>The Managed Care process has also introduced a very powerful concept to the industry: that of shared risk and shared rewards between healthcare providers and healthcare insurers. But while conceptually compelling, the actual implementation of the model in the real world of day-to-day healthcare in this country has proved daunting to say the least.</p><p>One of the key success factors for the successful implementation of the Managed Care model - and specifically for the shared risk and rewards approach - is the utilization of high-quality data and the analytics it can produce. Without the data to accurately verify patient, provider and payer performance the whole model becomes sub-optimized. To make sure that the data does, in fact, tell a comprehensive story, many providers and insurers are focusing on improving their claims analytics data, and particularly the idea of a Healthcare Encounter.</p><p>Simply put, a Healthcare Encounter is an interaction between a person (patient) and a healthcare provider. In the context of claims analytics as an example, providers are categorized as a professional (an individual medical practitioner) or a facility (a hospital, outpatient clinic, laboratory, etc.). Further, professional and facility providers submit different types of claims to payers (health insurance companies) for reimbursement. In the context of claims then, a Healthcare Encounter can be defined as a grouping of related claims for the same person, provider, payer and date range.</p><p>An example set of business rules for a Healthcare Encounter can help illustrate this approach:</p><img src='./assets/mind-share-img/oleg-image-1.png' /></p>Illustration: Grouping of Facility and Professional Claims to build an Inpatient Encounter</p><h1>Facility Inpatient Encounters</h1><p>These typically span multiple days. Encounter date range is built by grouping related Facility Claims (same Person, Facility Provider, Payer, Diagnosis, and adjacent or overlapping date ranges). Any Professional Claims for the same Person within the date range are linked to the Inpatient Encounter; there may be claims from multiple Professional Providers (Practitioners) linked to the same Facility Encounter.</p><p><ul><li>Skilled Nursing Facility or Nursing Home Encounter</li><li>Hospice Encounter</li><li>Home Health Encounter</li><li>Acute Inpatient Hospital Encounter</li><li>Inpatient Mental Health Encounter</li><li>Inpatient Rehabilitation Encounter</li></ul></p><h1>Facility Person-Day Encounter</h1><p>These follow the same business logic as Facility Inpatient but only span a single day - no merging of Claims with adjacent dates. An assumption is made that any Professional medical services the Person received on the same day are related to the Person-Day encounter.</p><p><ul><li>Observation Encounter</li><li>Emergency Room Encounter</li><li>Urgent Care Encounter</li><li>Outpatient Surgery Encounter</li></ul></p><h1>Facility Outpatient Encounter</h1><p>Facility Claims are grouped by Person, Facility Provider, Payer and Service Date. No merging of dates or linkage of Professional Claims.</p><p><ul><li>Dialysis</li><li>Physical Therapy</li><li>Occupational Therapy</li><li>Radiology</li><li>Lab/Pathology</li></ul></p><h1>Professional/Ambulatory Encounter Types</h1><p>Professional Claims are grouped by Person, Professional Provider (Practitioner), Payer and Service Date.</p><p><ul><li>Administered Drugs, including Chemo Drugs</li><li>Allergy Encounter</li><li>Cardiovascular Encounter</li><li>Surgery Professional Encounter</li><li>Among others</li></ul></p><p>The concept of a Healthcare Encounter is a powerful one. It starts to address the issue that many large providers encounter when accepting patient healthcare insurance from a multitude of insurers. The providers need a more efficient way to identify multiple insurance payers for the same patient.</p><p>One way to think about the concept of Healthcare Encounters is to imagine a  'day-in-the-life ' of a patient who has multiple outpatient appointments that might include a consultation, a procedure, blood work, physical therapy, etc., all from potentially different facilities and payers. The concept of the Healthcare Encounter pulls all these disparate encounters together into a more meaningful way to manage healthcare claims. Additionally, this concept goes right to the heart of the Managed Care model 's requirement for the kinds of analytics that can truly produce the quality risk measures needed to accurately and effectively assess provider performance.</p><p>There are, however, still obstacles to achieving the promise of the Healthcare Encounter concept. First and foremost, overall industry cooperation when it comes to data and analytics needs to improve. As of now, there is not a common and accepted set of data quality standards across the healthcare industry. Such standards are a critical part of the ability to create the kinds of advanced analytics required to support the dreams of the Managed Care model - inclusive of the Healthcare Encounter concept - in the industry. While progress has been made on optimizing and standardizing data code sets in the industry, there is still much work to be done.</p><p>The healthcare industry is fast approaching a critical inflection point. There is wide consensus that the Managed Care model remains the future of the industry, leading to better overall health, service, and satisfaction for patients, and better overall process, efficiency, and profitability for providers and insurers. The key to really achieving all of that is data analytics for verification and continual improvement. In this area, much work needs to be done.</p>",
    "tags": ["analytics", "encounter"],
    "nextMindShareId": "enterprise-architecture-in-an-agile-world",
    "nextMindShareTitle": "Enterprise Architecture In An Agile World"
  },
  {
    "id": "enterprise-architecture-in-an-agile-world",
    "title": "Enterprise Architecture In An Agile World",
    "authorName": "Rob McCurley",
    "authorTitle": "Architect",
    "authorImageUrl": "rob-mccurley.jpg",
    "shortDescription": "It has become common to view enterprise architecture as something that time has passed by; however, this view is misguided and potentially risky.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "03/14/2018",
    "readTimeInMinutes": 5,
    "publishName": "ITA Pro",
    "publishUrl": "http://www.emagazine.itapro.org/Home/Article/Enterprise-Architecture-in-an-Agile-World/2100",
    "content": "<p>It has become a common refrain over the past few years to view the practice of enterprise architecture (EA) as something that time has passed by, much like using email or making actual phone calls on a smartphone. The ascent of agile methodologies and practices has seemingly relegated architectural concepts to the dustbin of history.</p><p>This article will argue that the suggestions that EA 's time has past are misguided at best, and potentially risky at worst to companies that view it in that context. That said, the sad truth is that EA has been misunderstood and misinterpreted over many years, and has earned its less than stellar reputation through poorly organized, designed and executed architectural efforts.</p><p>In many insurers, EA has mostly failed to live up to its potential and promises. The reasons cited for this failure, familiar and in many cases justified, include:</p><p><ul><li>EA is too methodical and process-heavy when agility is needed</li><li>EA focuses on technology perfection rather than business practicality</li><li>EA focuses on governance rather than business technology enablement</li><li>Architects live in  'ivory towers ' and avoid issues of day-to-day IT</li></ul></p><p>If these reasons are valid - and they mostly are - then it 's hard to escape the conclusion that EA is not only unnecessary, but it may even be viewed as a negative proposition. If you dig deeper, however, one finds that these are not failures of EA per se, but instead, they are failures of practitioners to adhere to the principles and concepts of EA itself.</p><p>The purpose of EA is to enhance the interplay between technologies and business processes to better support and enable business needs and goals. When architects move away from this fundamental value proposition, they lose relevance to IT and among business leaders. Architects can regain relevance by refocusing their practices around two principles.</p><p>The first principle of EA is the linkage of architecture and business. Architects often act as if their problem domain is technology. It 's not. Architects, like everyone else in the company, are tasked with making the business more successful. Architecture teams currently misaligned with the business goals can realign themselves by making a few key adjustments:</p><p><ul><li>Understand the Business: Architects must make time to learn about the non-technical aspects of their company's business. They should spend time observing business processes in action, and discuss business challenges and opportunities with business management at every level.</li><li>Communicate in Business Terms: Architects sometimes project an uber-nerd persona. This can alienate business colleagues and even senior IT staff. Architects need to take their understanding of the business and communicate regarding solutions to business problems - reducing costs, lowering headcount, enhancing business processes, enabling customers, etc. They should use instructive analogies and stories. With self-awareness and coaching, architects can learn to communicate more effectively.</li><li>Measure Architecture in Business Terms: More evolved architecture teams maintain metrics about their practice. But often these metrics will seem artificial or useless to outsiders. Architects should examine metrics they collect and affirm that they provide business-relevant information. Not that every parameter must relate directly to the corporate bottom line. Metrics about technology are fine as long as they are framed to give insight into how it supports the company.</li></ul></p><p>Shifting to a business orientation doesn 't necessarily force a wholesale overhaul of an architecture practice. The key activities of analysis, modeling and technology governance can continue, but they need to be reassessed through the lens of business success. Consider pruning activities that are hard to justify through this perspective.</p><p>The second principle the modern architect should focus on is to embrace agility. Agility is used to mean different things, but overall it reflects the fact that everything moves at a much faster pace than it did a few decades ago.</p><p><ul><li>Support Business Agility: Accept that different parts of any insurer 's business run at different speeds. When speed to market is important, architectural standards can be relaxed. Prioritize modernization efforts around agility.</li><li>Support IT Agility: Don 't resist agile development methodologies - adopt a lighter touch to integrate with them instead. Be selective about the artifacts that the architecture team produces - create only enough and in enough detail to get the job done.</li><li>Get in the Trenches: Develop, hire and otherwise obtain architects that don't mind getting their hands dirty with the tough day-to-day work of communicating, planning, developing, adjusting and communicating again. It's challenging to have an agile architecture that responds quickly to changing business and technology requirements if you're not plugged into the business functional areas and the people around the company that drive the requirements. This is essential and has benefits on multiple levels, not the least of which is demonstrating to the rest of the company that architects add value to the business efforts, have a deep understanding of the business needs and can effectively wed those to the most effective technology approaches.</li></ul></p> <p>Finally, in a world where the insurance industry is changing in dramatic and fundamental ways, EA is, in fact, more necessary than ever. The macro trends of digital transformation, customer centricity, mobile first and the demographics of next-generation customers have driven nearly all insurers to reconsider their strategic and operational models. As a result, almost all insurers are in the midst of some combination of core systems transformations, creating or enhancing mobile platforms, and partnering with the Insurtech community to bring innovation and creativity into their companies.</p><p>This change requires insurers to take a fresh look at how they can best leverage the concepts and principles of EA for the long-term benefits it can provide. And, the good news is that none of this is pie-in-the-sky stuff. Instead, it's a common sense approach to reestablishing the practice of EA in an increasingly agile and digital world.</p>",
    "tags": ["enterprise architecture", "agile methodology"],
    "nextMindShareId": "how-providers-can-get-better-results-from-data-efforts",
    "nextMindShareTitle": "How Providers Can Get Better Results From Data Efforts"
  },
  {
    "id": "when-it-comes-to-large-projects-think-architecture-first",
    "title": "When it Comes to Large Projects, Think Architecture First",
    "authorName": "Oleg Sadykhov",
    "authorTitle": "Principal",
    "authorImageUrl": "oleg.png",
    "shortDescription": "Large information technology programs are often multiyear initiatives with large teams and large budgets, but this approach does not guarantee success.",
    "industry": "insurance",
    "publishDate": "01/25/2017",
    "readTimeInMinutes": 10,
    "publishName": "Insurance Canada",
    "publishUrl": "https://archive.insurance-canada.ca/ebusiness/canada/2017/Xby2-Project-Architecture-1701.php",
    "content": "<p>Large information technology programs are often multi-year initiatives with large teams - more than 20 to 30 people, often much larger - and budgets in eight figures. Examples include an implementation of a core system such as policy administration, claims or billing, or a large data-analytics project. The problem with large initiatives is that delivering them in a reasonable amount of time requires large teams, and large teams present large challenges such as:</p><p><ul><li><b>Communication:</b> Smaller groups can communicate more efficiently and directly, but large teams require more communication and documentation that increase project overhead.</li><li><b>Inefficient use of resources</b>: Team members either wait for one another to accomplish tasks they depend on or inadvertently break each other's functionality, which introduces rework.</li><li><b>Uneven skills</b>: More-capable people can take on larger, more complex tasks, but it's very difficult to assemble a team of all-stars.</li><li><b>Ramp-up and knowledge transfer:</b> Building the initial team, addressing turnover and other team configuration changes create the need for new people to quickly get up to speed.</li></ul></p><p>On large projects all of this becomes even more challenging due to the size and complexity of the solutions. Because issues with large teams are well known, many modern software development methodologies (especially agile) stipulate that teams should be small. For example, in Scrum, an agile methodology, teams are typically cross-functional groups of seven or so. In an ideal world, large initiatives would be broken down to create a set of small teams that works almost independently and efficiently, and ultimately assembles their work products into the overall solution. Of course, most everybody tries to break large teams into sub-teams (also called work streams, tracks and the like), but very rarely is it done well, where a variety of challenges, including communication and inefficient use of resources, are truly addressed.</p><p>This is where architecture comes in - and it's not talking about the selection of Java versus .NET, or what integration technology to employ, or what types of servers and networks to use - though these topics are all important. Instead, it's about architecture as decomposition: how big systems must be broken down into smaller pieces that will lend themselves well to independent work by teams. These pieces must be autonomous and loosely coupled. The task is not easy, but it's important since without a good breakdown of systems into parts (variously called subsystems, modules, services or bounded contexts; subsystems for purposes of this article) there will be no good breakdown of large teams into efficient and independent sub-teams. It's natural for sub-teams to form around subsystems. The stakes are even higher than just the team breakdown. If the system is not well-architected, it will result in a complex and inflexible solution. Cost overruns and major failures are inevitable.</p><p>Every project team that has dealt with big system implementations has hit the 'wall.' Suddenly, development tasks that used to take a week start to take a month and it's not clear why. This is a direct result of increasing complexity and insufficient architecture. If the parts of the overall solution overlap, then the teams responsible for these components will end up performing similar tasks. This leads to duplication of effort, inconsistent solutions and similar defects that show up in various parts of the system that seem elusive and hard to eliminate. Project teams will under perform if the architecture is poor. Inefficient communication paths between teams will start dragging the initiative down. Teams will be mired in coordination and dependency nightmares and spend their valuable time adjusting to the work performed by others.</p><h1>Creating Good Architecture</h1><p>Let's take a simplified example to illustrate the challenges. Assume that the project is the implementation of a policy administration system where insurance policies are stored and maintained. Further assume that customer information is also stored and updated by the same system. In order to break our system down into subsystems, a logical place to start might be to separate the functionality related to policies from the functionality related to customers. In this way we've created a policy subsystem that will deal with policy information, and a customer subsystem that will deal with customers. This is an example of a good architecture that has subsystems that are autonomous and loosely coupled. This creates the ability to make and release changes to these subsystems independently of one another. In this example, policy and customer subsystems should now be able to grow and evolve independently of each other.</p><p>A typical system consists of user interface components, business logic components and data. When breaking the system down into subsystems, consideration is given to each of the layers listed. In order to achieve the independence of the subsystems, all of the layers must be dealt with - UI, business and data, including their corresponding business components. Separation of the business components is probably the easiest task (though still not easy) and as such it's the only thing that is typically attempted. Take for example the many implementations of Service Oriented Architecture over the past several years, where there is an incorrect belief that good architecture results from exposing business components such as Web services, which implies loose coupling and autonomy.</p><p>But if both the customer and the policy subsystems continue to share the same underlying database, then any work done on customer functionality subsystem can inadvertently break the policy subsystem. In this configuration, when one subsystem is released, the others must be tested as well. And when the database is down, then both subsystems are down. So where is the autonomy? The same can be said of the UI. It's natural for the same screen to present information about both policies and customers, but the way such screens are often built further entangles the two subsystems. The challenge of architecture is to keep the criteria of autonomy and the loose coupling of subsystems very clearly in mind, and tackle the difficult issues such as data separation and the disentanglement of user interfaces to accomplish it.</p><h1>Data Replication</h1><p>So what's the problem with breaking the data down and letting the subsystems own their data in order to avoid one large database that supports it all? The problem is that the policy subsystem legitimately needs customer information, and the customer subsystem may also need to know information about the policies the customer owns. Clearly, the data needs to be shared, and there are a couple of choices for accomplishing that:</p><p><ul><li><b>Store the data in one place and provide access to it.</b> In this scenario the customer data is truly owned and only accessible by the customer subsystem. To share data, the customer subsystem can either expose the data via Web services, or provide UI screens or widgets for others to use in order to access the data.</li><li><b>Allow the partial replication of the data.</b> In this scenario, the replication of some of the customer data elements gets created so the policy subsystem can store it in its own database.</li></ul></p><p>The idea of storing data in one place initially looks advantageous, but after a deeper look, it presents several challenges. In many data intensive situations, centralized data access presents a performance challenge. Many off-the-shelf packages expect the data they need to be stored locally in the application database (this is called a replication scenario). Some of the popular enterprise subsystems will be difficult to scale, since everyone will need the same data. They also will be hard to change without affecting everyone. Additionally, downtime of one subsystem will lead to the downtime of others that depend on it. The bottom line is that data replication is an important tool to achieve the goals of autonomy and loose coupling of subsystems.</p><p>There is no hard-and-fast rule to say when data should be centralized and when replication makes sense. To decide, think about the amount of data in play, how much data processing is done by the subsystem that owns the data, how impactful the downtime of one subsystem will be on another, and so on. There is no hard-and-fast rule to say when data should be centralized and when replication makes sense. Of course, the moment one starts down the path of data replication is the moment one needs a good framework in order to implement data ownership and the corresponding rules required for the subsystems. One such framework is master data management, which is a tried-and-true way to track owners of data and the rules that should be followed.</p><h1>CQRS Aids Architecture</h1><p>Another relatively recent framework that helps one think about separation between subsystems, data ownership, and much more is called the Command and Query Responsibility Separation. The idea is fairly simple: It advocates separation of data that supports modifications of the system from data that supports inquiries. According to CQRS, systems should have two parts - one for data modifications and the other for data inquiries - each with their own databases. Even without fully following CQRS, the concepts introduced by CQRS are helpful when working on and discussing architecture concepts such as:</p><p><ul><li><b>Commands:</b> Requests to modify data (that is, to create a new customer). Their names should be in the present tense and sound like commands. Their processing involves data validation, execution of business rules and changing the data.</li><li><b>Events:</b> Notifications about data changes (such as, new customer created). Names of events should use past tense to indicate that they're just a notification of a change that's already occurred. Events notify interested parties that a data change has been approved and recorded. They're typically used to trigger other processing or to update decentralized data replicas.</li><li><b>Queries:</b> Read-only requests to access the for specific data record details. Here's how the CQRS approach can be applied to our example of policy and customer subsystems. Policy and customer subsystems will 'own' their data and be responsible for processing commands to change their data. Upon successful changes of data, they will publish events notifying the other subsystems. To accomplish this, a new subsystem is created - the operational data store (ODS) - that will combine data from policy and customer components in a way that is optimized for inquiries. The ODS will change its data in response to events published by the policy and customer subsystems. Therefore, to build a solution that involves interactions with the policy and customer subsystems, the ODS will be used to fulfill inquiries, and when data changes are made commands will be sent to the respective subsystems (policy and customer) to process them appropriately.</li></ul></p><p>In summary, software architects should consider newer approaches such as CQRS when working through the challenges of the decomposition of their systems. The goal is to achieve autonomy and loose coupling of any subsystems. If successful, it can pave the way for small, independent, and efficient teams that have a much better chance to deliver large and complex enterprise initiatives successfully.</p>",
    "tags": ["insurance", "architecture"],
    "nextMindShareId": "the-key-to-small-project-management-keep-it-simple",
    "nextMindShareTitle": "The Key to (Small) Project Management: Keep it Simple",
    "isFeatured": true
  }
]
