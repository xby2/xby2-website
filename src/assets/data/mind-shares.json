[
  {
    "id": "xby2-llc-20th-anniversary",
    "title": "X by 2 Celebrates Twenty Years",
    "authorImageUrl": "20th-anniversary.jpg",
    "shortDescription":
      "X by 2 recently crossed another significant milestone, our 20th Anniversary as a company. Thank you to all of our team members and clients that made it all possible.",
    "isFeatured": false,
    "isPublication": true,
    "authorName": "Company News",
    "authorTitle": "Press Release",
    "industry": "consulting",
    "publishDate": "07/30/2018",
    "readTimeInMinutes": 5,
    "publishUrl": "https://xby2.com/insights/xby2-llc-20th-anniversary",
    "content":
      "<p>X by 2 was founded in 1998 by three colleagues who left the traditional consulting ranks with an idea to deliver business technology services following a different set of principles.  Initially focusing on internet-based design and commerce, the firm evolved to focus on the practice of software and data architecture, program and project leadership, and business technology assessments and strategies.</p><p>The X by 2 difference, honed over many years of partnering with clients, is based on creativeness, persistence, and always doing what is in the best interest of its clients.  That difference has set X by 2 apart as an innovative and impactful consulting firm that has consistently delivered value to its clients in the P&C, Life, and Healthcare industries.</p><h2 class=\"text-left\">Reflections</h2><p>Reflecting on the journey, David Packer, X by 2’s President, recalls, “Our aim has always been to enable X by 2’s clients to achieve their strategic and operational business goals.”  A lot has changed in business technology over the past twenty years, but according to Packer “our different approach created the opportunity for X by 2 to serve some of the most forward-thinking insurance and healthcare organizations in North America over the last 20 years. While our day-to-day architecture practice has evolved, our core principles remain the same: objectivity, being results driven, quality over quantity, and creating client self-sufficiency remain the bedrocks of our company.”</p><h2 class=\"text-left\">In Appreciation</h2></hr><p>Accordingly, we would like to sincerely thank our clients and team members over the years on our twentieth anniversary as a software architecture consultancy.  This milestone would not have been possible without the trust and support of our sixty-plus clients who have engaged X by 2 on over two hundred and fifty projects.  Here’s to the next twenty years.</p><img src=\"/assets/mind-share-img/anniversary-image-1.jpg\" style=\"max-width: 80%;\" /><p>Learn more about X by 2 <a href=\"xby2.com/services\">Services, Expertise</a>, and <a href=\"xby2.com/careers\">Careers</a> at <a href=\"xby2.com\">xby2.com</a></p>",
    "tags": ["publication"],
    "nextMindShareId": "why-worry-about-business-architecture"
  },
  {
    "id": "why-worry-about-business-architecture",
    "title": "Why Worry About Business Architecture?",
    "authorName": "Priyanga Manoharan",
    "authorTitle": "Architect",
    "authorImageUrl": "pri-manoharan.jpg",
    "shortDescription":
      "The importance of effective business architecture in organizations is a way to, among other things, optimize processes and create standards that enable business technology success.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "07/09/2018",
    "readTimeInMinutes": 7,
    "publishName": "Becker's Health IT & CIO Report",
    "publishUrl":
      "https://www.beckershospitalreview.com/healthcare-information-technology/why-worry-about-business-architecture.html",
    "content":
      "<p>Many companies are engaging in large transformation initiatives with the goal of not only modernizing their technology but also creating a more efficient workflow that is powered with better technology to tackle new challenges and growth.</p>\r\n\r\n<p>In many of these cases, there is a clear need for a modernized technology architecture as the systems requiring modernization are antiquated. With a significant technology shift, the need for architectural guidance is clear, and as a result, many projects are staffed with various technology architects to ensure alignment with the established standards. Many organizations realize the need for enterprise-wide guidance and have created architecture groups or departments with the sole purpose of providing enterprise-level technology guidance. While this is all well and good, many organizations fall short when it comes to a similar architectural discipline on the business side.</p>\r\n\r\n<p>This article argues for the importance of effective business architecture in organizations as a way to, among other things, optimize processes and create standards that enable business technology success.</p>\r\n\r\n<h1>What is Business Architecture?</h1>\r\n<p>Before deciding on whether valuable and limited resources and energy should be dedicated to creating business architecture, a few definitions are in order. Similar to any technology architecture, business architecture provides guidance and a general structure from which to operate.</p>\r\n\r\n<p>The first step is to establish a vision for the business. This goes deeper than any strategic goals related to markets, profits or efficiencies, by defining the set of underlying tactical goals required to enable the higher-level goals. A goal of increasing profit by a certain percent annually doesn't provide enough guidance to create a comprehensive achievement plan.</p>\r\n\r\n<p>A better approach might be to break this goal down into a percentage reduction in cost combined with a percentage increase in revenue. The point is that higher-level goals need to be broken down into levels of abstraction such that they are tactical enough to start creating actionable plans for and around.</p>\r\n\r\n<h1>Optimizing Processes</h1>\r\n<p>Once the vision has been established, and tactical plans have been created, the next step is to review the business processes. In many if not most cases, older systems have influenced the business processes. Over time many organizational methods have evolved to include steps\u2014manual or otherwise\u2014that work around technology limitations.</p>\r\n\r\n<p>For example, the lack of a real-time lookup in a system may have resulted in a two-step overnight process \u2013 the traditional overnight batch process. Even modernizing portions of the platform might enable real-time lookups to make it a one-step instantaneous process. From there, further examination of the processes and an understanding of the various steps and business needs involved could help identify additional points for efficiency or even automation.</p>\r\n\r\n<p>A key component of examining the processes is understanding and determining where the process pain points are, and how much time is spent on them. This provides the focus required to help eliminate the process pain points, allowing for a better experience for those using the system. Also, prioritizing and subsequently addressing the most inefficient aspects of a process can provide a better return on resource and financial investment.</p>\r\n\r\n<p>One of the best practices of any business architecture is using observed evidence to ensure that the scope of any process improvements directly impacts the business in a positive way.<p>\r\n\r\n<h1>Establishing Standards</h1>\r\n<p>Standards and other best practices are also valuable sources to incorporate into any business technology solution where applicable. The first benefit is in not having to reinvent the wheel. There also may be many models or standards that can be leveraged that provide great process and technology options.</p>\r\n\r\n<p>An even larger benefit is that the standards offer a point of comparison to the current processes. This allows organizations\u2014and the business and technical resources that support them\u2014to better understand the value of their current processes. It is possible that the existing standards are too generic and don't quite fit a new or existing niche market of the organization. Either way, process validation or invalidation occurs, thus adding data points to making better decisions in the near and long-term.</p>\r\n\r\n<p>Adopting standards also help when implementing new solutions by making it to easier to implement the out-of-the-box features and functionality, if desired. Having a robust set of standards also means that IT does not spend its time on migrating ineffective or broken processes as part of upgrades and implementations, opening up scarce IT bandwidth. That added benefit allows business, and IT resources to focus on building and implementing value-added features, adding a competitive edge to the organization.</p>\r\n\r\n<p>The difference lies in making a focused effort to take advantage of new features and functions, rather than just approaching them opportunistically as most organizations currently do.</p>\r\n\r\n<h1>Change Management</h1>\r\n<p>Many organizations struggle with change. That\u2019s why a change management approach is an essential component of any business architecture. The change management impact of all of the business processes optimizations as a result of a new implementation can be daunting.</p>\r\n\r\n<p>For better or worse, many organizations use the change management argument for keeping things the way they are, as handling and training on all that change can be a mammoth effort in its own right. That said, there is a counter-argument for that, however, and that is that there has to be a balance.</p>\r\n\r\n<p>Organizations who avoid the pain of change often encounter additional implementation costs in keeping a product, feature or process working as it always has. This can, of course, increase the cost and complexity of changing when that day of reckoning finally occurs. Additionally, the cost of ownership in both implementing a custom feature or functionality\u2014and maintaining that feature or functionality\u2014needs to be balanced against the impact and cost of training the end users.</p>\r\n\r\n<p>In many cases, a phased approach can be beneficial, but for most organizations, the bottom line is that sometimes change is inevitable.</p>\r\n\r\n<p>All of this and more is why taking a more proactive approach to business architecture can help organizations not only improve their technology but also help them to modernize their business processes, thus maximizing the return in their investments in large transformation projects.</p>",
    "tags": ["business architecture", "modernize"],
    "nextMindShareId":
      "how-healthcare-organizations-can-pay-off-technical-debt",
    "nextMindShareTitle":
      "How Healthcare Organizations Can Pay Off Technical Debt"
  },
  {
    "id": "how-healthcare-organizations-can-pay-off-technical-debt",
    "title": "How Healthcare Organizations Can 'Pay Off' Technical Debt",
    "authorName": "Rob McCurley",
    "authorTitle": "Architect",
    "authorImageUrl": "rob-mccurley.jpg",
    "shortDescription":
      "By understanding what technical debt is, how it arises, and how to manage it, healthcare organizations can reduce the burden of their technical debt and use it to their benefit.",
    "isFeatured": false,
    "industry": "healthcare",
    "publishDate": "07/09/2018",
    "readTimeInMinutes": 10,
    "publishName": "Health Data Management",
    "publishUrl":
      "https://www.healthdatamanagement.com/opinion/how-healthcare-organizations-can-pay-off-technical-debt",
    "content":
      "<p>Technical Debt\u2014also known as Design Debt or Code Debt\u2014is a concept in software development that reflects the implied cost of additional rework caused by choosing an easy solution now instead of using a better approach that would take longer.</p>\r\n\r\n<p>Most healthcare organizations have a business technology ecosystem that has grown and advanced over years and decades. These are combinations of older and newer systems, legacy and modern data stores, and integration points and processes that have gotten more complex over time. Often, these issues prevent healthcare organizations from upgrading or replacing systems promptly and cost time, money and resources when leadership does decide to upgrade or replace systems.</p>\r\n\r\n<p>The accrual over time of all of these technical issues is called technical debt. For much of human history, debt has generally been considered as something best avoided. But modern times have brought the realization that borrowing can be a beneficial financial tool. So it is with technical debt. By understanding what technical debt is, how it arises, and how to manage it, healthcare organizations can reduce the burden of their technical debt and use it to their benefit.</p>\r\n\r\n\r\n<p>Ward Cunningham, an early contributor to object-oriented and agile programming, defined technical debt as the amount of beneficial work avoided or deferred to deliver a business solution earlier or to reduce the cost of the system.</p>\r\n\r\n<p>In the case of technical debt, the \u201Cprincipal\u201D becomes the amount of work left undone, and the \u201Cinterest\u201D is the additional work caused over time by this unfinished work. Among the most common sources of technical debt are:</p>\r\n\r\n<p><ul><li>\u201CQuick and dirty\u201D coding is created instead of modular, structured code</li>\r\n<li>Applications use ad-hoc data sources or create new and poorly structured sources, rather than taking the time to implement new, well-designed sources</li>\r\n<li>Functionality is \u201Cbolted on\u201D to existing applications without rewriting them</li>\r\n<li>Functionality is deployed using non-standard technologies</li>\r\n<li>Documentation is not created to spend more time on coding</li>\r\n<li>Tests are not created or run to spend more time on coding</li>\r\n<li>Deferred technology and software version upgrades</li><ul></p>\r\n\r\n<p>Technical debt can be acquired in either a reckless or prudent manner. Reckless technical debt occurs when design or coding shortcuts are taken inadvertently, most often through inexperience or laziness. Prudent technical debt occurs when beneficial work is deferred deliberately for a known benefit, such as omitting a less-useful feature or implementing a workable but less elegant solution to deliver a solution earlier.</p>\r\n\r\n<p>Whether reckless or prudent, each instance of technical debt created during development and maintenance adds to a virtual balance sheet of principal maintained within the organization\u2019s technical infrastructure. This principal represents the backlog of work required for the organization to get full functionality and value from its systems. But often more significant is the interest that the technical debt accrues from its inception.</p>\r\n\r\n<p>This interest manifests itself in healthcare organizations in myriad ways. Some of the most common healthcare examples are systems and applications that are expensive (budget and resources) to maintain, difficulty implementing and integrating newer systems, increased security exposures and risks, and an overall increase in the risk of system failures. This interest is paid daily over many years until the technical debt is retired by implementing the originally deferred functionality.</p>\r\n\r\n<p>So how can a healthcare organization eliminate technical debt? The bad news is that for all practical purposes, it can\u2019t. The existing balance of technical debt will typically be financially prohibitive to remediate as a project. And the normal course of IT work will introduce new debt continuously. But the good news is that an organization can reap significant benefits from actively managing its technical debt portfolio. To do this, an organization must do two things: address the current balance of technical debt, and improve processes to minimize the accrual of new technical debt.</p>\r\n\r\n<p>To address current technical debt, key activities include:</p> \r\n\r\n<p><ul><li>Creating a technical debt catalog to identify areas of debt and ranking their impact on existing processes and practices. This is the \u201Cbalance sheet\u201D for technical debt.</li>\r\n<li>Using the catalog as a backlog, pay-down of technical debt by funneling items of \u201Cprincipal\u201D into the organization\u2019s existing software delivery streams.</li>\r\n<li>Reviewing and updating the catalog regularly.</li></ul></p>\r\n\r\n<p>The technical debt catalog can be implemented without any special tools\u2014a table in a text document or spreadsheet makes a satisfactory catalog and can be maintained with little overhead. To populate the catalog initially, gather input from all areas of IT\u2014not just development areas\u2014about where they see friction within the technical infrastructure. Eliminate problems caused by broken processes or human error, leaving those that are truly technical debt. Assign an owner to each item and determine a priority or severity for it. Don\u2019t spend too much time trying to make the initial list complete or over-documenting each case.</p>\r\n\r\n<p>The technical debt catalog by itself won\u2019t bring value unless it\u2019s incorporated into the healthcare organization\u2019s software development and delivery processes. To do this, it's necessary to assign at least a rough size and complexity to each item in the catalog to determine the best avenue to remedy it.</p>\r\n\r\n<p>Items of debt that require a small amount of work and are isolated to a single system can typically be merged into regular maintenance and update delivery cycle for that system. Each system support group should mine the technical debt catalog for items that can be placed into their system enhancement backlog, where they can be prioritized alongside business enhancements and regulatory changes. These debt items are then removed from the catalog as they are addressed.</p>\r\n\r\n<p>Items of debt too large to address through regular system enhancement processes should be addressed within projects. Debt items can sometimes be added to current or planned projects if they fit naturally into the project scope and don't have an excessive impact on timeline or budget.</p>\r\n\r\n<p>However, if it can't be broken down into smaller pieces some items of technical debt will require a dedicated project to address. Large items of technical debt like this are often the hardest to retire. Healthcare organizations are reluctant to allocate precious project resources to what is seen as \"clean up\" work. But a valid business case can be made by highlighting the real cost of the \u201Cinterest payments\u201D required by a large piece of technical debt.</p>\r\n\r\n<p>In addition to tackling existing technical debt, healthcare organizations must look at how new debt is created. Not all technical debt can or should be avoided. It can be prudent to accept certain amounts of technical debt when time-to-market takes priority over completeness. But this decision should be made with full consideration of the interest the technical debt is likely to accrue. Reckless technical debt is avoidable, however, and can be reduced through developer training and quality control processes such as architectural reviews.</p>\r\n\r\n<p>Healthcare organizations that tackle technical debt can optimize their effort if they follow a few guiding principles:</p>\r\n\r\n<p><ul><li>Keep the debt catalog simple to understand and maintain. Aside from giving each item a name, description and rough size, it\u2019s typically sufficient to assign a responsible individual or department, along with the date the item was added to the list.</li>\r\n<li>Technical debt should not be viewed as the problem of one particular group or department \u2013 it needs to be a shared responsibility between IT and business areas.</li>\r\n<li>Assign responsibility for the catalog to a team that participates in the software and project initiation processes. An Enterprise Architecture team is often a good choice.</li>\r\n<li>Review and update the list no less than twice a year.</li>\r\n<li>Don't restrict the technical debt catalog to software items only. Technical debt can accrue in infrastructure as well, for example where failover or backup capabilities are omitted from initial installation.</li>\r\n<li>Above all, technical debt cannot be delegated to \u201Cwhen we have spare time\u201D priority \u2013 there is never spare time.</li></ul></p>\r\n\r\n<p>Like financial debt, technical debt is not a pleasant issue to grapple with and resolve. It is, however, a critically important issue that ignored can lead to harsh consequences like inefficient systems, costly and laborious maintenance cycles, and, in the worst case scenarios, the inability to migrate to modern system platforms. That said, it is possible for healthcare organizations to minimize technical debt\u2019s negative impacts with a little proactive planning and follow-through.</p>",
    "tags": ["technical debt", "healthcare"],
    "nextMindShareId": "four-ways-to-build-effective-agile-teams",
    "nextMindShareTitle": "Four Ways To Build Effective Agile Teams"
  },
  {
    "id": "four-ways-to-build-effective-agile-teams",
    "title": "4 Ways to Build Effective Agile Teams",
    "authorName": "Darius Cooper",
    "authorTitle": "Architect",
    "authorImageUrl": "darius-cooper.jpg",
    "shortDescription":
      "The objective of this approach is to leverage the benefits of an agile approach to software development and blend those benefits with the advantages of a centralized group of specialists.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "07/05/2018",
    "readTimeInMinutes": 5,
    "publishName": "Digital Insurance",
    "publishUrl":
      "https://www.dig-in.com/opinion/4-ways-to-build-effective-agile-teams?brief=00000159-faf0-d111-af7d-fbfd9f0a0000",
    "content":
      "<p>In part one of this series, the focus was on the advantages and disadvantages of creating a separate and distinct team for any project\u2019s integration work, as opposed to having integration work as part of a holistic agile team. That begs the question: How can one remain true to agile, while also bringing in the technical depth and the \u201Cflexible coupling\u201D that come from a separate team?</p>\r\n\r\n<p>A productive approach is to have each sprint team focus on its integrations but to also have integration specialists as members of that team to bring their knowledge of the technology and the standards necessary to ensure a robust architecture. Technical specialists who have seen similar types of integrations before, and who are familiar with standard approaches and patterns, can help immensely. It means that subsequent project teams do not have to reinvent the wheel to tackle problems for which there already are good patterns and tools. In the context of this example, the integrations specialists should join the core agile team, and work as part of the single team.</p>\r\n\r\n<h1>Center of Excellence</h1> \r\n<p>A good way to approach this is to establish an Integrations Center of Excellence, staffed with integration specialists. One role of this team is to choose the tools, provide the advice and create the organizational standard around integrations. However, purely \u201Cstaff\u201D teams can sometimes become ivory towers, imposing their ways upon \u201Cline\u201D teams. The best specialists learn by doing.</p>\r\n\r\n<p>In an agile environment, they should actually apply their knowledge to situations that are very concrete and specific to the organization. They need to base their work on the day-to-day work of the teams that are creating the actual integrations, as opposed to creating arbitrary and abstract one-size-fits-all standards.</p>\r\n\r\n<h1>Embedded Experts</h1> \r\n<p>Another best practice for this approach\u2014and a good way to keep the integration specialists grounded while also ensuring buy-in from both sides\u2014is to embed some specialists from the Center of Excellence into the sprint teams developing the specific integrations. That way they become members of the sprint team. Another benefit is that this is in keeping with an agile approach to an organizational structure: instead of creating another silo of specialists, form the teams tactically, pulling in specialists as needed.</p>\r\n\r\n<p>In this way, the Center of Excellence becomes a fluid organization rather than a sizeable static team, but is also the coordinator of a more comprehensive \"Interest Group.\" Only a few members are in the separate team\u2014in the Center of Excellence\u2014at any point. They may be creating organization-wide standards for integrations, or choosing specialized tools, or developing tools that can be useful to individual teams. Meanwhile, integration specialists are spread across the project or program, working in various development teams.</p>\r\n\r\n<h1>Guilds</h1> \r\n<p>Another idea that works well with this structure is the idea of a guild, whose members belong to various and different teams, but who do similar work in those teams.</p>\r\n\r\n<p>For example, the Quality Assurance resources or the Business Analysts from across the organization may be members of guilds, where they meet to discuss common problems faced and the solutions devised and implemented. Another benefit is that an \u201CIntegrations Guild\u201D becomes an effective mechanism for sharing ideas and knowledge about the special problems faced within and amongst the teams. It also helps to create an informal network of co-workers who can call on each other for help, resulting in shared knowledge, less re-invention of the same solutions, quicker learning curves for new members and higher productivity.</p>\r\n\r\n<p>The Center of Excellence can help coordinate the activities of the guild and to create common approaches and standards, but the day-to-day integration work is done by members who belong to various other development teams.</p>\r\n\r\n<h1>Phased Approaches</h1> \r\n<p>In the early design and development stages of a project, there is usually a greater need for specialists who have experience with certain types of issues, and who can create project-specific patterns and approaches.</p>\r\n\r\n<p>That need for specialists is reduced as the project progresses and the project team tackles a more extensive variety of problems with the patterns put in place by the specialists. At the start of any project, the specialists may be intimately involved with the project team in developing the initial integrations. Once the project is underway, however, and with established approaches and patterns of work, the specialists can shift their time and energy to helping keep things on track, introducing new technologies as appropriate, and with the inevitable tough problem that will crop up from time-to-time.</p>\r\n\r\n<p>In this way, the direct involvement of the Center of Excellence will be gradually reduced. As the project moves along, other developers may find themselves interested in the specific technologies they are working with, and as a result, they may wish to join the guild, creating a valuable pipeline for future integration specialists.</p>\r\n\r\n<h1>Blending Goals</h1>\r\n<p>In summary, the objective of this approach is to leverage the benefits of an agile approach to software development and blend those benefits with the advantages of a centralized group of specialists, in this case, integration specialists. To that end, the approach aims to blend both long-term and short-term goals. It leverages specialist knowledge, while also creating accountability on the part of the specialists for the delivery of well-architected software solution. It also enables a core, but more refreshed team of specialists to look at the bigger picture, across the organization, while at the same time keeping them grounded in the requirements of real projects and, thus, close to the action. This creates a blend of strategic vision and tactical implementation, and in the process sets a context for success.</p>",
    "tags": ["Agile", "integration"],
    "nextMindShareId": "living-in-a-devops-world-part-two",
    "nextMindShareTitle": "Living In A Dev-Ops World (Part Two)"
  },
  {
    "id": "living-in-a-devops-world-part-two",
    "title": "Living in a DevOps World (Part 2)",
    "authorName": "Dave Farinelli",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "dave-farinelli.jpg",
    "shortDescription":
      "DevOps was created to more seamlessly connect the development and operations processes, making code migration, testing and promotion to production a more efficient process.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "07/03/2018",
    "readTimeInMinutes": 7,
    "publishName": "DevOps.com",
    "publishUrl": "https://devops.com/living-in-a-devops-world-part-2/",
    "content":
      "<p>Part one of this article focused on some of the more behind-the-scenes benefits of an Agile DevOps approach. In part two the focus turns to some of the other traditional problems that a well-executed DevOps approach can address, and how doing so can benefit an organization in more ways than just a technical perspective.</p>\r\n\r\n<p>By way of quick review, DevOps was born out of the Lean and Agile software development methodologies when it became clear that, while those methodologies did indeed speed up the development process, a bottleneck still occurred when push came to shove and new code had to be moved to quality assurance and production environments.</p>\r\n\r\n \r\n<p>DevOps was created to more seamlessly connect the development and operations processes, making code migration, testing and promotion to production a more efficient process. To accomplish this, the DevOps approach had to find solutions for some of the issues that caused operational delays, and create new ways to organize, implement and continuously optimize the operations process.</p>\r\n\r\n \r\n<h1>Overproduction/Overprocessing</h1>\r\n \r\n<p>For those who have been in development and/or operations for any length of time, it quickly becomes clear that there is a multitude of operational safety checks that serve to protect a production environment. While that is vitally important, it was also clear that there had grown an \u201Cover\u201D problem around many operational procedures, and in many cases that manifested itself in the development process. That includes overproduction, when making or requesting more than was needed from requirements and/or operations perspective to clear arbitrary operations process hurdles.</p>\r\n\r\n \r\n<p>Alternatively, overprocessing, when development and operations resources do more work (as opposed to just enough, as Lean and Agile would suggest) than required to smooth the transition of code and functions from development to operations. This created waste regarding time, resources and budgets that were not proportional to the benefits derived from following the operations process.</p>\r\n\r\n \r\n<h1>Motion and Transportation</h1>\r\n \r\n<p>Similarly, DevOps also sought to solve the operational problems of both motion and transportation. That is, the amount of excess work required to deliver new code to meet the operational requirements for code migration. The friction caused by such requirements slowed the motion and momentum of the development process. The same is true of transportation, or the difficulty in moving code between environments such as testing, quality assurance and production.</p>\r\n\r\n \r\n<p>In both cases, development and project momentum was sacrificed for what often turned out to be a series of artificial hurdles that had long since become less effective or even obsolete parts of the operations process.</p>\r\n\r\n \r\n<h1>Correction and Inventory</h1>\r\n \r\n<p>In most instances, all of the above resulted in the final maladies of the pre-DevOps development and operational ways. The first was the number of in-flight corrections required when timelines were squeezed, and the rush was on to get to production. Unfortunately, this went hand in hand with the ultimate problem of good code being sacrificed for expedient delivery, often resulting in inadequate functionality, system outages and, in the end, lost market opportunity and revenue.</p>\r\n\r\n \r\n<h1>3 Keys to DevOps Success</h1>\r\n \r\n<p>Any successful DevOps implementation must address three critical factors in this order: culture, organization and tools.</p>\r\n\r\n \r\n<h1>Culture</h1>\r\n \r\n<p>It\u2019s critically important to connect an organization\u2019s values to the DevOps process. Valuing quality, timeliness and organizational alignment of goals and objectives is the first step toward DevOps success. Such cultural values translate directly into a DevOps organization.</p>\r\n\r\n \r\n<p>Providing empowerment and accountability to DevOps team members helps to build ownership among the team, and trust from their customers in the rest of the organization. It also helps to provide a physical environment that fosters collaboration, teamwork and continued learning. Co-working spaces and collaboration tools such as Slack are a good start. Attending external conferences to broaden perspectives and to bring new ideas back to the team is often beneficial. From there, brown bag lunch sessions where ideas and experiences can be shared, frequent post-mortems on implementations to hone best practices, and even internal mini-conferences where several departments come together for a day to discuss DevOps practices are all effective ways to build a strong DevOps culture.</p>\r\n\r\n \r\n<h1>Organization</h1>\r\n \r\n<p>Any good DevOps organization is two-sided; that is it has to work from the top down and from the bottom up at the same time.</p>\r\n\r\n \r\n<p>The top-down part is in the ability to \u201Csee the system\u201D from a macro level, allowing for process understanding and insights from a business workflow perspective. This helps to identify the pain points and bottlenecks in the current process that can be optimized through the DevOps process.</p>\r\n\r\n \r\n<p>Once that\u2019s accomplished, the bottom-up work begins. Identifying things such as inconsistencies in code deployment environments that cause delivery issues, elimination of manual and custom built deployment processes and quarantining inefficient and poorly written code until it can be redone or eliminated are all part of optimizing the time, quality, resources and success factors for deploying production systems on schedule. It\u2019s also important here to continually audit the current processes with an eye toward eliminating the processes that are no longer required or useful but have been kept in place out of the fear of  \u201Cbreaking something we don\u2019t understand.\u201D If nobody understands it, then it shouldn\u2019t be in production software.</p>\r\n\r\n \r\n<h1>Automation Tools</h1>\r\n \r\n<p>The final factor for DevOps success is to have the right toolset.</p>\r\n\r\n \r\n<p><b>Communication</b>: Any DevOps team requires the ability to quickly and directly communicate with other team members sans meetings. For this purpose, tools such Slack (real-time chat), Skype (video chat), and Confluence (for storing persistent information) are pretty good options.</p>\r\n\r\n \r\n<p><b>Planning, Monitoring & Consistency</b>: For the team\u2019s planning needs, a tool such as Trello that can provide Kanban board functionality is worth a look. For issue tracking and monitoring of any system\u2019s overall health, tools such as Jira and NewRelic respectively provide some good functionality. Likewise, consistency is vital in a DevOps world, and using automation to ensure that all systems are configured as desired across different environments is a crucial best practice. For this, a tool such as Ansible is worth a review.</p>\r\n\r\n \r\n<p><b>Integration & Deployment</b>: For continuous integration of systems in development and as a way to tighten the feedback loop for developers to determine if the central build used for deployment to production is working as intended, the Jenkins toolset might be a good fit. And finally, when it comes making any deployment process as painless as possible, a tool such as Docker that can handle created containers for an application that includes all dependencies, reducing the complexity of deployment to multiple environments, is a solid way to go.</p>\r\n\r\n \r\n<p>The point of all of this is to create an environment\u2014culturally, technically and physically\u2014where DevOps can succeed, grow and thrive. Organizations that can create an effective and efficient DevOps environment have also created a competitive advantage for themselves.</p>",
    "tags": ["devops", "agile"],
    "nextMindShareId": "living-in-a-devops-world-part-one",
    "nextMindShareTitle": "Living In A Dev-Ops World (Part One)"
  },
  {
    "id": "living-in-a-devops-world-part-one",
    "title": "Living in a DevOps World (Part 1)",
    "authorName": "Dave Farinelli",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "dave-farinelli.jpg",
    "shortDescription":
      "The first step on the journey to effective DevOps is the recognition that the concept is the result of the rise of the Lean and Agile software development methodologies.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "06/05/2018",
    "readTimeInMinutes": 5,
    "publishName": "DevOps.com",
    "publishUrl": "https://devops.com/living-in-a-devops-world-part-1/",
    "content":
      "<p>The concept of DevOps has grown and evolved into a conceptual and working model for more effective and efficient software development and implementation. That said, there are some differences of opinion on the real-world value of any DevOps approach to date, and on the best way to create and implement a real-world DevOps environment. This two-part article will focus on what an agile DevOps approach is meant to address, and what it is not meant to address.</p>\r\n\r\n<p>DevOps sits at the nexus of three essential business technology functions: software development, quality assurance and operations. A short and concise definition of DevOps proposed in 2015 seems as appropriate as any:</p>\r\n\r\n \r\n<q>DevOps is a set of practices intended to reduce the time between committing a change to a system and the change being placed into regular production while ensuring high quality.</q>\r\n\r\n \r\n<p>The definition was suggested in the book, \u201CDevOps: A Software Architect\u2019s Perspective,\u201D and the authors have hit upon the essence of the practice. The key, of course, is how to put that concept into practice.</p>\r\n\r\n \r\n<p>Perhaps the first step on the journey to effective DevOps is the recognition that the concept is the result of the rise of the Lean and Agile software development methodologies. Those methodologies, among other things, emphasize the following:</p>\r\n\r\n \r\n \r\n<p><ul><li>A focus on customer value.</li>\r\n \r\n<li>The elimination of waste.</li>\r\n \r\n<li>Reduced cycle time (accomplishing work faster, releasing faster).</li>\r\n \r\n<li>Shared learning.</li>\r\n \r\n<li>Avoiding batching (don\u2019t do things until required).</li>\r\n \r\n<li>Theory of constraints (break things up, focus on individual issues).</li>\r\n \r\n<li>Continuous integration, testing and delivery.</li>\r\n \r\n<li>Faster time to market.</li>\r\n \r\n \r\n<h1>DevOps in Practice</h1>\r\n \r\n<p>Adherence to the principles above meant that something had to be invented to accomplish them and that something was DevOps. Over time, an effective DevOps practice should address any number of business technology pain points. The following short list of those pain points and the DevOps response should prove instructive.</p>\r\n\r\n \r\n<h1>System Downtime</h1>\r\n \r\n<p>The developers\u2019 curse since systems were developed, system outages are inevitable as long as systems are designed, tested and implemented\u2014even with increased automation\u2014by imperfect beings. DevOps acknowledges that by changing the focus from trying to create applications that never fail to designing systems that can recover quickly, thus decreasing aggregate systems outage time over the life cycle of any application or system.</p>\r\n\r\n \r\n<h1>Stagnation</h1>\r\n \r\n<p>This was a staple of traditional systems development and is most closely associated with the waterfall methodology for systems development. After requirements were created, the development team would be locked away for weeks, months or, in some cases, years before emerging with \u201Cfully\u201D working software that inevitably no longer satisfied rapidly evolving business requirements. DevOps is designed to fit hand-in-glove with the Agile practice of short windows of incremental changes instead of long release cycles, putting working software in the hands of customers as quickly as possible.</p>\r\n\r\n \r\n<h1>Team Conflict</h1>\r\n \r\n<p>Having been borne from the cultural combination of Agile and Lean, DevOps has taken on the problem of functional silos that are often erected between development, operations and the business customers. It follows the methodological approaches of collaboration and teamwork first to understand what others know and to leverage the best of it to solve business problems more rapidly. There is also a cultural bent toward experimentation, continual learning and constant improvement. This leads to blameless post-mortems, where instead of finger pointing when something goes wrong there is collaborative discussion and learning to correct and prevent the problem from occurring again.</p>\r\n\r\n \r\n<h1>Knowledge Silos</h1>\r\n \r\n<p>Functional silos have led to compartmentalized knowledge. If the old game was that knowledge is power, the new game in the DevOps world is that knowledge is freely exchanged as an enabler to solving business problems. DevOps addresses the problem of information being lost in translation between the development and operations functions by eliminating the functional barricades and making knowledge sharing the highest form of collaboration.</p>\r\n\r\n \r\n<h1>Inefficiency</h1>\r\n \r\nWaiting for things to happen used to be a standard operating procedure in the pre-DevOps world. Project plans were created and managed to account for the time it might take for new code to be moved into a testing, quality or even production environment. This was a momentum killer for projects and at times a morale killer for developers waiting to see what changes they might need to make to their code set. The combined Agile and DevOps approach has rewritten the traditional approach to code migration, smoothing and eliminating wait times so that projects flow more seamlessly from start to finish. It also has the benefit of keeping business resources\u2014testers, approvers, etc.\u2014more engaged as a result of a constant flow of new functions and features to test and use. And lest this be viewed as simply a way to keep the technical stuff moving along, it\u2019s important to remember that there is a financial aspect to this as well. Reducing speed to market with new functionality, reducing or eliminating idle hands\u2014be they technical or business\u2014and delighting customers with a steady stream of enhancements and features all go directly to an organization\u2019s top and bottom lines.</p>\r\n\r\n \r\n<p>That, after all, is in many ways what the DevOps approach is all about. All of these critical areas become the means to accomplish it. Part two of this article will focus on some more of the benefits of a DevOps approach, and how to achieve them.</p>",
    "tags": ["devops", "agile"],
    "nextMindShareId": "how-to-integrate-systems-more-efficiently",
    "nextMindShareTitle": "How to Integrate Systems More Efficiently"
  },
  {
    "id": "how-to-integrate-systems-more-efficiently",
    "title": "How to Integrate Systems More Efficiently",
    "authorName": "Darius Cooper",
    "authorTitle": "Architect",
    "authorImageUrl": "darius-cooper.jpg",
    "shortDescription":
      "Any large software program has tentacles that stretch across the entire insurance organization. These dependencies are often undocumented and overlooked.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "06/01/2018",
    "readTimeInMinutes": 3,
    "publishName": "Digital Insurance",
    "publishUrl":
      "https://www.dig-in.com/opinion/how-to-integrate-systems-more-efficiently?brief=00000159-faf0-d111-af7d-fbfd9f0a0000",
    "content":
      "<p>As many insurers have discovered, the risks entailed in core system modernization are not what they used to be. There are still plenty of risks, of course, but in many cases these have less to do with new software implementation and more to do with systems integration.</p>\r\n\r\n<p>Any large software program has tentacles that stretch across the entire insurance organization. These dependencies are often undocumented and overlooked\u2014until such time when a new system is rolled out and promptly begins to break other systems in unexpected ways. To correct and prevent this, the integration effort may require as much work as the new core system itself.</p>\r\n\r\n<p>How then to achieve these integrations in the most efficient manner possible?</p>\r\n\r\n<p>One byproduct of all the core system modernization efforts underway is that many specialized integration tools are now available. And, to take advantage of them, many insurers have established an \"integration team\" that groups related technical specialists together.</p>\r\n\r\n<p>This approach, however, can cut against the business view of the project. From a business standpoint, what the user sees on a front-end screen is the visual representation of one or more business processes or user stories, and this is where some integration is often required. But this requires great coordination and communication. To avoid this, the agile approach emphasizes treating the user-story as a cohesive unit.<p>\r\n\r\n<p>But a good systems integration approach is to combine these strategies. This keeps the integration work close to the other work being done for the user story, while also bringing specialized knowledge to bear.</p>\r\n\r\n<p>Let me explain in a little more detail.</p>\r\n\r\n<p>When a team works independently on a project, the structure of the software they produce will often reflect that separation. This is sometimes called Conway\u2019s Law: that software structure often reflects organizational structure.</p>\r\n\r\n<p>Likewise, when the team working on a user-story does integration work, there is a risk that the integration software will also reflect that team\u2019s orientation and assumptions.</p>\r\n\r\n<p>Having a separate team develop the integration software can lead to more independently structured and universally applicable integration.</p>\r\n\r\n<p>However, this approach also comes with a risk: The generic solutions created by a more independent team can fail to fully support the different user stories and require costly rewrites.</p>\r\n\r\n<p>Instead of creating a separate integrations team, one way to avoid both types of problems is to treat the integration software as an extension of the business process software with an adaptable layer between one system and another. In effect, this approach creates an integration layer between the core system and the insurer\u2019s other systems, even though the software is developed by the core team. This helps to mitigate the adverse effects of Conway's Law. And the dedicated integration team can provide a single point of contact with the rest of the organization, thus reducing the risk of any communications falling through the cracks.</p>\r\n\r\n<p>In a follow-on article, I\u2019ll explore further how the two approaches can be successfully wedded.</p>",
    "tags": ["modernization", "integration"],
    "nextMindShareId":
      "analogical-storytelling-approach-to-technical-communication-barriers",
    "nextMindShareTitle":
      "Analogical Storytelling Approach to Technical Communication Barriers"
  },
  {
    "id":
      "analogical-storytelling-approach-to-technical-communication-barriers",
    "title":
      "Analogical Storytelling Approach to Technical Communication Barriers",
    "authorName": "Mohammed Hussain",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "mohammed-hussain.jpg",
    "shortDescription":
      "Data technologists are often challenged with effectively communicating complex technical information to the non-technical side of the business.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "05/07/2018",
    "readTimeInMinutes": 5,
    "publishName": "Becker's Health IT & CIO Report",
    "publishUrl":
      "https://www.beckershospitalreview.com/healthcare-information-technology/analogical-storytelling-approach-to-technical-communication-barriers.html",
    "content":
      "<p>Data technologists are often challenged with effectively communicating complex technical information to the non-technical side of the business to gain approval for the funding, the resources, the priorities, or all the above for any given data strategy.</p>\r\n\r\n<p>This situation is common practice, but being able to conceptually explain the strategy is where good storytelling comes into play. To better elaborate, consider the following story.</p>\r\n\r\n<p><i>Suppose you are in the year 300 BC and you need to create a way to collect, store and organize data. How do you do it? Well as a first step, you'll likely learn the language used to communicate and gather information, particularly in its written format. With this knowledge in hand, you\u2019ll be ready to start writing. To accomplish this, you\u2019ll acquire some papyrus or parchment and some ink (made from the local materials available) and begin recording information. When you run out of page space on one page, you'll start to create more pages. When the number of pages gets large enough that it becomes difficult to find a particular page quickly, you\u2019ll give each page a mark or a number to make it easier and faster to access the information you want. And when the amount of marked or numbered pages becomes too cumbersome to balance on your lap or to spread out on a table, you\u2019ll create a book by binding the pages together.</p>\r\n\r\n<p>As you create and acquire books, you find that you have to start organizing books by subject to make it easier to store and retrieve. The math information goes in the math book, and the math book is placed with other math books. Likewise, the biology information is placed in the biology book, the geography in the geography book, and so on. When you have amassed too many books to put on the table or the floor, you\u2019ll build a bookshelf to hold them all. When you run out of bookshelf space, you\u2019ll build another bookshelf to hold all of your books. As the number of books you\u2019ve acquired increases, you begin to organize them by sub-topics within subjects to make them easier to locate and access.</p>\r\n\r\n<p>Up to this point you\u2019ve done an effective job of organizing your two bookshelves worth of books \u2013 so far so good. But what happens when you have a hundred bookshelves full of books? As the number of similar subjects grows, organizing simply by subject and even sub-topic within the subject is difficult, so you decide to organize not only by subject but also by author. However, at one hundred bookcases and growing, it can still take a while to locate a specific book, so what do you do? You decide to number each of the one hundred bookshelves and create a separate filing system that tells you which book is on which shelf. To help further, you also label the individual shelves so that your filing system includes not only the bookshelf number but also the individual shelf any book is on. You could expand your filing further and maybe add a different filing system that tells you where books are by genre. Or maybe reading level. Or whatever other organization you can come up with so long as it makes the task of locating the information you want as efficient and effective as it can be. Congratulations are now in order, as you\u2019ve become one of the creators The Library of Alexandria c. 300 BC\u2014and your data is stored and easy to find.</i></p>\r\n\r\n<p>If you followed that story you now have a basic understanding of a simple data strategy along with the rationale for why it was required, and some of the insights surrounding the intuition behind why it was built the way it was built.</p>\r\n\r\n<p>To connect this back to modern software, think of the Book as the data entity or record, the Bookshelf as the data structure representing and storing the data (an array or a list maybe), the collection of bookshelves as representing your Database, and the filing system as the Database Index. We can expand this analogy even further to say that in your library you can check out a book so that someone cannot change it while you're reading it, or that two people cannot change it at the same time. And, as your library grows it will require regular updates. For example, if you add a new book to your library, your filing system needs to be updated to record all of the particulars of the new book. That will protect against the complications of running out of shelf space and having to move all of the books\u2014the filing system will remain accurate.</p>\r\n\r\n<p>In my experience, a quality storytelling approach has proven to be an effective way to convey the technical ideas involved and to get some interesting discussions going as a byproduct. By telling a non-technical but relatable story, a technologist can help explain what data is and how it's generated, as well as what it means to create a data strategy and why it's both necessary and important.</p>\r\n\r\n<p>And while it might not come naturally to many technologists, the happy fact is that stories and storytelling is a part of who we all are, the technical and non-technical alike. It's a part of human history and the human experience, uniquely so in fact, and as such provides an instantly recognizable format. Of course, if it were as simple as it sounded everybody would do it. The truth is that like anything else it requires practice and persistence, but once mastered, storytelling can provide huge dividends in communications and relationship building. The other requirement is finding the right story for the right technical concepts presented, which also requires practice and persistence.</p>\r\n\r\n<p>The introductory story takes between five and ten minutes to tell. The overall idea is that the story represents a relatively succinct and relatable way for understanding something very complex\u2014essentially giving non-technical listeners the conceptual building blocks to help understand complex problems. This method of formulating a creative analogy for a technical problem is foundational to any successful consulting career. It is a shockingly successful way of getting buy-in from business users by helping them understand a topic to the level that a new developer would, for example, without the explicit understanding of the underlying technical details.</p>\r\n\r\n<p>The art of analogical storytelling accomplishes a few things: first and foremost it captures the audience\u2019s attention - everyone likes to hear a story. Second, it uses a common and easily understood format that can be visualized by each listener, increasing the possibility of retention and understanding. And finally, it can be seamlessly connected back to its technical counterpart.</p>\r\n\r\n<p>Good stories are worth their weight in gold, and like any other skill, it is something that requires practice and forethought. So the next time you're struggling to explain something to someone, try telling a story.</p>",
    "tags": ["technical communication", "data"],
    "nextMindShareId":
      "new-life-insurance-delivering-a-next-gen-experience-to-the-next-gen-applicant",
    "nextMindShareTitle":
      "New Life Insurance: Delivering a Next-Gen Experience to the Next-Gen Applicant"
  },
  {
    "id":
      "new-life-insurance-delivering-a-next-gen-experience-to-the-next-gen-applicant",
    "title":
      "New Life Insurance: Delivering a Next-Gen Experience to the Next-Gen Applicant",
    "authorName": "Samir Ahmed",
    "authorTitle": "Principal",
    "authorImageUrl": "samir-ahmed.jpg",
    "shortDescription":
      "Delivering the complete next-gen customer experience requires assembling simple technology building blocks into a sophisticated and well-engineered solution.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "04/11/2018",
    "readTimeInMinutes": 7,
    "publishName": "Insurance-Canada",
    "publishUrl":
      "https://www.insurance-canada.ca/2018/04/11/xby2-next-gen-experience/",
    "content":
      "<p>The life insurance industry has been abuzz with improving the customer experience for the past few years, particularly in the new business and underwriting processes. It might seem like a tired topic to revisit in 2018, but the number of carriers that have yet to (fully) implement capabilities such as end-to-end electronic application, end-to-end eSignature, straight-through policy issuance, and eDelivery of policy, just to name a few, indicates otherwise. What is behind the low adoption rates for such capabilities? And more importantly, what can be done to drive higher adoption?</p>\r\n\r\n<h1>Current Applicant Experience</h1> \r\n\r\n<p>The current life insurance applicant is likely to be a Gen X or Millennial \u2013 and will soon be a Post-Millenial. Her buying experience leaves her frustrated and dissatisfied. As she does whenever she is faced with any new purchase, she starts on the internet. She quickly finds a website that offers life insurance quotes. This is where her experience first starts to deviate from her expectations. Instead of providing a quote, the website facilitates a phone call with an agent. Disappointed, the applicant provides her phone number and indicates a convenient call time. The agent calls and offers to visit her at her house and walk her through everything. The applicant is a bit taken aback. She wasn\u2019t expecting to have to work with someone to get a life insurance quote, let alone invite that person into her home, or go to his or her office. In her view of the world, this is not how things get done.</p>\r\n\r\n<p>When the agent arrives at her home, he starts by asking her to fill out an insurance \u201CNeeds Assessment\u201D questionnaire that is seven pages long. It all looks simple and straight-forward, yet, as she begins filling it out, she notices that the questions become progressively more intrusive. It starts with demographic information, such as her name, address, and occupation. That is followed by detailed questions about her monthly budget, including all her income and all her expenses broken down by category. Finally, it rounds out the assessment by asking about assets, liabilities, financial goals, and expectations for final expenses, debts and income replacement. Using the collected information, the agent prepares a few proposals, walks the applicant through them while answering her questions along the way. He also gives her a 41-page packet containing the insurance application and several associated forms. He asks the applicant to review the proposals and to fill out the application packet based on the plan she likes. He offers to return in a few days to take care of signatures and payment and to collect the paperwork for submission to the carrier.</p>\r\n\r\n<p>Her experience continues in this manner, inclusive of documentation follow-ups, family medical histories of which she knows little, a visit from a nurse, and it culminates with the agent telling her that the 41-page application packet is ready for evaluation and that she\u2019ll hear something in the next 90 days or so. To the applicant, the 21st century this is not.</p>\r\n\r\n<h1>The Next-Gen Experience</h1>\r\n\r\n<p>These sentiments held by the biggest pool of potential life insurance buyers are well known in the life insurance industry; it\u2019s not a surprise to anyone.</p>\r\n\r\n<p>It\u2019s equally well known that what these applicants desire instead is an experience that flows like this:</p>\r\n\r\n<p><ul><li>24/7 self-service on devices of their choosing with a seamless transition from one (e.g., mobile app) to another (e.g., desktop web browser);</li>\r\n<li>interactive questionnaires presenting a few questions at a time and tailored based on answers already provided;</li>\r\n<li>being asked the breadth of information needed during the application process (not as follow-ups during the evaluation process);</li>\r\n<li>comparison of products, coverages, premiums, etc.;</li>\r\n<li>review of the final application packet before signing it;</li>\r\n<li>electronic signature;</li>\r\n<li>electronic payment at the time of signature;</li>\r\n<li>an immediate decision, with an explanation in cases when the application requires further evaluation, followed by regular notification of its status; and</li>\r\n<li>electronically delivered documents, including the issued policy.</li></ul>\r\n\r\n<p>When both the source of frustration and the pathway to delight are known, why the low adoption rate on capabilities that matter most to this newest generation of consumers? The short answer is that while there is a simple to understand and implement technology solution for each element of the desired experience, delivering the complete experience requires assembling simple technology building blocks into a sophisticated and well-engineered solution. That might sound simple, but it is not easy and is what bedevils the industry.</p>\r\n\r\n<p>The technology components that need to be assembled consist of the following:</p>\r\n\r\n<p><ul><li>A modern user interface development framework that supports web, tablet and mobile access.</li>\r\n<li>A reflexive question engine that can determine what questions to ask based on answers already provided.</li>\r\n<BLOCKQUOTE><li>It requires codification of all application evaluation rules, including New Business, Compliance and Underwriting</li></BLOCKQUOTE>\r\n<li>A document generation system to present electronically completed application packets for review.</li>\r\n<li>eSignature and ePayment</li>\r\n<li>A system integration platform that facilitates:</li>\r\n<BLOCKQUOTE><li>Real-time communication with information sources, e.g., MIB, Rx history, MVR, etc.</li>\r\n<li>Real-time appointment scheduling with evidence providers, e.g., paramedical exam, labs, tele-interview, etc.</li></BLOCKQUOTE>\r\n<li>An underwriting rules engine, to codify underwriting rules, and provide real-time risk assessment with stratification by statistical confidence intervals.</li>\r\n<li>ePolicy and eDelivery</li>\r\n<li>Policyholder portal for receiving application status, viewing the issued policy and securely communicating with the agent and the insurer.</li>\r\n<li>Agent portal to view the status of submitted applications, and securely communicate with applicants and the insurer.</li></ul></p>\r\n \r\n<h1>Conceptual Solution</h1>\r\n\r\n<p>The following diagram illustrates a logical assembly of the necessary technology components into a conceptual future state for a typical life insurer:</p>\r\n\r\n<img src=\"/assets/mind-share-img/samir-image-1.jpg\" /> \r\n\r\n<p>Such a conceptual solution might seem daunting. Fortunately, the software architecture discipline provides a proven approach for accomplishing all of the above and more, which is to conceptualize the target state, acknowledge the current state, identify gaps between the two, outline a roadmap for closing the gaps, and then chip away at the solution one capability at a time. Care must be taken to ensure that each building block that adds functionality and capability on the back-end also enhances the front-end experience of next-generation customers. Given proper prioritization of resources and budgets, all of this can be accomplished in two to three years.</p>\r\n\r\n<p>Think big, start small, and move fast is the call of the hour. It\u2019s not rocket science, but it does take considerable focus and persistence \u2013 something the industry has been demanding of its applicants for decades.</p>",
    "tags": ["customer experience", "application"],
    "nextMindShareId": "the-purpose-driven-project-approach",
    "nextMindShareTitle": "The Purpose-Driven Project Approach"
  },
  {
    "id": "the-purpose-driven-project-approach",
    "title": "The Purpose-Driven Project Approach",
    "authorName": "Jason Brown",
    "authorTitle": "Developer",
    "authorImageUrl": "jason-brown.jpg",
    "shortDescription":
      "One of the issues with the current technology tsunami is that it can be difficult for those working on IT initiatives to gain a fuller understanding of a project.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "04/02/2018",
    "readTimeInMinutes": 7,
    "publishName": "Becker's Health IT & CIO Report",
    "publishUrl":
      "https://www.beckershospitalreview.com/healthcare-information-technology/the-purpose-driven-project-approach.html",
    "content":
      "<p>Modern Information Technology work is all about outcomes.</p>\r\n\r\n<p>Market pressures, competitive differentiators, customer service expectations and new capabilities are just a few of the reasons that insurers are sprinting to enable new technologies and processes that support their strategic goals and objectives. For people consulting and working in IT divisions that usually means long hours, shifting priorities and making decisions as quickly as possible with the information available. That\u2019s all well and good, but a recent experience has suggested an alternative approach.</p>\r\n\r\n<p>One of the issues with the current technology tsunami is that it can be difficult for those working on IT initiatives to gain a fuller understanding of why a particular technology project is being put together the way it is, as opposed to just having a rudimentary understanding of what the technology is supposed to be doing. That's an important distinction and one that deserves further consideration, as this article will attempt to do.</p>\r\n\r\n<p>For most developers, architects, data administrators and others on IT projects, it can be very easy to fall into the trap of only understanding how something is supposed to work in a rush to get new capabilities into production. One sure sign of this can be the introduction of new tools and frameworks as part of a new project that is duplicative of the tools and frameworks already in use. This leads to technology tunnel vision, with IT resources having an even narrower understanding of a new project, focusing only on their piece of the technology pie for the effort.</p>\r\n\r\n<p>Again, this is understandable given the short-term pressures of delivering, but it can be detrimental over the longer term in the context of developing well-rounded IT people who understand more than what something is supposed to do. The problems with this kind of approach are many and well documented, but a short list includes the likelihood of repeating patterns and mistakes from previous projects, IT people who are too narrowly skilled and focused on only what they\u2019ve done in the past, and an unwillingness to be open to new technologies and approaches that might be beneficial to any insurer. And a good deal of this can be attributed to focusing on the execution-driven issues (what and how), rather than the purpose-driven (why) business technology issues.</p>\r\n\r\n<p>That said, there are a couple of ways to begin to break this cycle, but both will take a company\u2019s commitment to altering at least some of its IT project approach. One is to allow the time on a new project to focus on the why, or purpose-driven questions: why that architecture, why that platform, why that technology stack, why that database approach, etc. And when time doesn\u2019t allow such an approach (which might be almost always), an alternative and perhaps more practical approach is to allow the purpose-driven questions (as opposed to the execution-driven questions) to be considered on subsequent project rework, maintenance and upgrade cycles. There\u2019s a risk to that, but there\u2019s also a risk of being late to the market with new technology capabilities, so those tradeoffs must be considered.</p>\r\n\r\n<p>Of course being part of a team considering the purpose-driven questions when implementing a project from the ground-up can provide valuable experience for IT developers and others. Among them are:</p>\r\n\r\n<p><ul><li>Ingraining the importance of considering the purpose-driven questions before starting on any subsequent IT project</li>\r\n<li>Providing time to evaluate technology and process patterns for the best fit for any particular project</li>\r\n<li>Offering time to dissect previous decisions made about architecture and database approaches</li> \r\n<li>Giving IT people an opportunity to discover and discuss the things they don't know about technology or project</li></ul></p>\r\n\r\n<p>As suggested earlier, recent experience on a project provided a valuable opportunity for asking such purpose-driven questions. The application had the same architecture and technology stack as the one from an immediately prior assignment. This provided a good opportunity to look at another application using the same technology stack, and evaluate more closely the why, instead of merely accepting the architecture and copying it blindly in the new project. Essentially the prior project could be used as a reference architecture comparison point leading to asking why it was designed as it was and why a specific approach was chosen for the project.</p>\r\n\r\n<p>This proved invaluable from an educational and a qualitative perspective and forced the kind of thinking and re-evaluations to occur that often don't happen in the first go-round of IT projects. In this latest project, the opportunity to consider the purpose-driven questions proved essential for professional development and skills growth, and the ability to carry the lessons forward into future projects.</p>\r\n\r\n<p>And from a more practical perspective, having more than just the architect understand why certain decisions have been made is a good hedge against sudden staff changes and critical knowledge walking out the door. When that happens, the use of ineffective patterns in the race just to complete the project at whatever the cost can quickly dilute the deliverables for any project. That said, the experience of being able to ask purpose-driven questions allowed for the reconsideration of the architectural approach rather than just accepting what was already in place. Asking the fundamental why questions also opened the doors to better problem-solving skills.</p>\r\n\r\n<p>After all, architecture or technology that works well for one project may not be the right thing for the next project. It may seem obvious in hindsight, but seeing and understanding the patterns and architectures \u2013 along with the reasoning \u2013 behind something can easily and quickly get lost if there isn\u2019t enough time to carefully think about an approach and ask questions about why things were done the way they were done. For this project, some of those why questions were things like:</p>\r\n\r\n<p><ul><li>Why was this application designed the way it was?</li> \r\n<li>Is this application solving the problem it was created to address, and why is it or is it not doing it well enough?</li>\r\n<li>Why was the architecture deployed chosen over others?</li>\r\n<li>Is there a better approach or why should this application remain in place?</li></ul></p>\r\n\r\n<p>Determinedly answering these questions has a direct impact on the overall quality of the project and its deliverables. In all, the purpose-driven approach can lead to benefits such as:</p>\r\n\r\n<p><ul><li><b>Technology Agility</b>: knowing when and when not to use specific patterns</li>\r\n<li><b>Maintainability</b>: hedging against IT staff changes by spreading knowledge</li>\r\n<li><b>Upgradability</b>: designing and implementing with upgrades in mind</li>\r\n<li><b>Integration</b>: understanding ahead of time prevents problems later</li>\r\n<li><b>Skills Development</b>: overall IT staff gets better</li></ul></p>\r\n\r\n<p>These and other benefits seem like as good as reasons as any to ask and answer the why questions. And while that might take a little more time up front, it will inevitably take several times longer to answer them after the project is in production.</p>",
    "tags": ["purpose-driven", "approach"],
    "nextMindShareId": "using-analytics-for-healthier-encounters",
    "nextMindShareTitle": "Using Analytics For Healthier Encounters"
  },
  {
    "id": "using-analytics-for-healthier-encounters",
    "title": "Using Analytics For Healthier Encounters",
    "authorName": "Oleg Issers",
    "authorTitle": "Architect",
    "authorImageUrl": "oleg-issers.jpg",
    "shortDescription":
      "To ensure data tells a comprehensive story, insurers are focusing on their claims analytics data, and particularly the idea of a Healthcare Encounter.",
    "isFeatured": false,
    "industry": "healthcare",
    "publishDate": "03/23/2018",
    "readTimeInMinutes": 7,
    "publishName": "Health IT Outcomes",
    "publishUrl":
      "https://www.healthitoutcomes.com/doc/using-analytics-for-healthier-encounters-0001",
    "content":
      "<p>It\u2019s no secret that a new business model has been emerging for the healthcare industry over the last several years. From its early days at the end of the previous century, the Managed Care model has slowly but surely been making its way into every nook and cranny of an industry desperately in need of reinventing itself. The reasons for this are many, but at its core, the promise of a healthcare delivery system organized to manage cost, utilization and the quality of healthcare services is just too promising to ignore.</p>\r\n\r\n<p>The Managed Care process has also introduced a very powerful concept to the industry: that of shared risk and shared rewards between healthcare providers and healthcare insurers. But while conceptually compelling, the actual implementation of the model in the real world of day-to-day healthcare in this country has proved daunting to say the least.</p>\r\n\r\n<p>One of the key success factors for the successful implementation of the Managed Care model \u2013 and specifically for the shared risk and rewards approach \u2013 is the utilization of high-quality data and the analytics it can produce. Without the data to accurately verify patient, provider and payer performance the whole model becomes sub-optimized. To make sure that the data does, in fact, tell a comprehensive story, many providers and insurers are focusing on improving their claims analytics data, and particularly the idea of a Healthcare Encounter.</p>\r\n\r\n<p>Simply put, a Healthcare Encounter is an interaction between a person (patient) and a healthcare provider. In the context of claims analytics as an example, providers are categorized as a professional (an individual medical practitioner) or a facility (a hospital, outpatient clinic, laboratory, etc.). Further, professional and facility providers submit different types of claims to payers (health insurance companies) for reimbursement. In the context of claims then, a Healthcare Encounter can be defined as a grouping of related claims for the same person, provider, payer and date range.</p>\r\n\r\n<p>An example set of business rules for a Healthcare Encounter can help illustrate this approach:</p>\r\n\r\n<img src=\"./assets/mind-share-img/oleg-image-1.png\" />\r\n\r\n</p>Illustration: Grouping of Facility and Professional Claims to build an Inpatient Encounter</p>\r\n\r\n<h1>Facility Inpatient Encounters</h1>\r\n\r\n<p>These typically span multiple days. Encounter date range is built by grouping related Facility Claims (same Person, Facility Provider, Payer, Diagnosis, and adjacent or overlapping date ranges). Any Professional Claims for the same Person within the date range are linked to the Inpatient Encounter; there may be claims from multiple Professional Providers (Practitioners) linked to the same Facility Encounter.</p>\r\n\r\n<p><ul><li>Skilled Nursing Facility or Nursing Home Encounter</li>\r\n<li>Hospice Encounter</li>\r\n<li>Home Health Encounter</li>\r\n<li>Acute Inpatient Hospital Encounter</li>\r\n<li>Inpatient Mental Health Encounter</li>\r\n<li>Inpatient Rehabilitation Encounter</li></ul></p>\r\n\r\n<h1>Facility Person-Day Encounter</h1>\r\n\r\n<p>These follow the same business logic as Facility Inpatient but only span a single day \u2013 no merging of Claims with adjacent dates. An assumption is made that any Professional medical services the Person received on the same day are related to the Person-Day encounter.</p>\r\n\r\n<p><ul><li>Observation Encounter</li>\r\n<li>Emergency Room Encounter</li>\r\n<li>Urgent Care Encounter</li>\r\n<li>Outpatient Surgery Encounter</li></ul></p>\r\n\r\n<h1>Facility Outpatient Encounter</h1>\r\n\r\n<p>Facility Claims are grouped by Person, Facility Provider, Payer and Service Date. No merging of dates or linkage of Professional Claims.</p>\r\n\r\n<p><ul><li>Dialysis</li>\r\n<li>Physical Therapy</li>\r\n<li>Occupational Therapy</li>\r\n<li>Radiology</li>\r\n<li>Lab/Pathology</li></ul></p>\r\n\r\n<h1>Professional/Ambulatory Encounter Types</h1>\r\n\r\n<p>Professional Claims are grouped by Person, Professional Provider (Practitioner), Payer and Service Date.</p>\r\n\r\n<p><ul><li>Administered Drugs, including Chemo Drugs</li>\r\n<li>Allergy Encounter</li>\r\n<li>Cardiovascular Encounter</li>\r\n<li>Surgery Professional Encounter</li>\r\n<li>Among others</li></ul></p>\r\n\r\n<p>The concept of a Healthcare Encounter is a powerful one. It starts to address the issue that many large providers encounter when accepting patient healthcare insurance from a multitude of insurers. The providers need a more efficient way to identify multiple insurance payers for the same patient.</p>\r\n\r\n<p>One way to think about the concept of Healthcare Encounters is to imagine a \u201Cday-in-the-life\u201D of a patient who has multiple outpatient appointments that might include a consultation, a procedure, blood work, physical therapy, etc., all from potentially different facilities and payers. The concept of the Healthcare Encounter pulls all these disparate encounters together into a more meaningful way to manage healthcare claims. Additionally, this concept goes right to the heart of the Managed Care model\u2019s requirement for the kinds of analytics that can truly produce the quality risk measures needed to accurately and effectively assess provider performance.</p>\r\n\r\n<p>There are, however, still obstacles to achieving the promise of the Healthcare Encounter concept. First and foremost, overall industry cooperation when it comes to data and analytics needs to improve. As of now, there is not a common and accepted set of data quality standards across the healthcare industry. Such standards are a critical part of the ability to create the kinds of advanced analytics required to support the dreams of the Managed Care model \u2013 inclusive of the Healthcare Encounter concept - in the industry. While progress has been made on optimizing and standardizing data code sets in the industry, there is still much work to be done.</p>\r\n\r\n<p>The healthcare industry is fast approaching a critical inflection point. There is wide consensus that the Managed Care model remains the future of the industry, leading to better overall health, service, and satisfaction for patients, and better overall process, efficiency, and profitability for providers and insurers. The key to really achieving all of that is data analytics for verification and continual improvement. In this area, much work needs to be done.</p>",
    "tags": ["analytics", "encounter"],
    "nextMindShareId": "enterprise-architecture-in-an-agile-world",
    "nextMindShareTitle": "Enterprise Architecture In An Agile World"
  },
  {
    "id": "enterprise-architecture-in-an-agile-world",
    "title": "Enterprise Architecture In An Agile World",
    "authorName": "Rob McCurley",
    "authorTitle": "Architect",
    "authorImageUrl": "rob-mccurley.jpg",
    "shortDescription":
      "It has become common to view enterprise architecture as something that time has passed by; however, this view is misguided and potentially risky.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "03/14/2018",
    "readTimeInMinutes": 5,
    "publishName": "ITA Pro",
    "publishUrl":
      "http://www.emagazine.itapro.org/Home/Article/Enterprise-Architecture-in-an-Agile-World/2100",
    "content":
      "<p>It has become a common refrain over the past few years to view the practice of enterprise architecture (EA) as something that time has passed by, much like using email or making actual phone calls on a smartphone. The ascent of agile methodologies and practices has seemingly relegated architectural concepts to the dustbin of history.</p>\r\n\r\n<p>This article will argue that the suggestions that EA\u2019s time has past are misguided at best, and potentially risky at worst to companies that view it in that context. That said, the sad truth is that EA has been misunderstood and misinterpreted over many years, and has earned its less than stellar reputation through poorly organized, designed and executed architectural efforts.</p>\r\n\r\n<p>In many insurers, EA has mostly failed to live up to its potential and promises. The reasons cited for this failure, familiar and in many cases justified, include:</p>\r\n\r\n<p><ul><li>EA is too methodical and process-heavy when agility is needed</li>\r\n<li>EA focuses on technology perfection rather than business practicality</li>\r\n<li>EA focuses on governance rather than business technology enablement</li>\r\n<li>Architects live in \u201Civory towers\u201D and avoid issues of day-to-day IT</li></ul></p>\r\n\r\n<p>If these reasons are valid \u2013 and they mostly are - then it\u2019s hard to escape the conclusion that EA is not only unnecessary, but it may even be viewed as a negative proposition. If you dig deeper, however, one finds that these are not failures of EA per se, but instead, they are failures of practitioners to adhere to the principles and concepts of EA itself.</p>\r\n\r\n<p>The purpose of EA is to enhance the interplay between technologies and business processes to better support and enable business needs and goals. When architects move away from this fundamental value proposition, they lose relevance to IT and among business leaders. Architects can regain relevance by refocusing their practices around two principles.</p>\r\n\r\n<p>The first principle of EA is the linkage of architecture and business. Architects often act as if their problem domain is technology. It\u2019s not. Architects, like everyone else in the company, are tasked with making the business more successful. Architecture teams currently misaligned with the business goals can realign themselves by making a few key adjustments:</p>\r\n\r\n<p><ul><li>Understand the Business: Architects must make time to learn about the non-technical aspects of their company's business. They should spend time observing business processes in action, and discuss business challenges and opportunities with business management at every level.</li>\r\n<li>Communicate in Business Terms: Architects sometimes project an uber-nerd persona. This can alienate business colleagues and even senior IT staff. Architects need to take their understanding of the business and communicate regarding solutions to business problems \u2013 reducing costs, lowering headcount, enhancing business processes, enabling customers, etc. They should use instructive analogies and stories. With self-awareness and coaching, architects can learn to communicate more effectively.</li>\r\n<li>Measure Architecture in Business Terms: More evolved architecture teams maintain metrics about their practice. But often these metrics will seem artificial or useless to outsiders. Architects should examine metrics they collect and affirm that they provide business-relevant information. Not that every parameter must relate directly to the corporate bottom line. Metrics about technology are fine as long as they are framed to give insight into how it supports the company.</li></ul></p>\r\n\r\n<p>Shifting to a business orientation doesn\u2019t necessarily force a wholesale overhaul of an architecture practice. The key activities of analysis, modeling and technology governance can continue, but they need to be reassessed through the lens of business success. Consider pruning activities that are hard to justify through this perspective.</p>\r\n\r\n<p>The second principle the modern architect should focus on is to embrace agility. Agility is used to mean different things, but overall it reflects the fact that everything moves at a much faster pace than it did a few decades ago.</p>\r\n\r\n<p><ul><li>Support Business Agility: Accept that different parts of any insurer\u2019s business run at different speeds. When speed to market is important, architectural standards can be relaxed. Prioritize modernization efforts around agility.</li>\r\n\r\n<li>Support IT Agility: Don\u2019t resist agile development methodologies \u2013 adopt a lighter touch to integrate with them instead. Be selective about the artifacts that the architecture team produces \u2013 create only enough and in enough detail to get the job done.</li>\r\n\r\n<li>Get in the Trenches: Develop, hire and otherwise obtain architects that don't mind getting their hands dirty with the tough day-to-day work of communicating, planning, developing, adjusting and communicating again. It's challenging to have an agile architecture that responds quickly to changing business and technology requirements if you're not plugged into the business functional areas and the people around the company that drive the requirements. This is essential and has benefits on multiple levels, not the least of which is demonstrating to the rest of the company that architects add value to the business efforts, have a deep understanding of the business needs and can effectively wed those to the most effective technology approaches.</li></ul></p>\r\n \r\n\r\n<p>Finally, in a world where the insurance industry is changing in dramatic and fundamental ways, EA is, in fact, more necessary than ever. The macro trends of digital transformation, customer centricity, mobile first and the demographics of next-generation customers have driven nearly all insurers to reconsider their strategic and operational models. As a result, almost all insurers are in the midst of some combination of core systems transformations, creating or enhancing mobile platforms, and partnering with the Insurtech community to bring innovation and creativity into their companies.</p>\r\n\r\n<p>This change requires insurers to take a fresh look at how they can best leverage the concepts and principles of EA for the long-term benefits it can provide. And, the good news is that none of this is pie-in-the-sky stuff. Instead, it's a common sense approach to reestablishing the practice of EA in an increasingly agile and digital world.</p>",
    "tags": ["enterprise architecture", "agile methodology"],
    "nextMindShareId": "how-providers-can-get-better-results-from-data-efforts",
    "nextMindShareTitle":
      "How Providers Can Get Better Results From Data Efforts"
  },
  {
    "id": "when-it-comes-to-large-projects-think-architecture-first",
    "title": "When it Comes to Large Projects, Think Architecture First",
    "authorName": "Oleg Sadykhov",
    "authorTitle": "Principal",
    "authorImageUrl": "oleg.png",
    "shortDescription":
      "Large information technology programs are often multiyear initiatives with large teams and large budgets, but this approach does not guarantee success.",
    "industry": "insurance",
    "publishDate": "01/25/2017",
    "readTimeInMinutes": 10,
    "publishName": "Insurance Canada",
    "publishUrl":
      "https://archive.insurance-canada.ca/ebusiness/canada/2017/Xby2-Project-Architecture-1701.php",
    "content":
      "<p>Large information technology programs are often multi-year initiatives with large teams \u2013 more than 20 to 30 people, often much larger \u2013 and budgets in eight figures. Examples include an implementation of a core system such as policy administration, claims or billing, or a large data-analytics project. The problem with large initiatives is that delivering them in a reasonable amount of time requires large teams, and large teams present large challenges such as:</p>\r\n\r\n<p><ul><li><b>Communication:</b> Smaller groups can communicate more efficiently and directly, but large teams require more communication and documentation that increase project overhead.</li>\r\n<li><b>Inefficient use of resources</b>: Team members either wait for one another to accomplish tasks they depend on or inadvertently break each other's functionality, which introduces rework.</li>\r\n<li><b>Uneven skills</b>: More-capable people can take on larger, more complex tasks, but it's very difficult to assemble a team of all-stars.</li>\r\n<li><b>Ramp-up and knowledge transfer:</b> Building the initial team, addressing turnover and other team configuration changes create the need for new people to quickly get up to speed.</li></ul></p>\r\n\r\n<p>On large projects all of this becomes even more challenging due to the size and complexity of the solutions. Because issues with large teams are well known, many modern software development methodologies (especially agile) stipulate that teams should be small. For example, in Scrum, an agile methodology, teams are typically cross-functional groups of seven or so. In an ideal world, large initiatives would be broken down to create a set of small teams that works almost independently and efficiently, and ultimately assembles their work products into the overall solution. Of course, most everybody tries to break large teams into sub-teams (also called work streams, tracks and the like), but very rarely is it done well, where a variety of challenges, including communication and inefficient use of resources, are truly addressed.</p>\r\n\r\n<p>This is where architecture comes in \u2013 and it's not talking about the selection of Java versus .NET, or what integration technology to employ, or what types of servers and networks to use - though these topics are all important. Instead, it's about architecture as decomposition: how big systems must be broken down into smaller pieces that will lend themselves well to independent work by teams. These pieces must be autonomous and loosely coupled. The task is not easy, but it's important since without a good breakdown of systems into parts (variously called subsystems, modules, services or bounded contexts; subsystems for purposes of this article) there will be no good breakdown of large teams into efficient and independent sub-teams. It's natural for sub-teams to form around subsystems. The stakes are even higher than just the team breakdown. If the system is not well-architected, it will result in a complex and inflexible solution. Cost overruns and major failures are inevitable.</p>\r\n\r\n<p>Every project team that has dealt with big system implementations has hit the \"wall.\" Suddenly, development tasks that used to take a week start to take a month and it's not clear why. This is a direct result of increasing complexity and insufficient architecture. If the parts of the overall solution overlap, then the teams responsible for these components will end up performing similar tasks. This leads to duplication of effort, inconsistent solutions and similar defects that show up in various parts of the system that seem elusive and hard to eliminate. Project teams will under perform if the architecture is poor. Inefficient communication paths between teams will start dragging the initiative down. Teams will be mired in coordination and dependency nightmares and spend their valuable time adjusting to the work performed by others.</p>\r\n\r\n<h1>Creating Good Architecture</h1>\r\n\r\n<p>Let's take a simplified example to illustrate the challenges. Assume that the project is the implementation of a policy administration system where insurance policies are stored and maintained. Further assume that customer information is also stored and updated by the same system. In order to break our system down into subsystems, a logical place to start might be to separate the functionality related to policies from the functionality related to customers. In this way we've created a policy subsystem that will deal with policy information, and a customer subsystem that will deal with customers. This is an example of a good architecture that has subsystems that are autonomous and loosely coupled. This creates the ability to make and release changes to these subsystems independently of one another. In this example, policy and customer subsystems should now be able to grow and evolve independently of each other.</p>\r\n\r\n<p>A typical system consists of user interface components, business logic components and data. When breaking the system down into subsystems, consideration is given to each of the layers listed. In order to achieve the independence of the subsystems, all of the layers must be dealt with - UI, business and data, including their corresponding business components. Separation of the business components is probably the easiest task (though still not easy) and as such it's the only thing that is typically attempted. Take for example the many implementations of Service Oriented Architecture over the past several years, where there is an incorrect belief that good architecture results from exposing business components such as Web services, which implies loose coupling and autonomy.</p>\r\n\r\n<p>But if both the customer and the policy subsystems continue to share the same underlying database, then any work done on customer functionality subsystem can inadvertently break the policy subsystem. In this configuration, when one subsystem is released, the others must be tested as well. And when the database is down, then both subsystems are down. So where is the autonomy? The same can be said of the UI. It's natural for the same screen to present information about both policies and customers, but the way such screens are often built further entangles the two subsystems. The challenge of architecture is to keep the criteria of autonomy and the loose coupling of subsystems very clearly in mind, and tackle the difficult issues such as data separation and the disentanglement of user interfaces to accomplish it.</p>\r\n\r\n<h1>Data Replication</h1>\r\n\r\n<p>So what's the problem with breaking the data down and letting the subsystems own their data in order to avoid one large database that supports it all? The problem is that the policy subsystem legitimately needs customer information, and the customer subsystem may also need to know information about the policies the customer owns. Clearly, the data needs to be shared, and there are a couple of choices for accomplishing that:</p>\r\n\r\n<p><ul><li><b>Store the data in one place and provide access to it.</b> In this scenario the customer data is truly owned and only accessible by the customer subsystem. To share data, the customer subsystem can either expose the data via Web services, or provide UI screens or widgets for others to use in order to access the data.</li>\r\n<li><b>Allow the partial replication of the data.</b> In this scenario, the replication of some of the customer data elements gets created so the policy subsystem can store it in its own database.</li></ul></p>\r\n\r\n<p>The idea of storing data in one place initially looks advantageous, but after a deeper look, it presents several challenges. In many data intensive situations, centralized data access presents a performance challenge. Many off-the-shelf packages expect the data they need to be stored locally in the application database (this is called a replication scenario). Some of the popular enterprise subsystems will be difficult to scale, since everyone will need the same data. They also will be hard to change without affecting everyone. Additionally, downtime of one subsystem will lead to the downtime of others that depend on it. The bottom line is that data replication is an important tool to achieve the goals of autonomy and loose coupling of subsystems.</p>\r\n\r\n<p>There is no hard-and-fast rule to say when data should be centralized and when replication makes sense. To decide, think about the amount of data in play, how much data processing is done by the subsystem that owns the data, how impactful the downtime of one subsystem will be on another, and so on. There is no hard-and-fast rule to say when data should be centralized and when replication makes sense. Of course, the moment one starts down the path of data replication is the moment one needs a good framework in order to implement data ownership and the corresponding rules required for the subsystems. One such framework is master data management, which is a tried-and-true way to track owners of data and the rules that should be followed.</p>\r\n\r\n<h1>CQRS Aids Architecture</h1>\r\n\r\n<p>Another relatively recent framework that helps one think about separation between subsystems, data ownership, and much more is called the Command and Query Responsibility Separation. The idea is fairly simple: It advocates separation of data that supports modifications of the system from data that supports inquiries. According to CQRS, systems should have two parts - one for data modifications and the other for data inquiries - each with their own databases. Even without fully following CQRS, the concepts introduced by CQRS are helpful when working on and discussing architecture concepts such as:</p>\r\n\r\n<p><ul><li><b>Commands:</b> Requests to modify data (that is, to create a new customer). Their names should be in the present tense and sound like commands. Their processing involves data validation, execution of business rules and changing the data.</li>\r\n\r\n<li><b>Events:</b> Notifications about data changes (such as, new customer created). Names of events should use past tense to indicate that they're just a notification of a change that's already occurred. Events notify interested parties that a data change has been approved and recorded. They're typically used to trigger other processing or to update decentralized data replicas.</li>\r\n\r\n<li><b>Queries:</b> Read-only requests to access the for specific data record details. Here's how the CQRS approach can be applied to our example of policy and customer subsystems. Policy and customer subsystems will \"own\" their data and be responsible for processing commands to change their data. Upon successful changes of data, they will publish events notifying the other subsystems. To accomplish this, a new subsystem is created \u2013 the operational data store (ODS) - that will combine data from policy and customer components in a way that is optimized for inquiries. The ODS will change its data in response to events published by the policy and customer subsystems. Therefore, to build a solution that involves interactions with the policy and customer subsystems, the ODS will be used to fulfill inquiries, and when data changes are made commands will be sent to the respective subsystems (policy and customer) to process them appropriately.</li></ul></p>\r\n\r\n<p>In summary, software architects should consider newer approaches such as CQRS when working through the challenges of the decomposition of their systems. The goal is to achieve autonomy and loose coupling of any subsystems. If successful, it can pave the way for small, independent, and efficient teams that have a much better chance to deliver large and complex enterprise initiatives successfully.</p>",
    "tags": ["insurance", "architecture"],
    "nextMindShareId": "the-key-to-small-project-management-keep-it-simple",
    "nextMindShareTitle":
      "The Key to (Small) Project Management: Keep it Simple"
  },
  {
    "id": "the-key-to-small-project-management-keep-it-simple",
    "title": "The Key to (Small) Project Management: Keep it Simple",
    "authorName": "Jeff Sallans",
    "authorTitle": "Senior Consultant",
    "authorImageUrl": "jeff-sallans.jpg",
    "shortDescription":
      "Can a methodology be slimmed down to something that fits the needs of a smaller project, but still provide enough structure to effectively manage it?",
    "industry": "insurance",
    "publishDate": "02/02/2017",
    "readTimeInMinutes": 7,
    "publishName": "Insurance Innovation Reporter",
    "publishUrl":
      "http://iireporter.com/the-key-to-small-project-management-keep-it-simple/",
    "content":
      "<q>There are ways to apply some of the best practices from larger project efforts to smaller projects, while not burdening smaller projects with unnecessary bureaucratic overhead.</q>\r\n\r\n<p>Today one sees many articles about standard practices and patterns for managing projects. But what if it\u2019s a small project or an ad hoc initiative that wasn\u2019t necessarily planned for? Can a standard methodology be slimmed down to something that fits the needs of a smaller project, but still provides enough structure to effectively manage it?</p>\r\n\r\n<p>When working on any sizable project, decisions are typically made for development and testing in the interest of resource and time trade-offs. On such projects, the questions are something like: \u201CShould we delay this project release to add this feature?\u201D or \u201CWe are pretty confident in this code, so should we decrease testing time in this area?\u201D  When it comes to smaller projects, why shouldn\u2019t these questions be applied to methodology and practices? The key is to identify what practices provide the most benefit, and more importantly, what practice can be simplified to use on a smaller project.</p>\r\n\r\n<p>A comparison of two larger and smaller projects might prove instructive. For this comparison, both are full-stack web development projects. One is a large insurance quoting project, and the other a small e-commerce project. The biggest difference between the two is their scale and timeline. To set the scene, here is a quick overview of the two project structures:</p>\r\n\r\n<p><b>Large Team Structure \u2013 Team of 18 \u2013 2-Year Timeline:</b></p>\r\n\r\n<p><ul><li>(2) Project Leads</li>\r\n<li>(2) Business Analysts (provides stakeholder sign-off)</li>\r\n<li>(4) Quality Analysts</li>\r\n<li>(2) Tech Leads</li>\r\n<li>(6-8) Developers</li></ul></p>\r\n\r\n<p><b>Small Team Structure \u2013 Team of nine \u2013 6-Month Timeline:</b></p>\r\n\r\n<p><ul><li>(3) Stakeholders</li>\r\n<br>Responsibilities include Business Analyst (provides stakeholder sign-off)</br>\r\n<br>Responsibilities include Quality Analyst</br>\r\n<li>(2) Part-time Project Leads</li>\r\n<br>Responsibilities include Tech Lead</br>\r\n<li>(2-4) Developers</li></ul></p> \r\n\r\n<h1>Estimated project durations:</h1>\r\n\r\n<p>Large project man-hours = (18 team members) x (24 months) = 432 working months</p>\r\n\r\n<p>Small project man-hours = (8 team members) x (6 months) = 42 working months</p>\r\n\r\n<p>The large project is approximately 10 times larger than the smaller project. Given this, how might the smaller team, working on the smaller project, simplify the project communication practices for effectiveness and efficiency? Here are some suggestions:</p>\r\n\r\n<h1>Daily Standup: General Team Communication</h1>\r\n\r\n<p>The purpose of a daily standup is to spread awareness about the team\u2019s work to keep everyone productive. In the larger project, the standup was between eight people and usually lasted 20 minutes. The biggest benefit of this process is coordinating between testers and developers.</p>\r\n\r\n<p>For the smaller project, the process was simplified by using email to report any standup details. However, this proved to be ineffective because emails couldn\u2019t duplicate the standup\u2019s quick back and forth discussion between team members. The compromise for the smaller project was to use a longer in-person meeting (four people and 30 minutes), which kept team members with multiple roles on the same page.</p>\r\n\r\n<h1>Design Reviews/Code Reviews \u2013 Developer-to-Developer Communication</h1>\r\n\r\n<p>Design and code reviews are typically done between fellow developers. A design review occurs when a plan or structure is reviewed before it is implemented. This is important to help create well-organized code and to reduce time when making improvements in the future. A code review occurs when the code written is reviewed before testing to help catch bugs, and to help with code consistency.</p>\r\n\r\n<p>For both large and small projects, design reviews are very important, especially at the beginning of the project because there are more opportunities to see the benefits of more easily extending existing functionality. Code reviews can be simplified for smaller projects because less communication is needed between developers when each person writes large sections of code. In many small projects, a single developer will write all the code for a certain feature. This removes a lot of developer coordination, and the code inherently gains the same benefits of reduced bugs and code consistency, while reducing the number of code reviews.</p>\r\n\r\n<h1>Business Engagement \u2013 Developer-to-Business Analyst Communication</h1>\r\n\r\n<p>Developer and business analyst interaction is crucial to a project, big or small. For those unfamiliar with the term business analyst, it refers to someone that understands the behavior and actions that the program will be responsible for executing. A useful way to think about it is that the business analyst knows the destination, and the developer builds the road to get there. Projects often veer off course when the two fall out of sync.</p>\r\n\r\n<p>For larger projects, this is usually addressed by adding a person to the development team dedicated to the business analyst role. For many smaller projects, however, that is not a practical option. More often than not, most members of small project teams must wear multiple hats and serve multiple roles. To simplify things for this purpose, it\u2019s often helpful to use a Google Document or something similar to act as a proxy for someone who would normally play this role full time on a larger project.</p>\r\n\r\n<p>In practice, developers would add a question to the document when they encountered a question while writing the code. Every other day, a business member of the team would respond to the questions posted. If a developer\u2019s question changed or was otherwise resolved prior to a response, they would update the Google Document, thus avoiding unnecessary work for the business members of the team.</p>\r\n\r\n<p>This method works quite well for quick questions about schema or the legacy systems but can be lacking when discussing new features. In those cases, a conference call is probably more appropriate.</p>\r\n\r\n<h1>Bug Reports: Developer-to-Quality Analyst Communication</h1>\r\n\r\n<p>For small project teams, everything is magnified. Things have to happen faster with fewer people, so finding the right tools that support the small project team structure is critical. As with developer-to-analyst communications, Google Docs or something similar can be quite helpful in the QA process.</p>\r\n\r\n<p>For larger projects, JIRA or something similar is a great tool for managing the QA process, but smaller project teams don\u2019t have that kind of tool luxury. That\u2019s why Google Spreadsheet fits this small project situation better than anything bigger and bulkier.</p>\r\n\r\n<p>In the end, having everything on one spreadsheet speeds-up data entry and offers a more holistic view of the data\u2014for a small project team, that\u2019s worth its weight in gold.</p><p>The lesson to be drawn here is that there are ways to apply some of the best practices from larger project efforts to smaller projects, while not burdening smaller projects with unnecessary bureaucratic overhead. The best practice of project communications can be adopted for this purpose, and those same principles can be applied to other project management practices.</p>\r\n\r\n<p>The key is looking at the current roles and communication practices of a smaller project team and asking these questions: Do the project team roles line up with the project team\u2019s communication practices? Or, is there a practice that seems unnecessary? If so, modify or eliminate it. Remember the benefit of a small project and a smaller project team is the ability to quickly change practices to suit the needs of the team. Use this to your advantage when simplifying your small project team processes to strike the balance that makes the most sense in order to meet the ultimate goal\u2014getting the project done.</p>",
    "tags": ["insurance", "project management"],
    "nextMindShareId":
      "how-neglecting-non-functional-requirements-makes-systems-non-functional",
    "nextMindShareTitle":
      "How Neglecting Non-Functional Requirements Makes Systems Non-Functional"
  },
  {
    "id":
      "how-neglecting-non-functional-requirements-makes-systems-non-functional",
    "title":
      "How Neglecting Non-Functional Requirements Makes Systems Non-Functional",
    "authorName": "Matt Flores",
    "authorTitle": "Architect",
    "authorImageUrl": "matt-flores.jpg",
    "shortDescription":
      "It's more often than not that the non-functional capabilities of a system determine its value and lifespan, and whether or not the company views it as a success.",
    "industry": "insurance",
    "publishDate": "03/17/2017",
    "readTimeInMinutes": 7,
    "publishName": "Insurance Innovation Reporter",
    "publishUrl":
      "http://iireporter.com/how-neglecting-non-functional-requirements-makes-systems-non-functional/",
    "content":
      "<q>Over the long term, it\u2019s more often than not that the non-functional capabilities of a system that determine its value and lifespan, and whether or not the company views it as a success, or just another sub-optimal delivery from IT.</q>\r\n\r\n<p>Whether partnering with a custom solutions provider, creating a home-brew application, or purchasing an out-of-the-box product, most companies will follow a standard approach for evaluating their needs for a new or updated system.  Commonly, the first step is to identify the core functional requirements that need to be satisfied. First and foremost, the solution must meet the base business and process needs. However, this is only half of the first step. These core functional requirements typically will only address how something works but not how well it works. To completely identify requirements includes considering the non-functional requirements (NFR) of the system. It can be argued that this second half of requirements is as important as the first, if not more so in some cases.</p>\r\n\r\n<p>Non-functional requirements differ from functional requirements in several important ways. Functional requirements describe how a system will work: what screens will users see, how the screens transition from one to the next, what data can be entered, how and when will the data be saved and retrieved, and so on. These are commonly seen through the lens of the primary users of the system, as they must have these requirements met in order to do their jobs.</p>\r\n\r\n<h1>Functionality and Quality</h1>\r\n\r\n<p>By contrast, non-functional requirements describe the quality of a system. This simple but broad description covers a number of attributes including performing well under normal <i>and</i> extreme usage patterns (performance and scalability), quickly adapting the application to changing or varying business rules (configurability), addressing productivity for novice and expert users (usability), and preventing unauthorized access or transfer of data (security). These sets of requirements can be seen and experienced by many stakeholders beyond just the primary end-users. Viewing it this way, it is clear that outlining and fulfilling only the functional requirements could lead to a half-baked solution.</p>\r\n\r\n<p>It is not uncommon or unexpected for a customer to be focused primarily on the functional requirements, as the visible and tangible assets can create the perception of a high-quality system. Often, non-functional requirements can be prioritized lower or treated as afterthoughts constrained by time, budget, and resources. This is a mistake.  Non-functional requirements have a significant impact not only on the architecture and design of a system, but also on the long-term total cost of ownership (TCO).</p>\r\n\r\n<p>To illustrate this, consider Amazon.com and the architecture, infrastructure, and hardware required to maintain a system serving over 150 million unique users per month.  Even when stripping the site down to the most basic features of e-commerce (search, add to cart, and checkout), having such an enormous user base demands constant up time, a highly responsive system, and an ordering process simple enough for your grandmother to send you a birthday present.  Hiccups in any of these areas could be costly.  In 2013, merely 30 minutes of down time cost Amazon nearly $2 million.  This is, of course, is an extreme example, and the engineers at Amazon have created a remarkably robust architecture to prevent disruptions, but the site would not function as well as it does without having considered the needs of the company beyond its front-facing capabilities.</p>\r\n\r\n<p>Given that non-functional requirements are not a new concept, one would hope that they have become so ubiquitous in software development that major applications should never experience such issues.  Unfortunately that is not the case.  In 2016, the launch of the highly anticipated mobile game Pokemon Go attracted over 50 million players in the first month after release.  The game gave players the chance to capture and power up fantasy creatures\u2014Pokemon\u2014in an augmented reality platform, an exciting experience for players familiar with the video game series. This massive userbase overwhelmed the game\u2019s servers often preventing users from logging in or reliably playing the game in the first few weeks. Additionally, features within the game, such as tracking nearby Pokemon, were slowly disabled and removed over time due to the volume of network traffic between the client and the servers. While features met their functional requirements well, it appears that non-functional requirements were not given adequate weight. Overall, this led to a poor user experience until the game stabilized.</p>\r\n\r\n<h1>The Bigger Picture</h1>\r\n\r\n<p>The importance non-functional requirements can be demonstrated to be more than stretch goals or afterthoughts by connecting their value to important business goals. Projected growth, potential features, geographic expansion, or the execution of a company\u2019s long-term strategy can be hindered by the dreaded \u201Csystem constraints\u201D born from neglecting NFRs.</p>\r\n\r\n<p>Given these observations, what are some considerations for specific non-functional requirements? Let\u2019s review two of the aforementioned non-functional requirements as a way to better understand see how they connect to business goals, and identify key areas of focus.</p>\r\n\r\n<p>A thoughtful product owner will consider how much and how quickly a system\u2019s user base will grow. Building a system only for the present or average load can limit growth and significantly limit business goals. Scalability non-functional requirements are as much a business issue as they are an IT issue. Though a solution can be quickly developed, tested, and deployed without considering how a system might function under increased load, addressing scalability concerns may be more difficult to solve on a live system than one still on the drawing board. When considering how much a system may need to scale, a product owner must consider when the system is under the most load, including what time of day and what time of year.  Perhaps most activity happens early in the morning or just after lunch. Maybe usage spikes significantly on weekends. Some industries, like retail, need to consider increased transactions leading up to major holidays. In any case, the system must be architected to handle peak loads as well as day-to-day traffic.</p>\r\n\r\n<p>How might systems handle spikes in demand? A straight forward tactic is to scale-up existing hardware with improved components thus increasing overall processing power. A system can also scale-out by adding additional hardware and servers to serve more concurrent users. However, a scalability bottleneck can occur in any layer of a system\u2019s architecture. Aside from the application running on its main hardware, less obvious areas to review include available network bandwidth, database responsiveness, and dependencies on external services and integrations.</p>\r\n\r\n<p>Aside from hardware concerns, a system must be easily usable by its core audience to be a success.  Usability as a non-functional requirement category entails too many parts to detail here. However, a key component that cannot be understated or overlooked is an understanding of what types of users will access the system. What about users who rarely access the application? Can such users stay familiar with the flow and nuances of the screens from session to session at that usage frequency? Are on-screen tips and a wizard-supported workflow helpful to guide users through a process? When accommodating novice or infrequent users, the application must have an intuitive and consistent layout to be accessible.</p>\r\n\r\n<p>Conversely, an expert user who works on the system daily may find this level of hand holding more annoying than helpful. For power users, the layout of screens can focus on completing as many tasks as possible in an efficient way.  This could mean that instead of a set of screens to enter one set of data, one screen with a different layout could allow for multiple entries at once with less on-screen help. Neither design here changes what data is stored from a user, thereby changing no functionally required fields, but the non-functional usability requirements must be assessed before the workflow is built to maximize the user experience.</p>\r\n\r\n<h1>Not Just How, But How Well</h1>\r\n\r\n<p>As solution providers increasingly deliver more functional, more stable, and more flexible software systems, the project leadership must consider not only how the product will work, but how well it will work.  To separate and neglect non-functional requirements\u2014or to prioritize them far below or after functional requirements\u2014is to tacitly approve building an inherently flawed and incomplete system. To meet the business\u2019 current and long-term goals, the product owners, architects, business analysts, and technical leaders must take a holistic approach to defining new features and updates.  Over the long term, it\u2019s more often than not that the non-functional capabilities of a system that determine its value and lifespan, and whether or not the company views it as a success, or just another sub-optimal delivery from IT.</p>",
    "tags": ["insurance", "requirements"],
    "nextMindShareId": "architecting-an-agile-enterprise",
    "nextMindShareTitle": "Architecting an Agile Enterprise"
  },
  {
    "id": "architecting-an-agile-enterprise",
    "title": "Architecting an Agile Enterprise",
    "authorName": "Samir Ahmed",
    "authorTitle": "Principal",
    "authorImageUrl": "samir-ahmed.jpg",
    "shortDescription":
      "In the insurance industry the need to innovate is dire. The common thread weaving through the industry is the role of technology; it is expected to lead the charge.",
    "industry": "insurance",
    "publishDate": "04/10/2017",
    "readTimeInMinutes": 7,
    "publishName": "Insurance Technology Association",
    "publishUrl":
      "http://buyersguide.itapro.org/Article?area=&articleID=289&channel=Solution%20Partner%20Views",
    "content":
      "<p>In the insurance industry, be it life, health or P&C, the need to innovate is dire. Disruption is the word of the hour. Discussions around innovations in core systems, business processes, omnichannel experience for producers and policyholders, and digitization (e.g. e-application, e-signature, e-delivery, etc.) abound.</p>\r\n\r\n<p>The common thread weaving through all of these discussions is the role of technology: It is expected to lead the charge. And why shouldn't it? Technologies such as social networking, mobile computing, big data, neural networks, machine learning, and artificial intelligence, are all expected to disrupt the marketplace, and pay big dividends to those willing to make the bet. <i>Venture Scanner's Insurance Technology Market Overview and Innovation Quadrant for Q1 of 2017</i> shows them tracking 1,050 start-up companies in 14 categories across 54 countries with a combined $17 billion in funding.</p>\r\n\r\n<p>For carriers, often viewed as evolving slower than a speeding microorganism, the challenge is figuring out how to participate in the market disruption that presumably awaits their fate. According to the venerable Harvard Business Review (HBR), the answer lies in being agile. In the May 2016 edition, in an article titled \"Embracing Agile,\" the authors posit that the same agile innovation methods that have worked wonders for software development teams over the past 25 to 30 years are ripe for the picking as innovation methods of choice in a broad range of industries and business functions. In other words, agile approaches are no longer just for skunkworks projects; they are the way to innovate.</p>\r\n\r\n<p>With HBR's recommendation understood, the challenge for carriers is figuring out how to architect their enterprises to be (more) agile. To frame this portion of the discussion in the proper context, let's define a few key terms.</p>\r\n\r\n<p><ul><li>By enterprise, we mean the three-legged stool of people, process, and technology that supports any given business function, and more specifically in our current discussion, any given innovation project.</li>\r\n<li>Next, by agile, we are not referring to any of the specific agile software development methodologies such as Extreme Programming (XP), Scrum, Kanban, and their ilk. Rather, we're referring to the underlying values that make a given approach or methodology agile. In a manner of speaking, we're discussing not just \"doing agile,\" but also \"being agile.\"</li>\r\n<li>Finally, by architecture, we`re referring to the structure or structures of a system, which comprise elemental building blocks of the system, the externally visible properties of those building blocks, and the relationships among them. This is an adaptation and generalization of the definition of software architecture, as defined by Bass, Clements, and Kazman in the 2nd edition of Software Architecture in Practice, published by Addison-Wesley in 2003.</li></ul></p>\r\n\r\n<p>Since we`re borrowing a methods leaf from the software development community, we should address an issue many agile software developers would take with mixing in architecture. Agile software developers have an aversion to heavy and seemingly endless documentation of a system, which, in traditional methods, would be the outcome of an activity they would call \u201Cbig design up front\u201D or BDUF.</p>\r\n\r\n<p>Instead, they prefer an emergent design that`s the result of following the principles of \u201Ckeep[ing] it simple\u201D (KISS) and \u201Cyou aren`t going [to] need it\u201D (YAGNI), the latter a reference to attempts to build product features and capabilities that are not required currently, but are being anticipated as being required at some future point.</p>\r\n\r\n<p>Such disdain for architecture by agile software developers is misplaced. The reality is that in viewing architecture as defined herein, we find that every system has an architecture; it's only a matter of whether that architecture is intentional or accidental. Even the \"emergent design\" that agile software developers gravitate towards is the outcome of operating in some context that is pre-defined.</p>\r\n\r\n<p>To borrow an analogy from construction, when building a house, one can certainly start out by building a single story structure, but if one anticipates needing to build a second story at any point, it had better be factored into the foundation. Not doing so will require tearing down the structure and rebuilding with a stronger foundation. This is an example of a feature that will not emerge. On the other hand, adding an additional room to the first story could certainly be an emergent feature, issues of physical space notwithstanding.</p>\r\n\r\n<p>As to living the values of agility in order to be agile, in 2001, a group of individuals branding themselves as the Agile Alliance promulgated an Agile Manifesto as a way to codify the values of agility they adhered to. While these were in the context of software development, the premise of HBR's recommendation is that agile methods are broadly applicable, so it's worth discussing.</p>\r\n\r\n<p>The manifesto states, \"\u2026 we have come to value: individuals and interactions over processes and tools; working software over comprehensive documentation; customer collaboration over contract negotiation; [and] responding to change over following a plan. That is, while there is value in the items on the right, we value the items on the left more.\" Let's imagine how these values might be seen in action on a project team.</p>\r\n\r\n<p>In assembling a team, valuing individuals and interactions over processes and tools can manifest itself in several forms. First, team members should not be assigned to the project by managers. Instead, positions on the team should be filled much the same as any open position is filled in a company: through a process of applying for the position, interviewing candidates, and reaching a joint commitment to join the team between the project leads and the candidates. In doing so, the bar on required skills should be kept high, and not compromised due to the presence of other factors. The skills being sought should be a versatile set of skills, enabling team members to perform a variety of tasks. They should not be a narrow and specialized skill-set, constraining team members only to perform tasks within their specialized area of skill. The team should look more like a military special operations team, rather than a complete army.</p>\r\n\r\n<p>Second, in valuing a working product more than a specification of it, we should measure our progress regularly and often. The only measure of progress should be demonstrable features and capabilities. Descriptions and status reports will not suffice. The purpose of this is to see the progress in action directly against the desired final outcome. Measuring frequently enables the team to detect issues early, allowing the team to address them sooner rather than later.</p>\r\n\r\n<p>Third, valuing collaboration more than contract negotiation can have many manifestations as well. For instance, teams should be collocated, with access to spaces for face-to-face interactions and collaboration. Since most organizations have teams comprising individuals spread across different locations, this isn't always possible. In such cases, the team should invest in real-time collaboration tools such as web and video conferencing, shared editing and drafting tools, instant messaging, etc. Investing in such tools even for completely collocated teams has benefit. As basic as these might sound, many carriers have not yet made these investments.</p>\r\n\r\n<p>Another manifestation would be to have the innovative project be a full-time focus, at least for key team members. Having competing commitments and differing availability between team members prevents spontaneous collaboration, requiring interactions to be formalized, which slows things down at best, and prevents progress at worst. Finally, the team ought to be empowered to make decisions on the spot during the collaborative interactions. When such empowerment is lacking, decision point has to be taken back to the true power of authority, thus undermining the effectiveness of the collaborative process.</p>\r\n\r\n<p>Finally, in valuing responding to change more than following a plan, I'm reminded of a quote from President Eisenhower: \"In preparing for battle I have always found that plans are useless, but planning is indispensable.\" A manifestation of this is engaging in the planning process regularly, and frequently. Such checkpoints allow the team to adapt based on feedback it`s receiving. They also create an opportunity for the team to benefit from new information that might have become available. Thus, at every checkpoint, the plan going forward can be refined.</p>\r\n\r\n<p>The aforementioned are just a few practical examples from real-life experiences in executing agile projects that are manifestations of living the values of agility. While the practices are examples of \"doing agile\" it's important to tie them back to values of \"being agile,\" without which they end up being mindless habits whose benefit start diminishing over time. As these values spread to more areas of any organization, the enterprise will inevitably become more agile in the best possible sense.</p>",
    "tags": ["architecture", "agile"],
    "nextMindShareId": "life-insurance-applications-a-better-way",
    "nextMindShareTitle": "Life Insurance Applications: A Better Way"
  },
  {
    "id": "life-insurance-applications-a-better-way",
    "title": "Life Insurance Applications: A Better Way",
    "authorName": "Yunus Burhani",
    "authorTitle": "Senior Architect",
    "authorImageUrl": "yunus-burhani.jpg",
    "shortDescription":
      "Life insurers are racing to become more innovative as a way to improve overall customer experience; specifically through the initial application process.",
    "industry": "insurance",
    "publishDate": "05/11/2017",
    "readTimeInMinutes": 5,
    "publishName": "Insurance-Canada",
    "publishUrl":
      "https://archive.insurance-canada.ca/poladmin/canada/2017/xby2-better-life-applications-1705.php",
    "content":
      "<p>Like the rest of the insurance industry, life insurers are racing to become more innovative as a way to improve the overall customer experience. While it's fair to say that life insurers are lagging behind the health and property and casualty verticals when it comes to innovation and customer engagement, there still are inroads being made. That said, there's room for a lot more improvement. One example of this, and one that touches a potential customer directly, is the initial application process for life insurance.</p>\r\n\r\n<p>Over the years life insurers have tried many approaches to enhance the buying experience for customers, but with little success. However, life insurance is still unique enough in terms of underwriting information that it requires potential customers to engage with an agent in order to find the right product for them. For better or worse, it is not as easy as buying auto insurance online \u2013 a process that is for the most part automated across the personal lines space and is widely accepted by consumers as the process for acquiring that insurance. However, a typical life insurance company sells a number of complex products, each with its own pros and cons that need to be carefully evaluated on an individual customer basis. In many respects it is more akin to investing money in financial sector products, which often requires engaging a broker, a certified financial planner, or some other financial expert.</p>\r\n\r\n<p>That's why the life insurance application process \u2013 and transitioning that to the underwriting process \u2013 continues to be an issue for most life insurance companies. Some of the issues that typically arise during the process are:</p>\r\n\r\n<p><ul><li>The application is not fully filled in by an agent.</li>\r\n<li>The insurer's web application is too strictly edited for required information so agents often try to avoid filling them out online and submits paper application.</li>\r\n<li>Not all the necessary underwriting requirements could be fulfilled because some application forms are not completely filled out.</li>\r\n<li>The scheduling of evidence vendors are not set at the time of the application, so it becomes difficult to reach out to a potential customer afterward.</li>\r\n<li>Applications are not signed electronically so insurers and consumers have to wait for wet signatures to occur.</li></ul></p>\r\n\r\n<p>These seem like relatively simple problems to overcome, but despite the perception they persist in the life insurance industry. So in a world growing increasingly accustomed to instant information and response gratification, life insurers run the risk of being perceived as backwards from a customer service and technology perspective, and with some justification.</p>\r\n\r\n<p>For example, while the quoting process for life insurance is pretty straightforward and does not require much data entry from a potential customer, once the potential customer agrees to the quote, the application process typically takes a long time and requires an extensive amount of data entry. Depending on the number of potentially insured's seeking coverage, the list of questions could easily go for an hour or two. This requires the customer(s) to take time out of their busy schedules in order to visit an agent's office, often for an extended period of time. Further, some of the questions on the application require the agent to physically see potential insured's, thus eliminating the option to have the conversation over the phone. A further complication can occur when some of the questions asked are HIPAA sensitive in nature, sometimes leading to awkward situations for potential insured's when they are required to answer such questions in front of others.</p>\r\n\r\n<p>Given all of that, the question is what can be done about this cumbersome application process? One possible approach would be a hybrid one that would allow the agent and the potential customer to each complete a part of the application. In this approach, an agent can initiate the quoting process over the phone, since it does not require lot of data to be entered. Once the customer agrees with the quote the agent can schedule a thirty-minute office interview with the customer to narrow down the possibility of life, health and annuity insurance products and better explain the best options for customer. During that same visit agent would initiate the application process electronically, and once they completed their portion send it electronically to the customer to complete the rest.</p>\r\n\r\n<p>As part of this self-serve approach, an evidence vendor scheduling system can be integrated allowing customers to schedule their exams appointments online. In the case where the insurer and/or local regulation requires the agent to be present when the customer signs, this suggested approach still makes it easier as the agent does not have to spend the time asking all the involved parties medical questions individually. The parties could be given tablets or use their cell phone to answer those questions in a lobby just like what occurs in a doctors office today. Once the signature is executed \u2013 either in person or electronically \u2013 the executed application can be forwarded to the insurer to initiate the underwriting process.</p>\r\n\r\n<p>Such an approach would significantly streamline the application and underwriting processes for the life industry by providing customers a convenient way to apply for a policy without having to sit in front of an agent for a long time answering questions about their medical and financial histories. This application process also has the potential to speed up the new business and underwriting process for insurers, since they do not have to deal with incomplete applications, or applications coming in without a signature. They would also know that all the underwriting requirements have been ordered during the application process, and as a result would not have to bother with calling customers to schedule an appointment, or sending application information to evidence vendors so they can call the potential customers. Finally, since this streamlined application process does not allow agents to fill in all of the customer information (almost always erroneously), it will reduce the amount of paper applications received by insurers greatly, which will eliminate the burden on the New Business department to key in paper applications before any underwriting starts.</p>\r\n\r\n<p>There's an old saying that first impressions \u2013 good or bad ones \u2013 tend to stick with people. In today's rapidly changing insurance environment, life insurers should be motivated to put their best customer service foot forward by making it as easy and simple as possible for potential customers to navigate the application and underwriting process. Any life insurer who fails to do so could find themselves losing potential new customers to their competitors.</p>",
    "tags": ["application", "customer experience"],
    "nextMindShareId": "the-bigger-they-are-the-tougher-the-talent",
    "nextMindShareTitle": "The Bigger They Are, The Tougher The Talent"
  },
  {
    "id": "the-bigger-they-are-the-tougher-the-talent",
    "title": "The Bigger They Are, The Tougher The Talent",
    "authorName": "David Packer",
    "authorTitle": "Principal",
    "authorImageUrl": "david-packer.jpg",
    "shortDescription":
      "Finding and keeping talented people for critical business technology programs and projects remains a difficult challenge across the insurance industry.",
    "industry": "insurance",
    "publishDate": "06/01/2017",
    "readTimeInMinutes": 7,
    "publishName": "HIT Leaders & News",
    "publishUrl":
      "https://us.hitleaders.news/the-bigger-they-are-the-tougher-the-talent/",
    "content":
      "<p>Finding and keeping talented people for critical business technology programs and projects remains a difficult challenge across the insurance industry. When it comes to large, transformational type efforts, the talent problem gets even more difficult.</p>\r\n\r\n<p>The projects can be multiyear, the risk to the organization can be high, the technology and process issues and problems can be daunting, and it can be difficult to keep talented people engaged on longer projects when they are in demand for other projects in other parts of the organization.</p>\r\n\r\n<p>That said, these large, transformational projects are now more common in the industry as insurers continue their efforts to modernize and innovate. Unfortunately, the effort required for finding the right talent is compromised in the name of quickly staffing these large projects. The compromised strategy often involves blindly staffing from vendor \u201Cstrategic partners,\u201D professional services firms and staff augmentation firms, often leading to increased cost and poor quality solutions.</p>\r\n\r\n\r\n<p>So is there a better way to go about staffing large projects with the right talent?</p>\r\n\r\n<p>In fact, there is a better approach, and it is based on a set of general principles that organizations often shortcut when staffing up. These principles are:</p>\r\n\r\n<p><ul><li><b>Quality over quantity.</b> The caliber of the people involved is the single most critical element to project success. Be patient and take the time needed to staff based on the quality of the people versus meeting arbitrary staffing goals that lead to filling the team quickly and compromising on quality.</li>\r\n\r\n<li><b>Project success trumps team member development.</b> The health and welfare of the project should not be compromised in the interest of giving less qualified employees \u201Csome experience doing this kind of stuff.\u201D Large, high-risk projects cannot afford to be training grounds.</li>\r\n\r\n<li><b>Recruiting and staffing is a continuous effort, not one and done.</b> Attrition, role rotation and performance issues will ensure that recruiting and staffing will be an ongoing process, so just plan for that eventuality up front. It makes for a healthier team environment anyway.</li>\r\n\r\n<li><b>Recruiting well takes time and resources; don\u2019t underestimate it.</b> For example, let\u2019s assume that it might take two interviews to qualify every candidate and that for every five candidates there is one candidate that is selected. If the project team needs 50 people, that equates to 500 interviews to staff a 50-person team, not to mention the effort that goes into the other essential aspects of recruiting, such as candidate screening, coordinating scheduling, interviewing and decision making. The overall project plan should account for a full-time resource to handle this process, and for several other team leads spending 20-30 percent of their allocated time on the interviewing process, at least at the outset of the project.</li>\r\n\r\n<li><b>Have an onboarding plan.</b> It is important to recognize onboarding as an essential part of the recruiting and staffing effort, and not an afterthought. Proper onboarding takes time and effort, but it goes a long way in increasing the overall effectiveness of individuals on the project and of the project team overall.</li></ul></p>\r\n\r\n<p>So let\u2019s break this down a bit. Just as it is with the technology and process portions of the project, recruiting for the overall project should have its plan, complete with ownership and accountability, for its success. The recruiting plan should consider the resources, time, effort and expense needed to staff the project initially and to rotate and replace team members continuously.</p>\r\n\r\n<p>A good place to start is at the top \u2013 start with the project leadership team before thinking about appropriate technical, project management or process skills. It\u2019s critical that team leads with strong skill sets are in place before the rest of the team is recruited, as the team leads will be vital to the rest of the recruiting process.</p>\r\n\r\n<p>Next, create actual job descriptions based on the project\u2019s needs. The descriptions should clearly define the skills and capabilities required for each role.</p>\r\n\r\n<p>Beyond the technical, it is imperative to consider the softer skills of candidates \u2013 communication, leadership, working with others, etc. While there is a need and place for specialist skill sets for particular focus areas, on balance it is a good practice to seek out more flexible people who are well-rounded and can adapt and grow as the project evolves.</p>\r\n\r\n<p>Once the team leads are in place, and the descriptions are created, it is time to create the rest of the recruiting team. The recruiting team will be responsible for the day-to-day screening, interviewing and placement for the rest of the project team.</p>\r\n\r\n<p>For a 50-person project team, an initial recruiting team of one or two people working full time on this task should suffice. However, this is a long-term task, so the recruiting team will persist throughout the lifecycle of the project. Plan accordingly.</p>\r\n\r\n<p>As in most worthwhile pursuits, time is the key element. Finding the right talent and resources takes time, so take that into account when planning.</p>\r\n\r\n<p>In the example previously given, staffing a team of 50 people can take a lot of time when being picky about who is selected. It will likely take hundreds, if not thousands, of person hours to find the right 50-75 (alternates for when attrition begins) for the project team.</p>\r\n\r\n<p>The same is true for the on-boarding process. With a larger project team, it is often more efficient and effective to onboard groups rather than individuals. A good approach is to set a regularly scheduled on-boarding day on a monthly basis for incoming team members. That session should include a general orientation of the project, processes and the overall team, and include any necessary training.</p>\r\n\r\n<p>Training should also include specific tools the team is using for collaboration and communication. And as talented as people may be, it will also take them time to ramp up to their specific roles in the project, so that needs to be built into the overall onboarding timeline.</p>\r\n\r\n<p>Additionally, finding the optimal mix of in-house and outside talent for any large project is often a delicate dance. Team chemistry versus overall team talent and competency should be given some consideration.</p>\r\n\r\n<p>However, training and \u201Cgrowing\u201D in-house resources should not come at the cost of the project. Similarly, satisfying contractual obligations with staff augmentation and consulting firms should not sway team member selection. Rather, the focus needs to stay on those resources that bring the most talent to the table and whose presence will improve the chances for project success the most.</p>\r\n\r\n<p>In this case, one great developer is more valuable than 10 average developers. And if organizational politics begin to get in the way of assembling the very best team for the effort, be sure to tie project responsibilities and accountabilities to those making the political waves \u2013 that usually calms the waters.</p>\r\n\r\n<p>The end goal in all of this is to put the best team together for that large project. Doing so immediately reduces the project and the organizational risks and increases the likelihood of project success, while allowing talented people to do what they do best.</p>",
    "tags": ["talent", "recruiting"],
    "nextMindShareId":
      "healthcare-data-sharing-is-only-as-good-as-your-data-standards",
    "nextMindShareTitle":
      "Healthcare Data Sharing is Only as Good as Your Data Standards"
  },
  {
    "id": "healthcare-data-sharing-is-only-as-good-as-your-data-standards",
    "title": "Healthcare Data Sharing is Only as Good as Your Data Standards",
    "authorName": "Saifuddin Bhagat",
    "authorTitle": "Senior Consultant",
    "authorImageUrl": "saif-bhagat.jpg",
    "shortDescription":
      "In this era of insurance industry innovation and rapid agile software development, many people blanch at the constriction that standards often represent.",
    "industry": "healthcare",
    "publishDate": "06/08/2017",
    "readTimeInMinutes": 7,
    "publishName": "Becker's Health IT & CIO Review",
    "publishUrl":
      "https://www.beckershospitalreview.com/healthcare-information-technology/healthcare-data-sharing-is-only-as-good-as-your-data-standards.html",
    "content":
      "<p>Standards are a funny thing. In this era of insurance industry innovation, Insurtech startups, and rapid agile software development and distribution, many people blanch at the constriction that standards often represent.</p>\r\n\r\n<p>While there is a well-established standards approach for medical treatment coding, healthcare data standards are noticeably lacking. That is important because standards are the essential ingredient for optimizing advanced analytics and the information value derived from a well-designed standards structure.</p>\r\n\r\n<p>The healthcare insurance vertical, like most other verticals in the insurance industry, has struggled with pulling together the vast treasure troves of provider and patient information in their many and sundry data stores. Many reasons for this are familiar to all: older systems with poor data editing capabilities; proprietary data stores that require specific data formats but are incompatible with other systems; and operational data stores that create data quality degradation to name a few. However, many of these problems have a root cause based on the fact that most healthcare organizations do not have strong data standards as a part of their overall business and technology ecosystem.</p>\r\n\r\n<p>An example of this issue using a couple of pretty common healthcare data elements should prove illustrative: the difference between insurance eligibility and attribution enrollment. With a solid set of data standards in place, the first order of business is defining the data elements once and only once for use by all interested systems:</p>\r\n\r\n<p><ul><li>Eligibility is health insurance coverage that one has for a certain or fixed period</li>\r\n<li>Enrollment is signing up for coverage or program for a certain or fixed period, or derivation of a relationship for a certain or fixed period. A good way to think about enrollment is as a relationship, much like a marriage or friendship as it exists over some period.</li></ul></p>\r\n\r\n<p>While it might seem mundane, these standards-driven definition distinctions are important for creating analytical insights and actionable value from this data. Because one of the foundations of analytical insight is data sharing across systems and platforms, it is critical to know when a particular data element was created and when the data contained therein was last updated. The same holds true for any subsequent subsets of data. This means that there may be a number of \u201Cdate\u201D elements that are defined in a particular way. For instance, a \u201Cdate\u201D definition for when a file was created, versus a \u201Cdate\u201D definition for when that file is updated, and a subsequent file is created. While this sounds elementary, more often than these initial data standard markers are not present in healthcare technology systems. Here are a couple of examples why that is a problem:</p>\r\n\r\n<p><ul><li>Eligibility data needs to be shared when building an analytical claims population file that includes the subset of the claims population a given set of claims represents, and if the date spans are based on either service dates as opposed to paid dates. That is important as there is often a lag between when a patient is seen by a doctor, and when that service is billed.</li>\r\n<li>For enrollment, an attribution enrollment analytical file needs first to include the date described time span of the data necessary to derive the attribution and for what period the attribution is valid. An example of this is a patient seeing a doctor for the first time then making an attribution of the patient-doctor relationship valid for some period that includes the past, present, and future.</li></ul></p>\r\n\r\n<p>In both cases, a standards-based definition of the various date elements required is crucial for analytical data quality and insight. So how does one define data standards that optimize healthcare data sharing?</p>\r\n\r\n<p>To start, it is important to fix any communication problems among the various parties involved. For instance, providers usually hire an external vendor or buy a COTS product for analytical needs. However, the claims data needed for provider data analysis is usually provided by the payers. In this scenario, there are often two contracts - one between the payer and the provider for the claim, and the other between the provider and the analytical service vendor for the reports. More often than not, the two contracts have different communication protocols, different points of contact, and different delivery timelines. As a result, the provider often plays the role of the middleman, exchanging questions and answers between the payer and the analytics vendor. Many times the communication happens during the beginning of the contract but then fades into an ad hoc basis only. Adding to the problem, there is often no service level agreement in place for questions and verifications between the parties.</p>\r\n\r\n<img src=\"./assets/mind-share-img/Saif_Picture_1.jpg\" />\r\n\r\n<p>To correct this common situation and make sure all of the parties are working synchronously, there needs to be some upfront collaboration between all the potential stakeholders before the creation and execution of the various contracts. The purpose of this collaboration is to ensure that each party understands the business goals and the analytical needs of the provider \u2013 both tactical and strategic. This collaboration will help the team understand what data is available; what will be provided to whom; and what will be required and expected to meet the data goals. This collaboration should then continue at regular intervals during the entire course of the contracts. This will provide a platform to ask questions and inform the group of any immediate or future changes to the data or process. There should be a medium set (i.e. Jira, MS Excel, or some other collaboration toolset) for asking questions and sharing relevant data.</p>\r\n\r\n<img src=\"./assets/mind-share-img/Saif_Picture_2.jpg\" />\r\n\r\n<p>Once these high-level agreements are in place, it is time to get into the details of what will allow the parties involved to work in an efficient and effective manner on behalf of the provider. For example, all medical claims cannot be provided in one file. There is a clear distinguishing feature between facility, professional and Rx claims. These claims should be provided in separate files, but if they are provided in one file, there should be a clear indication of that. Also, since claims files are usually created on a monthly basis from the source, the data is often extracted based on service date. This results in an incomplete file because not all claims in that service month might have made it into the system. Hence, the easiest and safest choice is to extract data based on paid dates. This will always be a complete set with no duplication from month to month.</p>\r\n\r\n<p>Furthermore, a primary file should always have a supporting file (e.g., Provider_Rx_022016.txt should also have Provider_Rx_022016_support.txt). The support file has information such as when the file was created and its corresponding eligibility file. Similarly, an attribution file will have look-back dates, validity date, etc. Another possible option would be that all files come bundled in a zip format with a summary file. Of course, it is understandable that for privacy purposes claims related to STD, mental illness, or drug abuse are not shared. However, not sharing such data might lead to incorrect analytical output. A solution for this is to effectively mask the member information and provide the rest so that it can be included in the analytics. Finally, it is important to create a mandatory and industry specific list of columns populated with relevant data before sharing. A good example of this is that a claims file should always contain the service date, paid date, member id, POS, ICD code, servicing provider, etc. An eligibility file should contain an effective date, end date, member id, subscriber id, coverage info, etc.</p>\r\n\r\n<p>All of the above can be thought of as part of a data standards approach that will lead to efficient and effective data sharing. Once these elements and standards are in place, providers should be able to create insightful and actionable data analytics that provide more effective patient care and more efficient financial management.</p>",
    "tags": ["data", "standards"],
    "nextMindShareId":
      "why-clinical-document-architecture-doesnt-solve-data-quality-issues",
    "nextMindShareTitle":
      "Why Clinical Document Architecture Doesn't Solve Data Quality Issues"
  },
  {
    "id": "why-clinical-document-architecture-doesnt-solve-data-quality-issues",
    "title":
      "Why Clinical Document Architecture Doesn't Solve Data Quality Issues",
    "authorName": "Mohammed Hussain",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "mohammed-hussain.jpg",
    "shortDescription":
      "CDA, for all its promises of better-organized and better-quality data, is not perfect. Its weakness is due, in part, to what it is able to do—exchange data.",
    "industry": "healthcare",
    "publishDate": "07/05/2017",
    "readTimeInMinutes": 7,
    "publishName": "HealthData Management",
    "publishUrl":
      "https://www.healthdatamanagement.com/opinion/why-clinical-document-architecture-doesnt-solve-data-quality-issues",
    "content":
      "<p>Clinical Document Architecture, for all its promises of better-organized and better-quality data, is not perfect. Its weakness is due, in part, to what it is able to do\u2014exchange data.</p>\r\n\r\n<p>To be sure, it is no secret that healthcare data has a quality issue, whether it\u2019s technical or otherwise. In fact, from a developer\u2019s perspective, health data is really at the mercy of those who treat patients and enter data into their records.</p>\r\n\r\n<p>However, it is possible to give doctors feedback on the data that they are entering to improve its usefulness. Why do they need such feedback? A big reason is the loose definition of Clinical Document Architecture (CDA), which is able to place results in many fields in the typical Electronic Medical Records (EMR).</p>\r\n\r\n<p>The CDA is a standard that provides the ability to place data in records, enabling clinician notes to be transferred as free text entry. For better or worse, that leaves developers at the whim of the data that a human has manually entered into an EMR program. It is difficult to tell a system what to do when the word \u201Cwheelchair\u201D is entered in a field for the height of a patient, or the word \u201Cchildhood\u201D is used for recording the date of a procedure in a surgical history questionnaire.</p>\r\n\r\n<p>EMR vendors do not take the time to educate doctors and other medical staff properly on how to enter data, especially after systems are running and operational. EMR systems typically do not run any analytics on information entered\u2014usually, they just do edit checks to ensure the data is valid. However, the systems do not ensure that entered data is actually accurate and useful.</p>\r\n\r\n<p>To extend the previous example, an entry such as \u201Cwheelchair\u201D is technically valid for height, weight, blood pressure, allergy, favorite soft drink and smoking habits, among other things. This scope of variation is bad for data analytics efforts, because it mixes invalid data with valid data.</p>\r\n\r\n<p>An analytics mindset needs to have a desire to drive down healthcare costs by allowing better risk calculations while, at the same time, helping doctors and other medical staff provide better healthcare by identifying which patients need treatment, and how that treatment needs to be organized.</p>\r\n\r\n<p>Physicians and other medical staff are increasingly receptive to feedback offered about how to improve data entry. In many cases, they just need the proper education. This means that with the proper analytics approach, the benefits of better data entry can provide tangible and actionable benefits for providers, patients and payers.</p>\r\n\r\n<p>And this is where software architecture can help out. CDA is an XML-based structure designed to contain any number of Continuity Care Documents (CCD). The documents are used for tracking patient data. However, while they both use XML as their document structure standard, the use of that XML for these purposes has not been stringently defined.</p>\r\n\r\n<p>Here is are examples of how two EHRs handle the \"effective time\" field:</p>\r\n\r\n<p><b>EMR1</b><br />&lt;effectiveTime value=&quot;2015-06-03&quot;&gt;<br />&lt;low&gt;&lt;/low&gt;<br />&lt;high&gt;&lt;/high&gt;<br />&lt;/effectiveTime&gt;</p><p><b>EMR2</b><br />&lt;effectiveTime&gt;<br />&lt;low&gt;&lt;/low&gt;<br />&lt;high&gt;&lt;/high&gt;<br />&lt;/effectiveTime&gt;</p><p>Those examples reflect the same patient data generated by two different EMR systems, and it is clear that the data is just different enough to be a programmatic nightmare. While the document structure is common between source systems, the location of values is not. In the example, the effective time is located somewhere in the 'effectiveTime' XML tag, but that is all the standard guarantees. To resolve this requires customization for each EMR, and potentially for each practice.</p>\r\n\r\n<p>Furthermore, there is currently nothing in the third-party software market that could truly and properly parse a CCD for the information needed to resolve this kind of issue. Although plenty of products can create and send CCDs, almost nothing can parse them out into a model with which a data analyst can work.</p>\r\n\r\n<p>One way to circumvent this is by parsing CCDs into a tabular format so that they can be used in an analytics program. Building a custom CCD parser that creates a configuration file that tells the parser exactly where to get its information from is a good place to start.</p>\r\n\r\n<p>To solve the problems posed by the example above, the parser created two configuration files. When parsing the data from EMR1, the parser pulls in that configuration and knows that to get the effective date; it needs to utilize the value attribute of the main 'effectiveDate' tag. For EMR2, the parser gets the effective date from the literal value of the child 'low' tag of the 'effectiveDate' tag.</p>\r\n\r\n<p>The key is the ability to rely on the fact that even though each EMR does it differently, a single EMR still does it consistently. That means for EMR1, one would always expect to find the effective date in that value attribute. The parser also accounts for different EMR versions, so if a specific EMR has multiple releases, a configuration can be created for each specific version. The parser can also allow for customization per medical practice, so if different practices use the same EMR version, but one adds more data than another, configurations can be simply added to pull that additional data.</p>\r\n\r\n<p>The payoff from this approach is the ability to collapse many different CCDs into a single set of tables upon which analytics can be performed. The parsing solution that was built was designed to be scalable. Clinics receive CCDs in real time, as patient encounter records are created. They are processed in a batch format and parsed in bulk every night by queuing them up and pushing them through the parser before being released to the data mart. However, the code could just as easily be repurposed to queue up records run in real time as loads increase.</p>\r\n\r\n<p>This is but one, albeit an elegantly effective one, approach for dealing with some of the inherent problems of a Clinical Data Architecture running headlong into real-world data entry in the medical world. It is an approach that begins to resolve the discord between the theoretical and the practical, and as a developer, it does not get much better than that.</p>",
    "tags": ["data", "quality"],
    "nextMindShareId":
      "non-core-insurance-system-modernization-matters-too-part-one-assessment",
    "nextMindShareTitle":
      "Non-Core Insurance System Modernization Matters Too: Part 1 - Assessment"
  },
  {
    "id":
      "non-core-insurance-system-modernization-matters-too-part-one-assessment",
    "title":
      "Non-Core Insurance System Modernization Matters Too: Part One - Assessment",
    "authorName": "Priyanga Manoharan",
    "authorTitle": "Senior Architect",
    "authorImageUrl": "pri-manoharan.jpg",
    "shortDescription":
      "Many insurers neglect the non-core systems they have that provide much of their day-to-day operational capability, struggling with how to approach and scale their efforts.",
    "industry": "insurance",
    "publishDate": "07/12/2017",
    "readTimeInMinutes": 5,
    "publishName": "Insurance Innovation Reporter",
    "publishUrl":
      "http://iireporter.com/non-core-insurance-system-modernization-matters-too-part-one-assessment/",
    "content":
      "<p>Core systems modernization efforts\u2014defined here as pertaining to the policy, claims and billing functions\u2014have dominated the industry over the past several years. Insurers continue to pour millions of dollars and significant human resources into the substantial work required to modernize legacy systems. At stake is an insurer\u2019s ability to stay competitive in its current markets and the potential to compete in new and emerging markets by employing systems that are more adaptive, responsive to changing market conditions and focused on the customer experience.  However, in all of that activity, the need to modernize non-core systems similarly remains. Many insurers have struggled with how to approach and scale their efforts for systems that don\u2019t have the bulk or sizzle of the newer core systems. This three-part article will suggest an approach for dealing with non-core systems in a way that matches the resources and efforts required to the potential benefits derived.</p>\r\n\r\n<p>The assessment process for non-core systems is similar to the process for core systems but with some tweaks to contextualize the approach for the size and scale of the project effort. In either case, the assessment begins with a thorough analysis of the current and future state business challenges and opportunities, the solution provider space and the functional parameters of the systems that might be considered. The tweak here is that while this might be an elaborate process that includes a specialized software product to manage the assessment and analysis, smaller non-core systems don\u2019t necessarily require that kind of investment. A thorough analysis can still be done with more moderate tools.  With non-core systems, the processes are simpler and don\u2019t require as detailed or as lengthy of analysis.</p>\r\n\r\n<p>For many insurers, the move to a modern non-core system might mean a move from a homegrown platform with homegrown processes.  It\u2019s important to keep in mind that while the systems may be lacking in modern touches, those who have been accustomed to working on them may show some reluctance in letting them go. These tend to bias the requirements for the new system toward acting and behaving like the legacy system, potentially adding time and training to the overall process. The senior management team can also significantly impact the evaluation and solution selection process. For example, a company might have a culture of working a certain way or of being reticent to change. If the leadership can make the decision to shift the company culture, it can significantly help with finding the best solution fit.</p>\r\n\r\n<p>The next step is a review of the relevant vendor space. Here again, there are some tweaks to the standard assessment process because of the non-core nature of the systems being evaluated:</p>\r\n\r\n<p><ul><li>The solution provider space for non-core systems is typically smaller, so there will likely be fewer options to consider. Technology, hosting, integration and reporting are a few factors that may help to narrow the field.</li>\r\n<li>Besides from a few exceptions, the solution providers are also typically not large companies themselves. Many have somewhere between 20-50 employees in the organization. This can be good and bad news.</li>\r\n<li>The good news is that smaller solution providers mean easier access to the provider\u2019s leadership. This can lead to quicker review and decision times.</li>\r\n<li>The bad news is that smaller solution providers, while willing and cooperative, don\u2019t typically have the bandwidth for a steady stream of change requests for their system.</li>\r\n<li>When it comes to non-core systems, there are also international providers in the mix. These companies typically have U.S. clients and support multiple currencies.</li></ul></p>\r\n\r\n<p>Like core systems analysis, both functional and non-functional requirements are analyzed and scored, solution providers perform in-person demonstration where possible, and references are provided and contacted. The tweak with references for non-core systems is that it\u2019s often easier\u2014and better for decision-making\u2014to visit a client of the provider to see their solution in action. The smaller functional footprint of most of these systems means that the users of them are knowledgeable about them and are often willing to talk about all of the positives and negatives about the system. Another important aspect of the assessment is any solution provider\u2019s ability to deliver. It\u2019s not unusual for smaller solution providers to over-commit themselves. Keep the following in mind:</p>\r\n\r\n<p><ul><li>Functional proof of concepts and technology deep dives are needed to ensure that what is \u201Csold\u201D actually exists. There is a larger chance of overselling with smaller solution providers than with the larger ones.</li>\r\n<li>Ask about in-flight implementations and how they\u2019re being resourced. Many smaller providers typically don\u2019t have the bandwidth to support multiple implementations at the same time, so the timeline of when they\u2019re available for the next implementation is key.</li>\r\n<li>Build timing flexibility into the analysis and proposed project plan. The project may need to be delayed until a time when the provider\u2019s teams are available.</li>\r\n<li>Request dedicated resources and be sure to carefully review the depth and breadth of experience of those resources.</li></ul></p>\r\n\r\n<p>Additionally, most non-core systems rely heavily on integration with feeder systems and other data sources. Be sure to carefully assess the relative maturity of the integration points and protocols of any system under consideration. As opposed to larger core systems, the sophistication of the integration architecture can vary widely in smaller non-core systems. Request specific documents that depict the architectural structure of any potential solutions as that\u2019s a great way to assess the architectural strengths and weaknesses of the overall platform. Finally, it\u2019s important to understand that many non-core systems do not have robust reporting capabilities\u2014sometimes that\u2019s intentional. Many smaller solution providers will work with the insurer to customize the kind of reporting required and to identify the required data integrations. This can be time-consuming, so it needs to be accounted for in the overall project plan once a solution is selected.</p>\r\n\r\n<p>In part two of this article, the focus will shift from assessments analysis to contracts and implementations.</p>",
    "tags": ["systems", "assessment"],
    "nextMindShareId": "a-short-guide-to-emerging-website-architectures",
    "nextMindShareTitle": "A Short Guide to Emerging Website Architectures"
  },
  {
    "id": "a-short-guide-to-emerging-website-architectures",
    "title": "A Short Guide to Emerging Website Architectures",
    "authorName": "Jeff Sallans",
    "authorTitle": "Senior Consultant",
    "authorImageUrl": "jeff-sallans.jpg",
    "shortDescription":
      "Front-end experience was previously limited to static websites that were dull for the customer and tedious for the architect or developer to maintain. That has all changed.",
    "industry": "insurance",
    "publishDate": "08/02/2017",
    "readTimeInMinutes": 5,
    "publishName": "Becker's Health IT and CIO Review",
    "publishUrl":
      "https://www.beckershospitalreview.com/healthcare-information-technology/a-short-guide-to-emerging-website-architectures.html",
    "content":
      "<p>It\u2019s no secret that mobile applications and games have upped the ante on the kinds of interactive experiences that customers have come to expect.</p>\r\n\r\n<p>Several years ago that front- end experience was limited to relatively static websites that, while rendering useful information and some connectivity, were dull for the customer and tedious for the architect or developer to maintain and update. That has all changed over the past few years, and today\u2019s organizations face intense pressure to create websites and mobile apps that are not only useful but also are intuitive, engaging and interactive. These kinds of experiences for customers \u2013 and potential customers \u2013 are becoming core to many organizations\u2019 customer service and retention approach, as well as to their branding and messaging appeal. The experiences that a consumer has with an organization\u2019s technological front door are quickly becoming one of the key aspects of acquiring and keeping a customer. With this change in mind a new architecture has been created called MVVM (Model-view-viewmodel):</p>\r\n\r\n<img src=\"/assets/mind-share-img/sallans-image-1.jpg\" />\r\n\r\n<p>MVVM makes the website\u2019s user experience comparable to that of a mobile application by increasing the speed of navigation and business logic by ten times. This increase is possible because MVVM moves the business logic that previously was coded on the server to the front-end, as shown below. With the majority of the codebase moving to the front-end, architectural tools are being developed to organize large front-end code bases to help maintain code quality. The following illustration highlights the way in which MVVM enables improved user experiences:</p>\r\n\r\n<img src=\"/assets/mind-share-img/sallans-image-2.jpg\" />\r\n\r\n<p>The rest of this article focuses on two of the architectural frameworks currently available \u2013 Angular from Google and React from Facebook \u2013 as a way to highlight these front-end technology trends.</p>\r\n\r\n<p>The good news is that both Angular and React produce great front-end experiences for customers. However, both frameworks accomplish that differently. That\u2019s important to project managers, tech leads, architects and developers who may be responsible for putting together the best UIX team for their organization.</p>\r\n\r\n<p>The Angular framework is better organized for development, despite requiring more coding by the architects and developers. Google has prefabricated the architecture so that most of the work of naming and defining architectural elements for consistency is already done. While there are tradeoffs when it comes to coding versus having to create and define a set of naming conventions, Angular is a more practical and ready-to-use framework in general.</p>\r\n\r\n<p>The React framework is less prefabricated, so that means it is better suited for talented developers and architects who don\u2019t mind coding the architectural infrastructure. That work enables more design flexibility and granularity when using React, along with advantages in testing and overall performance. For instance, using React allows the decoupling of the business logic layer from its - or some other subsequent - UI framework. React accomplishes testing through its virtual document object module that helps to make user experience modeling and testing much easier to perform. That said, React is still more of a \u201Cframework of a framework\u201D that requires some up-front work than Angular. The following illustration visualizes the capabilities and differences of these new front-ends, and what Angular and React bring to the party respectively:</p>\r\n\r\n<img src=\"/assets/mind-share-img/sallans-image-2.jpg\" />\r\n\r\n<p>In both cases, there are additional considerations--not the least of which is a learning curve if these frameworks are new to the organization. Both frameworks require practice and patience when it comes to coding and testing to find the best fit for the organization. Both frameworks also require updated infrastructure that might not be in place. Potential new tools and libraries for managing smaller and more numerous bits of JavaScript are but one example.</p>\r\n\r\n<p>In the end, most organizations will only need one of these frameworks. If there are already strong architectural skills in place, then React might be the better choice. The depth and breadth of its user experience capabilities, along with a more efficient and effective experience modeling and testing approach, makes React a stronger framework overall. It provides all the necessary tools and capabilities required to create responsive, flexible, and hopefully, retentive front-end customer experiences.</p>\r\n",
    "tags": ["architecture", "framework"],
    "nextMindShareId":
      "non-core-system-modernization-matters-too-part-two-contracts-and-implementation",
    "nextMindShareTitle":
      "Non-Core Insurance System Modernization Matters Too: Part 2 - Contracts & Implementation"
  },
  {
    "id":
      "non-core-system-modernization-matters-too-part-two-contracts-and-implementation",
    "title":
      "Non-Core Insurance System Modernization Matters Too: Part Two - Contracts & Implementation",
    "authorName": "Priyanga Manoharan",
    "authorTitle": "Senior Architect",
    "authorImageUrl": "pri-manoharan.jpg",
    "shortDescription":
      "Implementations for non-core systems follow many of the same best practices as for core systems, but with a few tweaks for scale and scope.",
    "industry": "insurance",
    "publishDate": "08/09/2017",
    "readTimeInMinutes": 5,
    "publishName": "Insurance Innovation Reporter",
    "publishUrl":
      "http://iireporter.com/non-core-insurance-system-modernization-matters-too-part-two-contracts-implementation/",
    "content":
      "<p>Part one of this article was a discussion regarding an approach for assessing and analyzing non-core systems (just about anything besides the policy, claims and billing functions) that suggested ways to scale the effort to match the potential benefits derived from the system. This article will focus on two of the other elements necessary for modernizing non-core systems: contracts and implementation. The premise of this installment is to provide a framework that insurers can use to treat their non-core system acquisitions and implementations as if they were core systems, but adjusted for scale and scope.</p>\r\n\r\n<p>When it comes to contract negotiations with many of the smaller solution providers that sell and support non-core systems, the good news is that the negotiations are often conducted with the owner or other senior members of the company. This usually means that decisions and changes can be discussed and made more quickly than with larger solution providers. It also means there may be fewer layers of lawyers and procurement managers involved. However, non-core solution providers often have their challenges and nuances, so among other things, it\u2019s important to:</p>\r\n\r\n<p><ul><li>Take extra care with the contracts to ensure that appropriate service level agreements have been defined. Many smaller providers have limited resources and can (and will) over-commit to what they can do and how quickly they can do it to close a deal.</li>\r\n<li>If the solution provider is hosting or is using a third-party hosting service, then an annual security review ought to be included in the contract beyond the initial review. Cyber liability coverage should be considered as well, either through the solution provided via an addendum in the contract or through an independent insurer specializing in such coverage.</li>\r\n<li>The contract should include a copy of the code that can be placed in escrow. This is particularly important for smaller solution providers, who may or may not have the financial wherewithal to stay in business for as long as the insurer uses their product(s). However, obtaining the code doesn\u2019t always mean that standing up an environment to house and protect the code is easy or feasible, so the time it might take to do that should be part of the overall project plan.</li>\r\n<li>Including a clearly defined list of functionality and expectations in the contract can also help mitigate the possibility of overselling by any of the vendors. Although it may take a bit of time, it is worth the investment so that the vendor is contractually bound to deliver the functionality.</li></ul></p>\r\n\r\n<p>Implementations for non-core systems follow many of the same best practices as for core systems but with a few tweaks for scale and scope. Practices like leveraging the functionality of the non-core system out of the box to avoid customization and scope creep and avoiding the mistake of comparing the new functionality to any of the incumbent systems (often homegrown) remain valid.  That said, there are some wrinkles to be aware of when implementing non-core systems as follows:</p>\r\n\r\n<p><ul><li>Smaller solution providers with multiple clients don\u2019t typically have a deep bench to handle the flow of change requests that always occur once testing begins. Implementation timelines should take into account that the vendor development team is on the critical path.</li>\r\n<li>Don\u2019t fall into the trap of attempting to make the non-core system compensate for functionality gaps and shortcomings in the core systems. This is especially important when it comes to data gaps or inconsistent data.</li>\r\n<li>Take into account the community using the non-core system. Many will be transitioning directly from older non-core systems, but others may be part of the group already exposed to modern core systems. This comes with the burden of ensuring that the user experience is a comparable one. Wherever possible, have the non-core system users use the new system and core system users stick to their core systems.</li>\r\n<li>As a result of not being a core system, reporting and analytics are usually under-developed in many non-core systems. Although the total user group might be small, the impact of processes can be significant if reporting and analytics structures need to be modified for the new system. For example, a premium audit group managing $200M in annual premium can typically recover 1-2 percent per year in premium leakage by using data analytics. If that capability is not immediately available in the new system then the cost it takes to replicate that functionality is more than just the time and resource burn for the technical effort.</li>\r\n<li>Similarly, if moving from a paper process to an automated one in the modern non-core system, then expectations have to be created and managed about what functions will be introduced over what period. It can be a fundamental process shift from a reactive analytics and reporting mode (usually paper-based) to a proactive one (usually automated). With the implementation of a modern non-core system, new data points are available, and companies should carefully consider their reporting options.</li></ul></p>\r\n\r\n<p>Finally, and as with modern core systems, good solution providers will take the time to understand the client\u2019s needs and processes prior to any development. This will eliminate the round-peg-into-the-square-hole issue that can occur. If the solution provider is not starting there, then it\u2019s incumbent on the client to begin that process to ensure that the capabilities are mapped to the appropriate business processes. Also, initial releases should be quick and contain as much out of the box as possible. Adjusting or updating the system as it matures with feedback from its actual use will be much more effective than attempting to build a \u201Cperfect\u201D system in a whiteboard setting.</p>\r\n\r\n<p>Following this suggested approach should benefit the assessment and implementation of any non-core system. While these systems are often in the background and don\u2019t get the same kind of attention and fanfare as modern core systems, they are essential to the ultimate success of any insurer. Part three of this article will focus on post-implementation best practices, including sustaining and maintaining non-core systems in the most efficient and effective ways.</p>",
    "tags": ["systems", "implementation"],
    "nextMindShareId": "avoiding-communication-breakdowns",
    "nextMindShareTitle": "Avoiding Communication Breakdowns"
  },
  {
    "id": "avoiding-communication-breakdowns",
    "title": "Avoiding Communication Breakdowns",
    "authorName": "Hatim Kader",
    "authorTitle": "Project Quarterback",
    "authorImageUrl": "hatim-kader.jpg",
    "shortDescription":
      "There's an adage that the 3 most important elements in real estate are all the same: location. The same is true for consulting, but replace location with communication.",
    "industry": "insurance",
    "publishDate": "08/17/2017",
    "readTimeInMinutes": 5,
    "publishName": "Insurance-Canada",
    "publishUrl":
      "https://www.insurance-canada.ca/2017/08/17/xby2-communication-breakdowns/",
    "content":
      "<p>There\u2019s an old real-estate adage that claims the three most important elements in any real estate sale are all the same one: <i>location, location, location</i>.  The same can be said for the practice of consulting by replacing the word \u201Clocation\u201D with the word that represents the most important element of consulting: <b>communication.</b></p>\r\n\r\n<p>Yes, the technology can be challenging, and yes, understanding the business goals and problems is vitally important, and yes, identifying the best-fit solution with the highest probability of success is key.  But none of these elements mean anything if they are not effectively communicated to the client, project team, solution provider, etc. Many projects have foundered as a result of inefficient and ineffective communications to the key stakeholders.</p>\r\n\r\n<p>Communication can take many forms, and it is such a vast and deep subject that after many years of practicing effective communication that I find myself learning and re-learning some of its basics time and time again. This is not to say that effective communication requires a Psychology or Marketing degree either. It just requires some common sense, and more often than not, some persistence.  For instance, anybody who has been a parent, a manager of people, or even in IT, knows that it\u2019s much easier to gain support for an idea or action if the party being communicated to believes that they helped to create said idea or action.</p>\r\n\r\n<p>Put another way, it is much easier if the other party believes it to be <i>their</i> idea. This is not to suggest that deception, coercion, or even subterfuge be employed.  Rather, it is to suggest that understanding the other party, their perspectives and building a working relationship that\u2019s based on mutual respect and trust helps to create the environment that yields successful outcomes.</p>\r\n\r\n<p>In the real world of consulting and project management, it takes some experience to recognize when and where there\u2019s a potential for communication issues. Many times the problem or issue is stark, and the eventual solution is evident; however, there could be other enterprise environmental constraints that influence the decision-making. Often things may seem clear on the surface, but just below the surface, there are obstacles to communicating effectively any proposed solution.  It\u2019s important to work to understand such things. For example, an organization might have some underlying experiences that make committing to a particular solution more difficult.  These experiences could be some of the following:</p>\r\n\r\n<p><ul><li>The organization is cautious or averse to solutions they haven\u2019t experienced, like cloud-based applications or storage solutions.</li>\r\n<li>There may be push back over concerns of initial and ongoing costs.</li>\r\n<li>There may have been prior bad experiences with a particular type of product (ex: COTS solution) or a particular provider.</li>\r\n<li>There may be cultural or technical resistance to a proposed solution due to the potential impact on long-held practices and processes.</li>\r\n<li>There may be political infighting that prevents progress from occurring.</li></ul></p>\r\n\r\n<p>Where such enterprise environmental constraints might be in play, a quick jump to the conclusion often carries the risk of a good solution proposal being rejected due to the real or imagined constraints. These could lead to lost opportunities for major transformational initiatives with high direct and indirect costs to the organization as a result of misunderstanding and miscommunication of the necessity for and benefits of the proposed solution. Good consultants should make note of such enterprise environmental constraints. As part of their overall strategy for understanding business problems and evaluating best-fit solutions, consultants should also devise strategies for getting their enterprise stakeholders to grasp the problem and assess the proposed solution options.</p>\r\n\r\n<p>To avoid this unpleasant but all too common consulting mistake, there are a number of communication approaches that I\u2019ve employed over the years that can help:</p>\r\n\r\n<p><ul><li>The deliberative approach. This approach allows time for the problem and its shortcomings to be understood, usually through persistent and understandable communication. It helps the stakeholders to grasp the problems involved fully.  While it may take a little longer, the benefit of this approach is that the stakeholders will discuss solutions naturally and often of their own accord.</li>\r\n<li>The team exercise approach. This method involves engaging the stakeholders in exercises that acknowledge the current state, analyze the gaps that need to be remediated to reach the desired future state and discuss potential solutions \u2013 technology-based or otherwise.</li>\r\n<li>The anticipation approach. This approach involves more than just anticipating questions, a common practice for good consultants.  Rather, this approach requires a deep understanding of the enterprise environmental constraints \u2013 technology, resources, budget and revenue, talent, priorities, market pressures, etc. \u2013 and preparing and presenting multiple options that challenge those limitations and/or address those constraints and their relative pros and cons.</li></ul></p>\r\n\r\n<p>These common sense approaches have been around for quite some time, but in the heat of the project battle they are often forgotten or overlooked. The importance of effective communication between all key stakeholders is on the critical path for any successful business technology initiative.  However, even with experience using these techniques as a guide effective communication can be difficult to achieve and maintain.  In my own experience, there are any number of variables that can impact successful communication.  Despite my best efforts as a consultant, I am not always as successful as I\u2019d like to be when establishing communication protocols with clients.  It\u2019s fair to say that sometimes it works better than others, and sometimes a favored approach needs to be abandoned for something else that might prove to be more effective.  The key is to understand the audience, the environment, the constraints, and then to tailor a communication approach that is the best fit for the combination of elements.  The final, most important key is to keep learning.</p>",
    "tags": ["communication", "consulting"],
    "nextMindShareId":
      "non-core-insurance-system-modernization-matters-too-part-three-sustainability",
    "nextMindShareTitle":
      "Non-Core Insurance System Modernization Matters Too: Part Three - Sustainability"
  },
  {
    "id":
      "non-core-insurance-system-modernization-matters-too-part-three-sustainability",
    "title":
      "Non-Core Insurance System Modernization Matters Too: Part Three - Sustainability",
    "authorName": "Priyanga Manoharan",
    "authorTitle": "Senior Architect",
    "authorImageUrl": "pri-manoharan.jpg",
    "shortDescription":
      "The problem with maintaining non-core systems after production implementation is that they never get the priority and resource dedication that core systems typically get.",
    "industry": "insurance",
    "publishDate": "09/18/2017",
    "readTimeInMinutes": 5,
    "publishName": "Insurance Innovation Reporter",
    "publishUrl":
      "http://iireporter.com/non-core-insurance-system-modernization-matters-too-part-three-sustainability/",
    "content":
      "<p>Parts one and two of this series focused on assessing and implementing non-core systems (defined as just about anything other than policy, claims, and billing systems) that suggested ways to prioritize and scale the efforts required when modernizing non-core systems. This, the third part of the series, focuses on how to overcome the challenges inherent in supporting and maintaining non-core systems post production. As with the first two parts of this article, the premise remains on suggesting a framework that insurers can use to treat their non-core systems as if they were core systems, but adjusted for scale and scope.</p>\r\n\r\n<p>The primary problem with maintaining non-core systems after their production implementation is that they are non-core systems and because of that, will never get the priority and resource dedication that core systems typically get. With core systems, there are often large teams of dedicated resources that are committed to the effort for years due to the strategic, operational, and financial importance of the system or systems. This makes perfect sense in an insurance landscape where modern core systems are the table stakes for many of the other innovations and initiatives insurers want and need to take on.</p>\r\n\r\n<p>With non-core systems, however, the project teams are not nearly as large and are often disbanded and shifted to higher priority projects just as soon as the non-core system is implemented in production. More often than not the business group is left managing and supporting the system with a skeleton support staff that may not be dedicated to that effort, and may not even include any IT resources.  This often creates a cascade of problems:</p>\r\n\r\n<p><ul><li>A business group left to their own devices to manage the newly installed system has other day-to-day priorities, and this reality will inevitably slow down the overall adoption and maturation of the new system.</li>\r\n<li>Modernized non-core systems typically introduce new processes and functions to business areas. To get to production, many customers defer some of these enhancements to hasten the delivery of the new system. That creates a post-production backlog of potential enhancements and features that nobody has the time to investigate.</li>\r\n<li>That backlog can lead to the system being sub-optimized and therefore unable to deliver the efficiencies it promised. The danger is that since non-core systems generally have a long lifespan, people will just come to accept the system as is, without it reaching its full potential.</li>\r\n<li>Since neither IT nor the business has the time and resources to maintain the system, insurers will turn to the vendor for that support. This often makes the problem worse. The vendor may, in fact, work diligently to resolve any issues, but without the proper internal attention required to separate technical problems from process problems, many vendors simply adapt the system to incorporate existing bad processes.</li>\r\n<li>This pretty typical cascade of events often results in an overall negative view of the new non-core system by most involved, and that can lead, quite unfortunately, to the insurer never achieving the potential improvements and innovations in effectiveness and efficiencies that modern systems can bring to any company.</li></ul></p>\r\n\r\n<p>That said, there are a few simple steps that insurers can take to avoid the scenarios above for their non-core systems modernization efforts. Employing a little foresight, insurers should plan at the outset for a temporary allocation of business and IT resources post implementation. These resources should be used to form two small teams\u2014one for routine support (upgrades, patches, performance, etc.) and one for focusing on the process and functional impacts of the new system. Depending on the size of the system and the insurer, they may be as small as 2-3 people per team.</p>\r\n\r\n<p><h1>Roles of Support and Impact Teams</h1><p>\r\n\r\n<p>While the support team helps to settle the system into production, the impact team should begin the analysis of identifying the process changes that remain, quickly remedying any initial pain points, and prioritizing the enhancements backlog. The impact team can quickly gather the data they need to optimize the system by interviewing new users of the system to identify process opportunities, and by working with the vendor to make sure they\u2019ve fully implemented the latest features of the system. The focus of the impact team is on process improvement first, followed by any technical system changes required to enable the process changes.</p>\r\n\r\n<p>One of the biggest benefits of most new systems is the creation and availability of data not previously available. One of the key roles of the impact team is the identification of opportunities where any new data can be put to good use. That\u2019s often in the form of business process improvement and regulatory efficiencies, but there are many other areas where access to new data elements could prove useful and valuable.</p>\r\n\r\n<p>Both the impact and support teams should work toward creating a prioritized list of such opportunities. That list should then become part of an ongoing support plan that packages system improvement initiatives into regularly scheduled releases. The releases should be scheduled on a quarterly basis (as opposed to core systems where releases are often deferred for years due to the complexity and the disruption inherent in them) to take advantage of the value of performing incremental improvements on a consistent basis.</p>\r\n\r\n<p>Finally, this ongoing support work can be done by a small team, sometimes even ad hoc, that comes together for a couple of weeks each quarter to implement the scheduled release. Resources from the original impact and support teams might be used for this purpose, but it\u2019s not mandatory.</p>\r\n\r\n<p>In fact, it\u2019s often the case with non-core systems that after a year or two the system settles in and any functional or technical releases\u2014from the vendor or the insurer\u2014become fewer and farther between. Once any potential enhancements reach the point of diminishing return, many non-core systems can be put into a simple maintain mode until the next modernization cycle begins.</p>\r\n\r\n<p>Non-core systems modernization should be an essential part of any insurer\u2019s short and long-term technology plans. With the proper planning and execution, any insurer\u2019s non-core systems should be humming as well as any of their core systems\u2014perhaps even better.</p>",
    "tags": ["systems", "sustainability"],
    "nextMindShareId": "automating-a-little-can-help-a-lot",
    "nextMindShareTitle": "Automating a Little Can Help a Lot"
  },
  {
    "id": "automating-a-little-can-help-a-lot",
    "title": "Automating a Little Can Help a Lot",
    "authorName": "David Mitzel",
    "authorTitle": "Architect",
    "authorImageUrl": "david-mitzel.png",
    "shortDescription":
      "Reliance on information has become the way that hospitals operate and creating, organizing and optimizing information on a real time basis is an operational imperative.",
    "industry": "healthcare",
    "publishDate": "09/01/2017",
    "readTimeInMinutes": 5,
    "publishName": "HIT Leaders & News",
    "publishUrl":
      "https://us.hitleaders.news/automating-a-little-can-help-a-lot/",
    "content":
      "<p>Hospitals are nothing if not busy places. Doctors, nurses, patients, families, and support staff make up the human fabric of the daily life at any hospital system. And undergirding all of these stakeholders and activities is another sort of lifeblood for the hospital \u2013 data and information. The reliance on information has become the way that hospitals operate, and creating, organizing, and optimizing information on a real time basis is an operational imperative for any hospital system. One of the problems, however, is that hospitals need and use a fair amount of information that they do not create. This article focuses on one of those information streams \u2013 the data sent to hospitals from insurance companies \u2013 and suggests some best practice approaches for quickly automating the file load process as part of any hospital\u2019s overall data quality improvement efforts.</p>\r\n\r\n\r\n<p>The initial process is likely familiar to anybody in a hospital working with data from insurance companies. There are multiple file loads \u2013 from dozens to hundreds to thousands \u2013 on a daily and monthly basis, sent from a multitude of insurance companies. Somebody, often from the IT group, is tasked with reviewing the files, determining what should and shouldn\u2019t be loaded, firing off the necessary load processes, validating the results, and following up on any files that weren\u2019t received this month or didn\u2019t load successfully. The entire process is laborious and time-consuming, and can easily result in errors. It screams out for an automated solution.</p>\r\n\r\n<p>That said, there are challenges that any automated solution needs to address. Chief among the challenges is deciding what can and should be automated, and what can and should be left for operator intervention and decision-making. While automating the entire process is the goal, in some cases, experience and intuition cannot be automated, so a process should be created that allows for an operator to make a judgment on something where appropriate. Another challenge is determining how to handle the multitude of file format and data errors that inevitably occur on a daily basis. Some of the errors are often due to developer mistakes, while others are transmission problems, and still, others are from files that should not be loaded at all, either because they are duplicates, or because they aren\u2019t relevant.</p>\r\n\r\n<p>The first step in the automation process is to identify what human operators already know and codify that through enhanced metadata. Operator decisions such as file action (load, archive, ignore, etc.), load process, load frequency, identifying file formats, and many more can all be easily expressed in metadata. From there, a standard and automatic way of responding to data and load errors and exceptions should be determined. This is not just a technical requirement, but also requires the input of those members of the team responsible for communication back to the insurance companies. All of that becomes input for the requirements for developing an automated system that handles incoming files from any and all insurance companies.</p>\r\n\r\n<p>Automating file handling in this way will significantly reduce the time required of operators as a part of the process. The whole process orientation is flipped from being manual labor dependent \u2013 complete with such human delays as conflicting priorities, run on meetings, and sick days \u2013 to a process where operator intervention is entirely exception based. Going forward, rather than manual monitoring of a file load system, operators are instead notified via an electronic ticket from any number of collaborative software tools available. This kind of automation approach also has the important side benefit of allowing valuable business and IT resources to be repurposed for much more important tasks \u2013 tasks that have more of a direct impact on patient and employee well being.</p>\r\n\r\n<p>From a technical perspective, automating the file load process can drastically reduce the number of file processing errors by reducing operator intervention. Also, the creation of more robust metadata to define the files took what was information that was housed in people\u2019s heads and placed it in a systems structure, thus reducing organizational risk. Finally, one of the most important technical benefits is that the automation structure provides a place to perform data validation on each and every file, thereby beginning the process of improving any hospital\u2019s data quality quotient over time. That is a non-trivial benefit that pays short and long term dividends for any hospital hoping to improve their data quality for the purpose of implementing an analytics approach to information.</p>\r\n\r\n<p>There is one additional benefit worth mentioning. Many hospitals struggle with breaking the cultural barriers of doing things differently. It can be difficult to abandon something that works \u2013 even if it is cumbersome \u2013 for something that requires less human intervention. Some people will view the automation of formerly manual tasks as a diminishment in the value of the job they perform for the hospital. That, of course, is not true, and the key is to allow people to do things that are more important for, and valued by the hospital. This relatively simple automation process is a great step in that direction and has the potential to significantly improve the validity and integrity of data received from insurance companies. It can also be used as a good teaching moment for hospitals working toward modernization and better data usage and insights: a little automation can go a long way.</p>",
    "tags": ["data", "automation"],
    "nextMindShareId": "emr-data-integration-still-a-long-way-to-go",
    "nextMindShareTitle": "EMR Data Integration: Still a Long Way to Go"
  },
  {
    "id": "emr-data-integration-still-a-long-way-to-go",
    "title": "EMR Data Integration: Still a Long Way to Go",
    "authorName": "Bill Sun",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "bill-sun.jpg",
    "shortDescription":
      "For all its benefits the process of aggregating and consuming EMR (and its offspring the Electronic Health Records or EHR) information is still problematic at best.",
    "industry": "healthcare",
    "publishDate": "10/01/2017",
    "readTimeInMinutes": 7,
    "publishName": "HIT Leaders & News",
    "publishUrl":
      "https://us.hitleaders.news/emr-data-integration-still-a-long-way-to-go/",
    "content":
      "<p>The advent of Electronic Medical Records (EMR) has been a boon to the operational and analytical abilities of healthcare analytics firms and medical providers, allowing them to more accurately document, track and create the analytics required to better care for patients. Likewise, patients have benefited from the ability to track and monitor their care needs across time, allowing proactive actions to take place when identified and recommended by their healthcare providers. One of the core goals of an EMR-based medical practice is to improve the overall quality of care over time across their particular patient population. This is welcome news all around, but it ultimately entirely depends on the effective and efficient creation, organization, normalization and distribution of medical practice and patient-related data. And that\u2019s where the trouble begins.</p>\r\n\r\n<p>For all its benefits\u2014realized and potential\u2014the process of aggregating and consuming EMR (and its offspring the Electronic Health Records or EHR) information is still problematic at best. To date, there is no single technical standard for creating, formatting and distributing EMR data between healthcare analytics firms and EMR vendors. The net result of that is the sub-optimization of the accuracy and usefulness of EMR data. That\u2019s not only an operational issue for healthcare analytics firms but also a serious quality of care issue for patients.</p>\r\n\r\n\r\n<p>One of the benefits of EMR data is that it can be converted into insightful and actionable analytic data, both on an individual patient level and on a patient population level. However, the current lack of standards and therefore consistency amongst EMR vendors and healthcare analytics firms continues to be a challenge to producing high-quality analytic information. That means that for most healthcare analytics firms the conversion of EMR data to analytical insights is still a cumbersome one.</p>\r\n\r\n<p>Most healthcare analytics firms consume EMR data from multiple vendors in this space. The EMR vendors typically extract and aggregate data from various available sources that they then send to healthcare analytics firms that have contracted with to receive the data on a regular or ad hoc basis. However, the lack of standards by which the EMR vendors create, format and transmit that data makes it very difficult for healthcare analytics firms to consume it efficiently.</p>\r\n\r\n<p>For example, things like Continuity of Care Documents (CCD)\u2014a potential valuable data element\u2014are often sent to healthcare analytics firms with missing or incomplete data, or with \u201Cstandard\u201D data fields in one place from one EMR vendor, and in another place from a different EMR vendor. Also, the data is not usually well edited or normalized, so initial input errors are seldom corrected before the data is redistributed for consumption. This process is further complicated by the fact that EMR vendors each use different transmission methods. Some use a web service approach, others use a file transfer protocol hybrid, while others create proprietary transmission channels. This all adds up to burdensome technical complexity and overhead for any healthcare analytics firm trying to consume the data and turn it into useful analytical insights.</p>\r\n\r\n<p>The Clinical Document Architecture (CDA) is one of the ways the industry has tried to achieve standardized data, but to date these efforts have fallen short. For healthcare analytics firms needing to consume this data, that leaves few options. Most use consultants or other technical expertise to create front-end data consumption processes that translate and configure EMR data into something more useful to their data purposes. And there are ways to do that, even as it remains incumbent on healthcare analytics firms and other consumers of the data to do so. From a variety of lessons learned in this area, a few steps stand out as critical.</p>\r\n\r\n<p><ul><li>First, with EMR vendors providing data in a number of formats, the practice of data profiling is essential. More often than not there are too many assumptions made on the healthcare analytics firm side about what type of data is being sent and how that data will be formatted. Is it test or live data? Will data be sent via the same channel and in the same format as the last time? Has the data been put into a format that can be used for integration on the healthcare analytics firm end? A good practice is to create a pre-integration process that requires the EMR vendor to send live data samples before any regular transmissions commence. This will go a long way toward ensuring that all of the necessary data elements are included and that the data will be ready for integration when it is received.</li>\r\n<li>Second, once the above is in place, an automated routine should be created that addresses any potential integration issues before they reach the data store by categorizing incoming EMR data along a data quality rubric. The quality check should result in the creation of a data feedback report that can be used by healthcare analytics firms and vendors alike to improve their data input and formatting accuracy.</li>\r\n<li>Third, any integration process should be built in such a way that it\u2019s flexible enough to handle similar\u2014but not the same\u2014data types from different EMR vendors. For example, CCDs from different EMR vendors can be formatted completely different, even though there is a standard for CCDs. That means it\u2019s critical for healthcare analytics firms to build their integration platforms with as much flexibility as possible to handle the same but different \u201Cstandard format\u201D of CCDs.</li></ul></p> \r\n\r\n<p>Taken as a whole, these and other similar steps\u2014including the standardization of EMR data distribution channels by EMR vendors\u2014will improve the quality and usability of EMR data for healthcare analytics firms. However, these steps are also an acknowledgment that the data quality problems associated with EMR data are likely to persist into the foreseeable future.</p>\r\n\r\n<p>At present, there are not any overriding market, regulatory or operational forces strong enough to motivate EMR vendors, healthcare analytics firms, insurers or any other stakeholders to invest the time and resources necessary to improve things dramatically. Although, a recent effort to improve the accuracy and the quality of EMR data as part of a process to score medical providers\u2019 eligibility for incentives and benefits for the delivery of proactive patient care holds some promise, but it\u2019ll likely be a long time in coming.</p>\r\n\r\n<p>In the interim, it remains incumbent on EMR vendors and healthcare analytics firms to put the necessary technologies and associated processes in place to improve the overall quality, integrity and integration of EMR data into their operational analytics.</p>",
    "tags": ["data", "integration"],
    "nextMindShareId": "modern-business-intelligence-for-a-modern-world",
    "nextMindShareTitle": "Modern Business Intelligence for a Modern World"
  },
  {
    "id": "modern-business-intelligence-for-a-modern-world",
    "title": "Modern Business Intelligence for a Modern World",
    "authorName": "Zahid Ansari",
    "authorTitle": "Architect",
    "authorImageUrl": "zahid-ansari.jpg",
    "shortDescription":
      "Modern BI is about business enablement, consumable dashboards, visual data discovery and producing insightful analytics that are both timely and actionable.",
    "industry": "insurance",
    "publishDate": "11/15/2017",
    "readTimeInMinutes": 7,
    "publishName": "Insurance Innovation Reporter",
    "publishUrl":
      "http://iireporter.com/modern-business-intelligence-for-a-modern-world/",
    "content":
      "<q>Modern BI is about business enablement, consumable dashboards, visual data discovery, self-service, leveraging the data capabilities of traditional BI, and producing insightful analytics that are both timely and actionable.</q>\r\n\r\n<p>Much like core systems transformations and mobile applications efforts in the insurance industry, the business intelligence trend in the industry is all about putting the customer at the center of the universe. The rapid advent and adoption of what is known as modern business intelligence (BI) and analytics is proof positive of that.  Put simply, the modern BI approach shifts the focus and effort from an IT-centric initiative where reports are produced with complex tools that require specific product knowledge, to a customer centric approach where reports and analysis can be generated quickly by business analysts or others without an IT background. This shift, already well under way, has implications in and across insurance verticals that should allow organizations to more quickly harness and leverage data and information for actionable insights.</p>\r\n\r\n<p>Like most industry initiatives, however, conceptualizing modern BI is one thing, while successfully building and using a modern BI platform is quite another.  It\u2019s not a well-kept secret that many insurers have expended copious amounts of time, resources, and money on BI and data initiatives that have not proved successful. The reasons why are many, but a short list that includes poor data organization and quality, complex and expensive tools, competing IT priorities, and soft business sponsorship/ownership are usually right at the top. These BI efforts have created data fatigue in many insurers, so any modern BI approach must begin with focusing on the ways these traditional risk factors are overcome.</p>\r\n\r\n<p>First and foremost, modern BI efforts are business driven and as such are not as dependent on the IT department. The business owns the effort, and commits the resources necessary to make some data inroads. Second, modern BI efforts can be business driven because the tools (such as Tableau and Power BI to name just two) are more intuitive and user friendly. The tools require little to no technical training, and within days (as opposed to months and years) most business analysts can create rudimentary dashboards that can be built upon for insights. Third, since IT supports rather than implements the BI effort, issues like competing IT priorities and long delays for IT generated reports goes away almost overnight. And fourth, since the new tools are agile in nature\u2014favoring quick development and incremental improvements along the way\u2014it naturally leads both the business and IT down the path of working together in a more agile way.</p>\r\n\r\n<p>Adopting a modern BI approach still has some challenges. Just as with traditional BI efforts, any successful and effective BI initiative still very much comes down to the data.  No modern BI tool will add any appreciable value to any organization if the data being used is locked in legacy data stores and formats. The good news there is that many insurers have been working on data organization and quality for some years now, and if at this point the data isn\u2019t perfect, it\u2019s still probably in much better shape than it was. Assuming an insurer has made some strides in improving their overall data and information ecosystem, there are some things to keep in mind when transitioning from a traditional BI orientation to a modern BI orientation.</p>\r\n\r\n<p>Since a modern BI platform is designed for IT-supported (as opposed to IT-produced) analytic content development, it\u2019s important that the appropriate communications channels and expectations are created between IT and those on the business side who will be developing the analytics. Both sides need to understand their roles and responsibilities clearly, as that will vastly improve the probability for a successful transition from a traditional to modern BI mindset.  One of the keys to the business level usability of modern BI platforms is that they have self-contained architectures. This is what allows business users to execute the full analytic life cycle\u2014from data access to collaborative sharing of insights\u2014and why they are quickly becoming so popular within the business functional areas of insurers.</p>\r\n\r\n<p>That said, modern BI platforms do have limitations that this new class of business users needs to keep in mind. Chief among these limitations is scalability. For reporting and visual discovery purposes modern BI tools are providing great value. However, when it comes to advanced analytics where advanced data technology skills and larger amounts of data are needed, there should be an evaluation process to determine whether or not to keep the advanced analytics capabilities within the business function, or move such initiatives to a more centralized technology environment.</p>\r\n\r\n<h1>A Way to Leverage Existing Investments</h1>\r\n\r\n<p>Adopting a modern BI approach also serves as a way for insurers to leverage some of the investments already expended on the traditional BI and data approach.  One of the frustrations of traditional BI was low user adoption. Since business users couldn\u2019t use the complex tools required and as a result had to request time and effort from IT, many just moved on after a few bad experiences. Now, however, business users can use modern BI tools with little or no training, so there\u2019s an opportunity to rebuild some goodwill while producing actionable and useful analytics.  From a broader operational perspective, modern BI can be used as a way to improve data and analytics governance across an enterprise, by extending the traditional BI governance and data quality processes into the modern BI world.</p>\r\n\r\n<p>The point of all of this is that insurers who are serious about their analytics capabilities should take adopting a modern BI approach seriously. Modern BI is about business enablement, consumable dashboards, visual data discovery, self-service, leveraging the data capabilities of traditional BI, and producing insightful analytics that are both timely and actionable. For any insurer interested in leveraging the investments they\u2019ve already made in traditional data and BI initiatives, and in putting the power of insightful data into the hands of their frontline service providers, modern BI is worth a look.</p>",
    "tags": ["data", "business intelligence"],
    "nextMindShareId":
      "health-information-exchanges-the-future-is-now-for-life-insurers",
    "nextMindShareTitle":
      "Health Information Exchanges: The Future is Now for Life Insurers"
  },
  {
    "id": "health-information-exchanges-the-future-is-now-for-life-insurers",
    "title":
      "Health Information Exchanges: The Future is Now for Life Insurers",
    "authorName": "Yunus Burhani",
    "authorTitle": "Senior Architect",
    "authorImageUrl": "yunus-burhani.jpg",
    "shortDescription":
      "Life insurance still involves a complex process to assess an individual's health. What has changed are the tools, techniques and information to perform such assessments.",
    "industry": "insurance",
    "publishDate": "11/30/2017",
    "readTimeInMinutes": 5,
    "publishName": "Becker's Health IT & CIO Review",
    "publishUrl":
      "https://www.beckershospitalreview.com/healthcare-information-technology/health-information-exchanges-the-future-is-now-for-life-insurers.html",
    "content":
      "<p>At its core, life insurance underwriting hasn\u2019t changed much. It still involves a complex process to assess an individual\u2019s health and longevity risks. What has changed, and what continues to evolve, are the tools, techniques, and information available to perform such assessments.</p>\r\n\r\n<p>An application for a traditional life insurance policy still requires detailed information about any past medical conditions, including any procedures performed, the doctors and hospitals involved, the medications prescribed and taken, etc. However, these applications are often partially or incorrectly completed and submitted because they still rely, at least in part, on the applicant\u2019s memory and forthrightness. It\u2019s not unusual for applicants to have difficulty recalling the specifics of procedures or ailments they may have had five to 10 years ago. And of course, certain coverage/risk amounts (generally more than $2M) may also require life insurers to get an applicant\u2019s full medical record from any or all of their physicians. These documents are usually provided as a written statement from a physician that then has to be read and interpreted by life insurance underwriters.</p>\r\n\r\n<p>In short, it\u2019s still a process that is open to misinterpretations, assumptions, errors and omissions, and at the same time, it lengthens the time it takes to underwrite an insurance policy.</p>\r\n\r\n<p>However, advances in the healthcare industry and the formation of the Health Information Exchange (HIE) provide the means to improve the life underwriting process significantly. One of the key advancements in the health industry is the usage of the Electronic Medical Record (EMR). Physicians uniformly use diagnosis and procedure codes to record the condition or ailment, and the procedures performed to address medical ailments and procedures. These codes are also used to create claims for health insurers for payments to physicians. The EMR and claims are then submitted to the HIEs. An HIE typically contains EMRs for patients that are not just from a single patient-physician relationship, but rather from a patient to many other relationships other physicians, hospitals, medical practices and the like. Most HIEs will also have the pharmaceutical records detailing the history of the medications taken by the patient.</p>\r\n\r\n<p>Since most any of the exchanges contains vast amounts of data, an AI-approach could be used to mine the required data to determine the existence of any key diseases, illnesses or other issues for underwriting. Life insurers could more reasonably rely on the diagnosis and procedure codes contained in the HIE to determine a more specific medical history for any applicant. Done well, this has the potential to greatly improve the speed and accuracy of the life insurance underwriting process.</p>\r\n\r\n<p>Many of the current HIEs are funded by physicians and providers and as such are always struggling to survive on a limited budget. If life insurers started to use this data, it would provide another revenue option for the survival of HIEs making it a win-win solution.</p>\r\n\r\n<p>The best way to start using the data in these HIEs would be to leverage evidence vendors who are used by life insurers to provide medical histories using traditional paper-based and in-person methods. One of the services these evidence vendors provide is calling the insureds to ascertain a detailed medical history along with urine and blood tests. Of course, this service is highly dependent on the insureds availability. A better alternative could be first to check the HIE data to see if there is information available for that insured and use it accordingly. This could speed up the process of underwriting considerably.</p>\r\n\r\n<p>And for life insurers, such a process could be essential for appealing to a new demographic of life insurance customers. It is clear that the most recent generations\u2014Gen X, Y and Millennial\u2014are less interested in insurance than their parents and grandparents, and part of the reason for that is the onerous process new life insurance applicants must submit to. These generations will not abide by the \u201Cold\u201D rules of life insurance underwriting that often includes in-person interviews and in-home medical exams. That is anathema to the sensibilities of these new generations, and they will, apparently, forego life insurance rather than comply with these outdated (in their view) rules.</p>\r\n\r\n<p>Instead, these new generations are much more comfortable with non-personal touch for acquiring services of any kind. For life insurers, that means creating services using AI, telematics, automated call and support services, and more robust customer portals and mobile applications.</p>\r\n\r\n<p>The wider adoption and use of HIEs is a step in that direction. However for that to occur, life insurers, providers and physicians need to collaborate more effectively, including building a consensus of vision for what and how the HIE ecosystem should evolve. For the life insurance industry, the future really is now.</p>",
    "tags": ["underwriting", "data"],
    "nextMindShareId":
      "building-great-technical-solutions-requires-more-than-technical-prowess",
    "nextMindShareTitle":
      "Building Great Technical Solutions Requires More Than Technical Prowess"
  },
  {
    "id":
      "building-great-technical-solutions-requires-more-than-technical-prowess",
    "title":
      "Building Great Technical Solutions Requires More Than Technical Prowess",
    "authorName": "Hatim Kader",
    "authorTitle": "Project Quarterback",
    "authorImageUrl": "hatim-kader.jpg",
    "shortDescription":
      "Be it an innovative use of population health analytics or the use of artificial intelligence, technological  breakthroughs are shaping or creating new business opportunities.",
    "industry": "insurance",
    "publishDate": "12/12/2017",
    "readTimeInMinutes": 7,
    "publishName": "Health IT & CIO Review",
    "publishUrl":
      "https://www.beckershospitalreview.com/healthcare-information-technology/building-great-technical-solutions-requires-more-than-technical-prowess.html",
    "content":
      "<p>In this era of hyper-competitiveness, businesses are increasingly looking at leveraging technology for that added edge or the distinguishing factor that sets them apart from their competition.</p>\r\n\r\n<p>Be it an innovative use of population health analytics by a health insurer or a care provider, or the use of artificial intelligence by the auto industry, in every sphere technological breakthroughs are shaping or creating new business opportunities.</p>\r\n\r\n<p>In all these shiny examples, the underlying key to success is the right balance of understanding the business needs and applying the right technological solution to meet or exceed those needs. This underlying factor applies to every business and technology customer interface. Whether it is the development of that bleeding-edge application that creates new models such as autonomous driving or the routine application of technological improvements to any business's operational systems, such as modernizing an insurer's claims system. No matter the initiative, achieving this balance requires that the business and technologists can communicate with each other and comprehend each other.</p>\r\n\r\n<p>However, more often than not, challenges emerge because of the interfacing parties - i.e., business and technologists - may not be speaking a common language. It's not that IT practitioners are not aware of such language barriers, yet it's still very easy to miss the signs when such barriers start emerging in interactions between IT and business stakeholders. For example, it's easy to imagine an IT practitioner using the word \"metadata\" in his/her communication with the end-user or customer, and not realizing that even that concept could be very foreign to someone in their audience. This may lead to the parties ending up disconnected, and eventually impacting the effectiveness of that excellent technology in solving the end customer's critical business need.</p>\r\n\r\n<p>So how do you deal with this situation? The key is to step into their shoes, to grasp the view from their vantage point as a way to tailor the interactions appropriately.</p>\r\n\r\n<p>A non-IT example can help illustrate this point. Imagine you are a person with no prior experience with law enforcement or the judicial system, and you are sitting in a courtroom as a first-time juror. You are listening to \"opening statements,\u201D and the \"prosecutor\" calling upon witnesses. There is also a \"defense counsel\" interjecting with phrases like \"objection,\u201D \"hearsay, and not admissible.\" The judge is also using some technical language to \"overrule\" or \"sustain\" something the attorneys have said, and you find yourself sometimes struggling to understand the proceedings fully. You may wonder why these fine people - the judge, the prosecutor and the defense attorney - need your help to decide this case, and yet they are not helping themselves by making it easy for you to grasp the proceedings effectively. Why can't they use common sense language? What do they mean by \"hearsay,\" and why is it \"not admissible?\u201D</p>\r\n\r\n<p>Well, that's precisely how the business people might feel in their interactions with IT practitioners where the IT practitioner - an accomplished individual - may be presenting an elegant solution and explaining it using common industry technical terminologies. However, the business people may not be able to connect with everything being presented, and thereby are not able to partner effectively in the development or application of that just-right solution.</p>\r\n\r\n<p>Of course, it would be beneficial if all the stakeholders were equally tech-savvy, as in the case that a juror may be familiar with the law, or the judicial system, or law enforcement terms more generally. But that does not always happen, and here the IT practitioner needs to make that effort in ensuring they first comprehend the business audience \u2013 try to see the world from their vantage point \u2013 and communicates using simple and commonly understandable language \u2013 language that would be readily comprehensible by that business audience.</p>\r\n\r\n<p>This is not \u201Cdumbing\u201D down anything; instead, it\u2019s assimilating the business's perspective to see it through the same lenses. We used an example here to illustrate how the use of technical jargon by some or the lack of understanding of technical jargon by others, can impair outcomes. But the point is not just about using terminologies appropriately or simplification of communication only, preferably it is about pursuing effectiveness of technical solutions and creating positive impacts on the business, which requires business and IT to team up effectively and that in turn requires them to fully and completely understand each other.</p>\r\n\r\n<p>The key to all of the above is to employ skills that will allow one to see and understand something from another's perspective. Things like listening deeply and attentively, having an honest and open dialogue without preconceived notions of what an outcome should be, understanding that the other person's ideas opinions are just as important to them as yours are to you, and yes, even exercising a little empathy towards the other party when and where appropriate.</p>\r\n\r\n<p>In a nutshell, we are talking about emotional intelligence or EQ. \u201CYour EQ is the level of your ability to understand other people, what motivates them and how to work cooperatively with them\u201D as stated by the noted Harvard theorist Howard Gardner.</p>\r\n\r\n<p>IT practitioners usually possess good IQs, but they must also nurture and mature EQ skills that are required to understand better, empathize and communicate or negotiate with their business customers. These skills need to be employed in all interactions between IT and business customers, be it simple conversations, or moderating sessions to discuss end-user needs, or presenting solutions or results.</p>\r\n\r\n<p>People do possess some or all of these skill sets at varying degrees, but don\u2019t always excel in them \u2013 especially in the IT realm \u2013 and it takes commitment and practice to master and wield them effectively. But done well, blending and leveraging EQ and IQ prowess will lead to bonding (in a workplace context), which leads to trust, which fosters better communication and understanding, and eventually manifests into the development of an effective solution that provides significant positive impact to the business. Now that sounds like it's worth the practice to perfect.</p>",
    "tags": ["communication", "technology"],
    "nextMindShareId": "a-moneyball-approach-to-insurance-it-recruiting",
    "nextMindShareTitle": "A 'Moneyball' Approach to Insurance IT Recruiting"
  },
  {
    "id": "a-moneyball-approach-to-insurance-it-recruiting",
    "title": "A 'Moneyball' Approach to Insurance IT Recruiting",
    "authorName": "Jeff Sallans",
    "authorTitle": "Senior Consultant",
    "authorImageUrl": "jeff-sallans.jpg",
    "shortDescription":
      "Among the many challenges facing the insurance industry, none might be more critical - and problematic - than finding and keeping strong technical talent.",
    "industry": "insurance",
    "publishDate": "01/02/2018",
    "readTimeInMinutes": 7,
    "publishName": "DigitalInsurance",
    "publishUrl":
      "https://www.dig-in.com/opinion/a-moneyball-approach-to-insurance-it-recruiting?brief=00000159-faf0-d111-af7d-fbfd9f0a0000",
    "content":
      "<p>Among the many challenges facing the insurance industry, none might be more critical\u2014and problematic\u2014than finding and keeping strong technical talent. And this is not a new problem. It has slowly but surely been creeping up on the industry for decades, as technology companies, the lure of startups and the industry\u2019s reputation for living behind the technology curve have all created disincentives for young technical talent to take a look at the industry.</p>\r\n\r\n<p>So what\u2019s the answer? In part, it's focusing on the assessment process as a way to find talent that others might overlook by measuring skill\u2014real and potential\u2014over experience. And much like the popular baseball book and movie <i>Moneyball</i>, the key to better assessing talent and outcomes is to know the right questions to ask.</p>\r\n\r\n<p><ul>This <i>Moneyball</i> approach promises several benefits:\r\n<li>First, it could provide the industry with an influx of strong IT talent that could help to move technology capabilities forward.</li>\r\n<li>Second, it could\u2014over time\u2014improve the quality of systems and processes in the industry, leading to better customer service overall and improved profitability.</li>\r\n<li>Third, it could enhance the efficiency of IT departments as they become restocked with fewer but more capable people.</li>\r\n<li>Fourth, it could start to break the perception of the industry as a backwater for technology innovation.</li></ul></p>\r\n\r\n<p>The key to accomplishing all of this is by improving the way in which technical talent is assessed. However, asking the right questions is something that seems easy, but actually isn\u2019t. It can only be learned through the bitter experience of asking many of the wrong questions, which leads to bad hires.</p>\r\n\r\n<p>One of the keys to asking the right questions is to ask the questions that would likely arise while somebody was working in that position. To get a holistic view of the experiences and potential of each candidate, the interview questions should be organized into three categories that align with the desired position.</p>\r\n\r\n\r\n<p>In the following examples, the positions used are an IT developer, a technical lead, or a junior architect. Through the above-mentioned trial and error, the question categories that have been developed over time for the interview include technical skills, previous successes/failures and soft skills.</p>\r\n\r\n<h1>Technical skills</h1>\r\n\r\n<p>The assessment objective behind the Technical Skills category is to determine with some reasonableness of certainty that the candidate is or will be capable of developing quality software within a large project scope. Therefore, the specific questions will not focus on knowing <i>this</i> language or <i>that</i> configuration tool, but rather on a thoroughness of understanding of best practice coding patterns, testing, and integration techniques, and designing software for scalability and maintainability.</p>\r\n\r\n<p>This approach generally leads to an insightful dialog and avoids simply checking yes and no boxes for specific skills. The focus is on the long-term probability of success for any given interviewee. The analogy to the <i>Moneyball</i> approach is that this qualitative approach is trying to get at how an interviewee will perform over a full season, given enough at-bats, pitching starts or other applicable metrics.</p>\r\n\r\n<h1>Previous success/failures</h1>\r\n\r\n<p>This approach is continued for the two other main categories of the interview. For the Previous Success/Failures category, the whole point here is that evaluating somebody based upon their experiences to date and extrapolating their development into some future point is usually a better success indicator than evaluating somebody on the specific skills listed on their resume.</p>\r\n\r\n<p>So the questions in this category focus on asking the candidate to tell a particular project story from their experience, why they thought it turned out the way it did, what they might do differently next time, and what they learned from the experience. This is a good opportunity to have the candidate use a whiteboard or some other medium that allows them to illustrate and elaborate. There are great insights in this category for a candidate\u2019s thinking and logic process, the technical and operational complexity of their experiences, and their organizational and communication skills.</p>\r\n\r\n<h1>Soft skills</h1>\r\n\r\n<p>For the final category, the previous discussion on the candidate\u2019s experiences can be used as input to evaluate their Soft Skills. Asking questions that center on <i>why</i> something works the way it does, or <i>why</i> certain decisions were made, are much more valuable than questions about <i>how</i> something works.</p>\r\n\r\n<p>For example, answering the question of how to maintain software over time is a very different answer than why it's important to do so. Answering the why question leads to broader thinking about the value of software to a company, its relation to operational processes in the business, etc. There are no real right or wrong answers, and it again becomes an opportunity to have a conversation about a variety of things. The focus is mainly on communications, and the goal is to ascertain a candidate's ability to:</p>\r\n\r\n<p><ul><li>Understand technical conversations</li>\r\n<li>Meaningfully contribute to technical conversations</li>\r\n<li>Summarize the technical details for others</li>\r\n<li>Participate in overall team dynamics</li></ul></p>\r\n\r\n<p>The premise posited here is that the insurance industry has a technical talent issue and that the suggested approach herein is one way to begin to address that issue. This approach is of course only one way, and other approaches may be equally effective.</p>\r\n\r\n<p>The larger point is that the industry's overall approach to talent assessment needs to get better, and not just by a little bit. The consequences of not doing so are problematic and would leave the industry exposed to continuous innovation and technology deficit as compared to other sectors. That's why the industry needs to be proactive rather than reactive about talent assessment, and judging someone on their current skills even if they haven't had the experience to prove them out entirely is one way to do that. As the statistical analyst opined to his baseball general manager boss in <i>Moneyball</i>: \"Your goal shouldn't be to buy players. Your goal should be to buy wins. To buy wins, you need to buy runs.\" We need to score more runs.</p>",
    "tags": ["talent", "recruiting"],
    "nextMindShareId": "if-its-not-mobile-dont-bother",
    "nextMindShareTitle": "If It's Not Mobile, Don't Bother"
  },
  {
    "id": "if-its-not-mobile-dont-bother",
    "title": "If It's Not Mobile, Don't Bother",
    "authorName": "Tyler Sullivan",
    "authorTitle": "Marketing Consultant",
    "authorImageUrl": "tyler-sullivan.jpg",
    "shortDescription":
      "The customer experience expectations of millennials, with a specific focus towards mobile technologies, should be a focal point across the insurance industry.",
    "industry": "insurance",
    "publishDate": "01/04/2018",
    "readTimeInMinutes": 5,
    "publishName": "Insurance-Canada",
    "publishUrl":
      "https://www.insurance-canada.ca/2018/01/04/not-mobile-dont-bother/",
    "content":
      "<p>A plethora of articles have been written on how the insurance industry should engage with younger generations of potential customers. Whether millennial, Gen X or Y or anything in-between, the industry has rightly recognized a shift in the research and buying patterns of younger consumers. As a millennial, this change has everything to do with growing up as near digital natives and thus having a different kind of relationship with technology and time than prior generations might have had. For most of the millennial generation then, that means that if it\u2019s not possible to use a mobile device to manage a relationship with an insurer, then we\u2019d just as soon move on, thank you very much.</p>\r\n\r\n<p>Now before devolving into stereotypical clich\u00E9s about what a millennial may or may not want, and why or why not that may be, it\u2019s important to understand that just like every prior generation, the millennial generation has their way of relating to the world around them. Objectively speaking, those ways are neither good nor bad; they\u2019re just the way the millennial perspective has evolved. So let\u2019s get specific. My generation has grown up in a world where researching and purchasing things (meals, books, cars, houses) can be accomplished online exclusively via mobile devices.  It should come as a shock to exactly nobody in the insurance industry that the same expectation for researching and purchasing would hold true for insurance. However, since that is not entirely possible at this point, it should also come as no surprise that most millennials tend to just skip the whole insurance thing, at least for now. This is due to many other influences as well, but online accessibility and ease of access remain the key prohibitors in millennials actively seeking insurance. That said, there is an opportunity here. As the millennial generation gets on with their lives, insurers who develop viable mobile interfaces along with targeted products and services can reap the financial rewards of selling into the most prominent demographic since the baby boomers. That\u2019s no small opportunity, and it should be one that insurers are stepping all over themselves to leverage.</p>\r\n\r\n<p>One way that the insurance industry might leverage that opportunity is to make investments now to begin to turn themselves into digital businesses. That doesn\u2019t mean that they won\u2019t have buildings or offices, but it does mean that they\u2019ll take a digital/mobile-first approach to their next generations of customers.</p>\r\n\r\n<p>For those insurers who are interested in going down this road, here are the basic millennial requirements:</p>\r\n\r\n<p><ul>\r\n<li>Researching and purchasing insurance happens online and can be completed in full by the customer.</li>\r\n<li>Any subsequent amendments, alterations or customizations happen online with text chat to answer any questions.</li>\r\n<li>Claim reporting happens online in real-time, with AI and robotic technologies to assist.</li>\r\n<li>Through the whole process, self-research and text chatting with a live person on the other end are fine, but wasting time on unproductive and inefficient phone calls should never be required.</li></ul></p>\r\n<p>This, of course, is a major change for the industry. But from a millennial perspective, is this a bigger change than what the retail, music, book, or countless other industries have gone through? Or should millennials infer that the insurance industry is just less interested in having them as customers than other industries are? Yes, it is all a matter perspective. For the millennial generation, technology exists to make life \u2013 and everything that\u2019s part of it \u2013 easier and more manageable. If a given technology, or industry, doesn\u2019t achieve that then millennials are loath to have anything to do with it unless absolutely necessary, and even then grudgingly.</p>\r\n\r\n<p>And for the millennial generation\u2014and this is important to the insurance industry\u2014technology is directly related to time. What does that mean? That means that the millennial generation has a relationship with time\u2014managing it, wasting it, accumulating it\u2014that is quite different from prior generations. The millennial generation has grown up in an era of high-speed networking, social technologies, and for better or worse, near instant gratification. This means that any technology or service that can save us time is a technology or service that we want in our lives. Yes, this is part of the clich\u00E9d list of complaints that our parents and grandparents have of this generation, along with every kid gets a trophy, etc. However, that\u2019s how we were wired, and that\u2019s no different in the abstract than our parent\u2019s generation being wired for rock and roll and protests, and our grandparent\u2019s generation being wired for the greatest generation and suburban bliss. It\u2019s just part of the millennial experience set, and the insurance industry would be well advised to recognize it and make plans to deal with it sooner rather than later.</p>\r\n\r\n<p>So no thank you. The millennial generation is not interested in waiting on hold to report a claim, or waiting for days for an insurance policy or product to be completed, or filling out seemingly endless and redundant forms only to be told that our information can\u2019t be located by the branch office, or to have a nurse come into our home for an intrusive exam for a life insurance application. Like electricity, this generation will flow to the paths of least resistance and will look for other ways to get the insurance type products needed \u2013 hello Insurtech startups \u2013 rather than dealing with an industry that doesn\u2019t care to understand our needs. The demographic reality is that the millennial generation will need insurance products as it starts businesses and families, and purchases homes and cars. The demand will be there, so the supply just needs to get aligned in a way that is more palatable and consumable for the millennial generation.</p>\r\n\r\n<p>So drop us a text insurance industry, and let us know how you\u2019re doing. Oh wait, your old system can\u2019t text\u2014we have a problem.</p>",
    "tags": ["mobile", "customer experience"],
    "nextMindShareId": "how-providers-can-get-better-results-from-data-efforts",
    "nextMindShareTitle":
      "How Providers Can Get Better Results From Data Efforts"
  },
  {
    "id": "how-providers-can-get-better-results-from-data-efforts",
    "title": "How Providers Can Get Better Results From Data Efforts",
    "authorName": "Mohammed Hussain",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "mohammed-hussain.jpg",
    "shortDescription":
      "Information in the healthcare industry has seen project successes and failures as insurers race to develope ways to extract actionable information from their data stores.",
    "industry": "healthcare",
    "publishDate": "02/12/2018",
    "readTimeInMinutes": 7,
    "publishName": "HealthData Management",
    "publishUrl":
      "https://www.healthdatamanagement.com/opinion/how-providers-can-get-better-results-from-analytics-and-data-warehouses",
    "content":
      "<p>The primacy of information to the healthcare industry over the last several years has seen project successes and failures, as health insurers and other health stakeholders race to develop ways to extract actionable information from their data stores.</p>\r\n\r\n<p>While healthcare data and analytics holds promise to bring increased effectiveness and efficiency to the industry, the route to achieving the things that these analytics promise\u2014proactive healthcare, better customer service, process improvements, better claims modeling, increased profitability and more\u2014has been littered with failed attempts. That begs the question of whether there is an approach to healthcare data and analytics that offers some reasonable hope of success.</p>\r\n\r\n<p>Through trial and error, a successful pattern for implementing analytics solutions has emerged that is centered around conferring closely with the business customer. By having business analysts and data scientists actually coding their own solutions and employing IT just to standardize, optimize, scale and automate those solutions, there is a correspondingly much higher success rate in delivering those solutions. The idea behind this was and is\u2014because the business had built the original prototype, regardless of how inelegant it may have been, they had a clear understanding of what they should be expecting.</p>\r\n\r\n<p>To examine this approach more closely, let\u2019s suppose there are two different tracks of work that were conducted in widely different ways by two different teams. Each track was defined by business and developed by IT, but one was successfully implemented by IT, and the other track was plagued with bugs and ultimately shelved by business pending further analysis.</p>\r\n\r\n<p>Briefly going through each track in a simple retrospective and outlining the best practices around how to conduct and coordinate projects between IT and business should illustrate the difference in approaches, and therefore the difference in potential success. The focus here is on healthcare data and analytics and how IT can help facilitate actionable analytics in a continuous delivery and reporting cycle.</p>\r\n\r\n<p>Let\u2019s explore two short contrasting case studies that outline how this suggested approach could be applied and its benefits:</p>\r\n\r\n<h1>Person matching</h1>\r\n<p>The project goal was to develop a master person index using the conformance layer of an Enterprise Data Warehouse project. The driver and business case for the project was to have all outgoing extracts use the same person matching logic to generate data, in the current state (at the time) each individual extract had its own custom matching algorithm and the development of a new track required re-coding this matching in each component.</p>\r\n\r\n<p>There were two primary functional requirements. The existing extracts must use this new generic matching logic, and any new system or process must be able to easily integrate with this matched person data.</p>\r\n\r\n<p>For non-functional requirements, The major consideration in architecting a solution was to ensure that the core logic of the matching algorithm was easily modifiable so that the algorithm could be refined over time as more was learned on how to properly match disparate person records.</p>\r\n\r\n<p>The end status of this work track was that it did not pass acceptance testing. The final product was shelved pending an analysis by the business. In the analysis of this project, there were several lessons learned, but the core issues were that the business resources did not have enough involvement in the design process, and IT did not push enough for that involvement.</p>\r\n\r\n<p>The business resources had essentially outlined the problem domain and requested that IT provide a solution, and IT complied by doing an analysis, determining an architecture and developing a solution. The problem was that the business resources had no clear idea (aside from design artifacts) as to what the final deliverable should look like. And because the analytics team could not conceptualize the solution well, the integration was never fully accepted, and the business shelved the project awaiting further research and development.</p>\r\n\r\n<p>This track of work only spanned a single sprint before it was pushed back to planning and leadership, but the retrospective results challenged IT\u2019s ability to work properly with the business. It was clear that a change in approach would be required for the next project.</p>\r\n\r\n<h1>Dimensional model</h1>\r\n<p>The analytics team wanted a dimensional model built out of the conformed data in the enterprise data warehouse. The final result should be a framework on which dimensions and fact tables can be quickly modeled and then moved to production. The primary use case was to build Claims and Membership facts across over 16 dimensions.</p>\r\n\r\n<p>The functional requirements of the project was that the solution would be business defined fact tables and their associated dimension tables. For non-functional requirements, the design must support new business needs. New facts and dimensions that are identified should be easily added to the existing integration.</p>\r\n\r\n<p>The end result of this work track was successful Quality Assurance (QA) and User Acceptance Testing (UAT) and a limited launch to the analytics team for a \u201Cbeta acceptance.\u201D Additional facts and dimensions were added in the following months as the data scientist performed further discovery.</p>\r\n\r\n<p>This track of work followed a different requirements gathering phase. This need was communicated upfront to the business resources assigned to ensure more participation and accountability for the systems that were being built by IT. The business analyst assigned to this work was responsible for creating drafts of the data models and coding the final expected output for each fact and dimension. The models were then refined by the architect and technical leads, and the developers had a business-built and accepted model along with the expected output to test against.</p>\r\n\r\n<p>Another success factor was limiting the number of high impact decisions. There were still issues and redesigns done during the development and QA phases, but the overall design stayed the same, and the solution was delivered to the business on time and met their expectations.</p>\r\n\r\n<p>This method of providing requirements and defining the final results so that IT can set proper expectations was ultimately successful. Developers are very good at (and primarily responsible) for delivering well-structured and performing solutions to business problems, but often lack the proper intuitions to dissect and disseminate the business problem thoroughly.</p>\r\n\r\n<p>Of course, business involvement in a development process is not a new concept\u2014the Agile methodology actually encourages and depends on this\u2014leading to a modus operandi designed to increase business involvement to remove a lot of the guesswork that IT tends to do in these situations.</p>\r\n\r\n<p>While not foolproof, the suggested approach significantly increased the likelihood of project success. The business analysts intuitively knew how the data should be queried and used and what the output should look like. Once they provided that to IT, all the assumptions and guesswork was eliminated, and IT could focus on the things it does best by preparing the new dimensional model for production. This should serve as a sharp reminder that the best business technology outcomes happen when business and IT resources find the right way to work together.</p>",
    "tags": ["data", "analytics"]
  }
]
