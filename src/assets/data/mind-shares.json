[
  {
    "id": "xby2-llc-20th-anniversary",
    "title": "X by 2 Celebrates Twenty Years",
    "authorImageUrl": "20th-anniversary.jpg",
    "shortDescription": "X by 2 recently crossed another significant milestone, our 20th Anniversary as a company. Thank you to all of our team members and clients that made it all possible.",
    "isFeatured": false,
    "isPublication": true,
    "authorName": "Company News",
    "authorTitle": "Press Release",
    "industry": "consulting",
    "publishDate": "07/30/2018",
    "readTimeInMinutes": 5,
    "publishUrl": "https://xby2.com/insights/xby2-llc-20th-anniversary",
    "content": "<p>X by 2 was founded in 1998 by three colleagues who left the traditional consulting ranks with an idea to deliver business technology services following a different set of principles.  Initially focusing on internet-based design and commerce, the firm evolved to focus on the practice of software and data architecture, program and project leadership, and business technology assessments and strategies.</p><p>The X by 2 difference, honed over many years of partnering with clients, is based on creativeness, persistence, and always doing what is in the best interest of its clients.  That difference has set X by 2 apart as an innovative and impactful consulting firm that has consistently delivered value to its clients in the P&C, Life, and Healthcare industries.</p><h2 class='text-left'>Reflections</h2><p>Reflecting on the journey, David Packer, X by 2’s President, recalls, “Our aim has always been to enable X by 2’s clients to achieve their strategic and operational business goals.”  A lot has changed in business technology over the past twenty years, but according to Packer “our different approach created the opportunity for X by 2 to serve some of the most forward-thinking insurance and healthcare organizations in North America over the last 20 years. While our day-to-day architecture practice has evolved, our core principles remain the same: objectivity, being results driven, quality over quantity, and creating client self-sufficiency remain the bedrocks of our company.”</p><h2 class='text-left'>In Appreciation</h2></hr><p>Accordingly, we would like to sincerely thank our clients and team members over the years on our twentieth anniversary as a software architecture consultancy.  This milestone would not have been possible without the trust and support of our sixty-plus clients who have engaged X by 2 on over two hundred and fifty projects.  Here’s to the next twenty years.</p><img src='/assets/mind-share-img/anniversary-image-1.jpg' style='max-width: 80%;' /><p>Learn more about X by 2 <a href='xby2.com/services'>Services, Expertise</a>, and <a href='xby2.com/careers'>Careers</a> at <a href='xby2.com'>xby2.com</a></p>",
    "tags": ["publication"],
    "nextMindShareId": "why-worry-about-business-architecture"
  },
  {
    "id": "why-worry-about-business-architecture",
    "title": "Why Worry About Business Architecture?",
    "authorName": "Priyanga Manoharan",
    "authorTitle": "Architect",
    "authorImageUrl": "pri-manoharan.jpg",
    "shortDescription": "The importance of effective business architecture in organizations is a way to, among other things, optimize processes and create standards that enable business technology success.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "07/09/2018",
    "readTimeInMinutes": 7,
    "publishName": "Becker's Health IT & CIO Report",
    "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/why-worry-about-business-architecture.html",
    "content": "<p>Many companies are engaging in large transformation initiatives with the goal of not only modernizing their technology but also creating a more efficient workflow that is powered with better technology to tackle new challenges and growth.</p><p>In many of these cases, there is a clear need for a modernized technology architecture as the systems requiring modernization are antiquated. With a significant technology shift, the need for architectural guidance is clear, and as a result, many projects are staffed with various technology architects to ensure alignment with the established standards. Many organizations realize the need for enterprise-wide guidance and have created architecture groups or departments with the sole purpose of providing enterprise-level technology guidance. While this is all well and good, many organizations fall short when it comes to a similar architectural discipline on the business side.</p><p>This article argues for the importance of effective business architecture in organizations as a way to, among other things, optimize processes and create standards that enable business technology success.</p><h1>What is Business Architecture?</h1><p>Before deciding on whether valuable and limited resources and energy should be dedicated to creating business architecture, a few definitions are in order. Similar to any technology architecture, business architecture provides guidance and a general structure from which to operate.</p><p>The first step is to establish a vision for the business. This goes deeper than any strategic goals related to markets, profits or efficiencies, by defining the set of underlying tactical goals required to enable the higher-level goals. A goal of increasing profit by a certain percent annually doesn't provide enough guidance to create a comprehensive achievement plan.</p><p>A better approach might be to break this goal down into a percentage reduction in cost combined with a percentage increase in revenue. The point is that higher-level goals need to be broken down into levels of abstraction such that they are tactical enough to start creating actionable plans for and around.</p><h1>Optimizing Processes</h1><p>Once the vision has been established, and tactical plans have been created, the next step is to review the business processes. In many if not most cases, older systems have influenced the business processes. Over time many organizational methods have evolved to include steps manual or otherwise that work around technology limitations.</p><p>For example, the lack of a real-time lookup in a system may have resulted in a two-step overnight process - the traditional overnight batch process. Even modernizing portions of the platform might enable real-time lookups to make it a one-step instantaneous process. From there, further examination of the processes and an understanding of the various steps and business needs involved could help identify additional points for efficiency or even automation.</p><p>A key component of examining the processes is understanding and determining where the process pain points are, and how much time is spent on them. This provides the focus required to help eliminate the process pain points, allowing for a better experience for those using the system. Also, prioritizing and subsequently addressing the most inefficient aspects of a process can provide a better return on resource and financial investment.</p><p>One of the best practices of any business architecture is using observed evidence to ensure that the scope of any process improvements directly impacts the business in a positive way.<p><h1>Establishing Standards</h1><p>Standards and other best practices are also valuable sources to incorporate into any business technology solution where applicable. The first benefit is in not having to reinvent the wheel. There also may be many models or standards that can be leveraged that provide great process and technology options.</p><p>An even larger benefit is that the standards offer a point of comparison to the current processes. This allows organizations and the business and technical resources that support them to better understand the value of their current processes. It is possible that the existing standards are too generic and don't quite fit a new or existing niche market of the organization. Either way, process validation or invalidation occurs, thus adding data points to making better decisions in the near and long-term.</p><p>Adopting standards also help when implementing new solutions by making it to easier to implement the out-of-the-box features and functionality, if desired. Having a robust set of standards also means that IT does not spend its time on migrating ineffective or broken processes as part of upgrades and implementations, opening up scarce IT bandwidth. That added benefit allows business, and IT resources to focus on building and implementing value-added features, adding a competitive edge to the organization.</p><p>The difference lies in making a focused effort to take advantage of new features and functions, rather than just approaching them opportunistically as most organizations currently do.</p><h1>Change Management</h1><p>Many organizations struggle with change. That 's why a change management approach is an essential component of any business architecture. The change management impact of all of the business processes optimizations as a result of a new implementation can be daunting.</p><p>For better or worse, many organizations use the change management argument for keeping things the way they are, as handling and training on all that change can be a mammoth effort in its own right. That said, there is a counter-argument for that, however, and that is that there has to be a balance.</p><p>Organizations who avoid the pain of change often encounter additional implementation costs in keeping a product, feature or process working as it always has. This can, of course, increase the cost and complexity of changing when that day of reckoning finally occurs. Additionally, the cost of ownership in both implementing a custom feature or functionality and maintaining that feature or functionality needs to be balanced against the impact and cost of training the end users.</p><p>In many cases, a phased approach can be beneficial, but for most organizations, the bottom line is that sometimes change is inevitable.</p><p>All of this and more is why taking a more proactive approach to business architecture can help organizations not only improve their technology but also help them to modernize their business processes, thus maximizing the return in their investments in large transformation projects.</p>",
    "tags": ["business architecture", "modernize"],
    "nextMindShareId": "how-healthcare-organizations-can-pay-off-technical-debt",
    "nextMindShareTitle": "How Healthcare Organizations Can Pay Off Technical Debt"
  },
  {
    "id": "how-healthcare-organizations-can-pay-off-technical-debt",
    "title": "How Healthcare Organizations Can 'Pay Off' Technical Debt",
    "authorName": "Rob McCurley",
    "authorTitle": "Architect",
    "authorImageUrl": "rob-mccurley.jpg",
    "shortDescription": "By understanding what technical debt is, how it arises, and how to manage it, healthcare organizations can reduce the burden of their technical debt and use it to their benefit.",
    "isFeatured": false,
    "industry": "healthcare",
    "publishDate": "07/09/2018",
    "readTimeInMinutes": 10,
    "publishName": "Health Data Management",
    "publishUrl": "https://www.healthdatamanagement.com/opinion/how-healthcare-organizations-can-pay-off-technical-debt",
    "content": "<p>Technical Debt also known as Design Debt or Code Debt is a concept in software development that reflects the implied cost of additional rework caused by choosing an easy solution now instead of using a better approach that would take longer.</p><p>Most healthcare organizations have a business technology ecosystem that has grown and advanced over years and decades. These are combinations of older and newer systems, legacy and modern data stores, and integration points and processes that have gotten more complex over time. Often, these issues prevent healthcare organizations from upgrading or replacing systems promptly and cost time, money and resources when leadership does decide to upgrade or replace systems.</p><p>The accrual over time of all of these technical issues is called technical debt. For much of human history, debt has generally been considered as something best avoided. But modern times have brought the realization that borrowing can be a beneficial financial tool. So it is with technical debt. By understanding what technical debt is, how it arises, and how to manage it, healthcare organizations can reduce the burden of their technical debt and use it to their benefit.</p><p>Ward Cunningham, an early contributor to object-oriented and agile programming, defined technical debt as the amount of beneficial work avoided or deferred to deliver a business solution earlier or to reduce the cost of the system.</p><p>In the case of technical debt, the  'principal ' becomes the amount of work left undone, and the  'interest ' is the additional work caused over time by this unfinished work. Among the most common sources of technical debt are:</p><p><ul><li> 'Quick and dirty ' coding is created instead of modular, structured code</li><li>Applications use ad-hoc data sources or create new and poorly structured sources, rather than taking the time to implement new, well-designed sources</li><li>Functionality is  'bolted on ' to existing applications without rewriting them</li><li>Functionality is deployed using non-standard technologies</li><li>Documentation is not created to spend more time on coding</li><li>Tests are not created or run to spend more time on coding</li><li>Deferred technology and software version upgrades</li><ul></p><p>Technical debt can be acquired in either a reckless or prudent manner. Reckless technical debt occurs when design or coding shortcuts are taken inadvertently, most often through inexperience or laziness. Prudent technical debt occurs when beneficial work is deferred deliberately for a known benefit, such as omitting a less-useful feature or implementing a workable but less elegant solution to deliver a solution earlier.</p><p>Whether reckless or prudent, each instance of technical debt created during development and maintenance adds to a virtual balance sheet of principal maintained within the organization 's technical infrastructure. This principal represents the backlog of work required for the organization to get full functionality and value from its systems. But often more significant is the interest that the technical debt accrues from its inception.</p><p>This interest manifests itself in healthcare organizations in myriad ways. Some of the most common healthcare examples are systems and applications that are expensive (budget and resources) to maintain, difficulty implementing and integrating newer systems, increased security exposures and risks, and an overall increase in the risk of system failures. This interest is paid daily over many years until the technical debt is retired by implementing the originally deferred functionality.</p><p>So how can a healthcare organization eliminate technical debt? The bad news is that for all practical purposes, it can 't. The existing balance of technical debt will typically be financially prohibitive to remediate as a project. And the normal course of IT work will introduce new debt continuously. But the good news is that an organization can reap significant benefits from actively managing its technical debt portfolio. To do this, an organization must do two things: address the current balance of technical debt, and improve processes to minimize the accrual of new technical debt.</p><p>To address current technical debt, key activities include:</p> <p><ul><li>Creating a technical debt catalog to identify areas of debt and ranking their impact on existing processes and practices. This is the  'balance sheet ' for technical debt.</li><li>Using the catalog as a backlog, pay-down of technical debt by funneling items of  'principal ' into the organization 's existing software delivery streams.</li><li>Reviewing and updating the catalog regularly.</li></ul></p><p>The technical debt catalog can be implemented without any special tools a table in a text document or spreadsheet makes a satisfactory catalog and can be maintained with little overhead. To populate the catalog initially, gather input from all areas of IT not just development areas about where they see friction within the technical infrastructure. Eliminate problems caused by broken processes or human error, leaving those that are truly technical debt. Assign an owner to each item and determine a priority or severity for it. Don 't spend too much time trying to make the initial list complete or over-documenting each case.</p><p>The technical debt catalog by itself won 't bring value unless it 's incorporated into the healthcare organization 's software development and delivery processes. To do this, it's necessary to assign at least a rough size and complexity to each item in the catalog to determine the best avenue to remedy it.</p><p>Items of debt that require a small amount of work and are isolated to a single system can typically be merged into regular maintenance and update delivery cycle for that system. Each system support group should mine the technical debt catalog for items that can be placed into their system enhancement backlog, where they can be prioritized alongside business enhancements and regulatory changes. These debt items are then removed from the catalog as they are addressed.</p><p>Items of debt too large to address through regular system enhancement processes should be addressed within projects. Debt items can sometimes be added to current or planned projects if they fit naturally into the project scope and don't have an excessive impact on timeline or budget.</p><p>However, if it can't be broken down into smaller pieces some items of technical debt will require a dedicated project to address. Large items of technical debt like this are often the hardest to retire. Healthcare organizations are reluctant to allocate precious project resources to what is seen as 'clean up' work. But a valid business case can be made by highlighting the real cost of the  'interest payments ' required by a large piece of technical debt.</p><p>In addition to tackling existing technical debt, healthcare organizations must look at how new debt is created. Not all technical debt can or should be avoided. It can be prudent to accept certain amounts of technical debt when time-to-market takes priority over completeness. But this decision should be made with full consideration of the interest the technical debt is likely to accrue. Reckless technical debt is avoidable, however, and can be reduced through developer training and quality control processes such as architectural reviews.</p><p>Healthcare organizations that tackle technical debt can optimize their effort if they follow a few guiding principles:</p><p><ul><li>Keep the debt catalog simple to understand and maintain. Aside from giving each item a name, description and rough size, it 's typically sufficient to assign a responsible individual or department, along with the date the item was added to the list.</li><li>Technical debt should not be viewed as the problem of one particular group or department - it needs to be a shared responsibility between IT and business areas.</li><li>Assign responsibility for the catalog to a team that participates in the software and project initiation processes. An Enterprise Architecture team is often a good choice.</li><li>Review and update the list no less than twice a year.</li><li>Don't restrict the technical debt catalog to software items only. Technical debt can accrue in infrastructure as well, for example where failover or backup capabilities are omitted from initial installation.</li><li>Above all, technical debt cannot be delegated to  'when we have spare time ' priority - there is never spare time.</li></ul></p><p>Like financial debt, technical debt is not a pleasant issue to grapple with and resolve. It is, however, a critically important issue that ignored can lead to harsh consequences like inefficient systems, costly and laborious maintenance cycles, and, in the worst case scenarios, the inability to migrate to modern system platforms. That said, it is possible for healthcare organizations to minimize technical debt 's negative impacts with a little proactive planning and follow-through.</p>",
    "tags": ["technical debt", "healthcare"],
    "nextMindShareId": "four-ways-to-build-effective-agile-teams",
    "nextMindShareTitle": "Four Ways To Build Effective Agile Teams"
  },
  {
    "id": "four-ways-to-build-effective-agile-teams",
    "title": "4 Ways to Build Effective Agile Teams",
    "authorName": "Darius Cooper",
    "authorTitle": "Architect",
    "authorImageUrl": "darius-cooper.jpg",
    "shortDescription": "The objective of this approach is to leverage the benefits of an agile approach to software development and blend those benefits with the advantages of a centralized group of specialists.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "07/05/2018",
    "readTimeInMinutes": 5,
    "publishName": "Digital Insurance",
    "publishUrl": "https://www.dig-in.com/opinion/4-ways-to-build-effective-agile-teams?brief=00000159-faf0-d111-af7d-fbfd9f0a0000",
    "content": "<p>In part one of this series, the focus was on the advantages and disadvantages of creating a separate and distinct team for any project 's integration work, as opposed to having integration work as part of a holistic agile team. That begs the question: How can one remain true to agile, while also bringing in the technical depth and the  'flexible coupling ' that come from a separate team?</p><p>A productive approach is to have each sprint team focus on its integrations but to also have integration specialists as members of that team to bring their knowledge of the technology and the standards necessary to ensure a robust architecture. Technical specialists who have seen similar types of integrations before, and who are familiar with standard approaches and patterns, can help immensely. It means that subsequent project teams do not have to reinvent the wheel to tackle problems for which there already are good patterns and tools. In the context of this example, the integrations specialists should join the core agile team, and work as part of the single team.</p><h1>Center of Excellence</h1> <p>A good way to approach this is to establish an Integrations Center of Excellence, staffed with integration specialists. One role of this team is to choose the tools, provide the advice and create the organizational standard around integrations. However, purely  'staff ' teams can sometimes become ivory towers, imposing their ways upon  'line ' teams. The best specialists learn by doing.</p><p>In an agile environment, they should actually apply their knowledge to situations that are very concrete and specific to the organization. They need to base their work on the day-to-day work of the teams that are creating the actual integrations, as opposed to creating arbitrary and abstract one-size-fits-all standards.</p><h1>Embedded Experts</h1> <p>Another best practice for this approach and a good way to keep the integration specialists grounded while also ensuring buy-in from both sides is to embed some specialists from the Center of Excellence into the sprint teams developing the specific integrations. That way they become members of the sprint team. Another benefit is that this is in keeping with an agile approach to an organizational structure: instead of creating another silo of specialists, form the teams tactically, pulling in specialists as needed.</p><p>In this way, the Center of Excellence becomes a fluid organization rather than a sizeable static team, but is also the coordinator of a more comprehensive 'Interest Group.' Only a few members are in the separate team in the Center of Excellence at any point. They may be creating organization-wide standards for integrations, or choosing specialized tools, or developing tools that can be useful to individual teams. Meanwhile, integration specialists are spread across the project or program, working in various development teams.</p><h1>Guilds</h1> <p>Another idea that works well with this structure is the idea of a guild, whose members belong to various and different teams, but who do similar work in those teams.</p><p>For example, the Quality Assurance resources or the Business Analysts from across the organization may be members of guilds, where they meet to discuss common problems faced and the solutions devised and implemented. Another benefit is that an  'Integrations Guild ' becomes an effective mechanism for sharing ideas and knowledge about the special problems faced within and amongst the teams. It also helps to create an informal network of co-workers who can call on each other for help, resulting in shared knowledge, less re-invention of the same solutions, quicker learning curves for new members and higher productivity.</p><p>The Center of Excellence can help coordinate the activities of the guild and to create common approaches and standards, but the day-to-day integration work is done by members who belong to various other development teams.</p><h1>Phased Approaches</h1> <p>In the early design and development stages of a project, there is usually a greater need for specialists who have experience with certain types of issues, and who can create project-specific patterns and approaches.</p><p>That need for specialists is reduced as the project progresses and the project team tackles a more extensive variety of problems with the patterns put in place by the specialists. At the start of any project, the specialists may be intimately involved with the project team in developing the initial integrations. Once the project is underway, however, and with established approaches and patterns of work, the specialists can shift their time and energy to helping keep things on track, introducing new technologies as appropriate, and with the inevitable tough problem that will crop up from time-to-time.</p><p>In this way, the direct involvement of the Center of Excellence will be gradually reduced. As the project moves along, other developers may find themselves interested in the specific technologies they are working with, and as a result, they may wish to join the guild, creating a valuable pipeline for future integration specialists.</p><h1>Blending Goals</h1><p>In summary, the objective of this approach is to leverage the benefits of an agile approach to software development and blend those benefits with the advantages of a centralized group of specialists, in this case, integration specialists. To that end, the approach aims to blend both long-term and short-term goals. It leverages specialist knowledge, while also creating accountability on the part of the specialists for the delivery of well-architected software solution. It also enables a core, but more refreshed team of specialists to look at the bigger picture, across the organization, while at the same time keeping them grounded in the requirements of real projects and, thus, close to the action. This creates a blend of strategic vision and tactical implementation, and in the process sets a context for success.</p>",
    "tags": ["Agile", "integration"],
    "nextMindShareId": "living-in-a-devops-world-part-two",
    "nextMindShareTitle": "Living In A Dev-Ops World (Part Two)"
  },
  {
    "id": "living-in-a-devops-world-part-two",
    "title": "Living in a DevOps World (Part 2)",
    "authorName": "Dave Farinelli",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "dave-farinelli.jpg",
    "shortDescription": "DevOps was created to more seamlessly connect the development and operations processes, making code migration, testing and promotion to production a more efficient process.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "07/03/2018",
    "readTimeInMinutes": 7,
    "publishName": "DevOps.com",
    "publishUrl": "https://devops.com/living-in-a-devops-world-part-2/",
    "content": "<p>Part one of this article focused on some of the more behind-the-scenes benefits of an Agile DevOps approach. In part two the focus turns to some of the other traditional problems that a well-executed DevOps approach can address, and how doing so can benefit an organization in more ways than just a technical perspective.</p><p>By way of quick review, DevOps was born out of the Lean and Agile software development methodologies when it became clear that, while those methodologies did indeed speed up the development process, a bottleneck still occurred when push came to shove and new code had to be moved to quality assurance and production environments.</p> <p>DevOps was created to more seamlessly connect the development and operations processes, making code migration, testing and promotion to production a more efficient process. To accomplish this, the DevOps approach had to find solutions for some of the issues that caused operational delays, and create new ways to organize, implement and continuously optimize the operations process.</p> <h1>Overproduction/Overprocessing</h1> <p>For those who have been in development and/or operations for any length of time, it quickly becomes clear that there is a multitude of operational safety checks that serve to protect a production environment. While that is vitally important, it was also clear that there had grown an  'over ' problem around many operational procedures, and in many cases that manifested itself in the development process. That includes overproduction, when making or requesting more than was needed from requirements and/or operations perspective to clear arbitrary operations process hurdles.</p> <p>Alternatively, overprocessing, when development and operations resources do more work (as opposed to just enough, as Lean and Agile would suggest) than required to smooth the transition of code and functions from development to operations. This created waste regarding time, resources and budgets that were not proportional to the benefits derived from following the operations process.</p> <h1>Motion and Transportation</h1> <p>Similarly, DevOps also sought to solve the operational problems of both motion and transportation. That is, the amount of excess work required to deliver new code to meet the operational requirements for code migration. The friction caused by such requirements slowed the motion and momentum of the development process. The same is true of transportation, or the difficulty in moving code between environments such as testing, quality assurance and production.</p> <p>In both cases, development and project momentum was sacrificed for what often turned out to be a series of artificial hurdles that had long since become less effective or even obsolete parts of the operations process.</p> <h1>Correction and Inventory</h1> <p>In most instances, all of the above resulted in the final maladies of the pre-DevOps development and operational ways. The first was the number of in-flight corrections required when timelines were squeezed, and the rush was on to get to production. Unfortunately, this went hand in hand with the ultimate problem of good code being sacrificed for expedient delivery, often resulting in inadequate functionality, system outages and, in the end, lost market opportunity and revenue.</p> <h1>3 Keys to DevOps Success</h1> <p>Any successful DevOps implementation must address three critical factors in this order: culture, organization and tools.</p> <h1>Culture</h1> <p>It 's critically important to connect an organization 's values to the DevOps process. Valuing quality, timeliness and organizational alignment of goals and objectives is the first step toward DevOps success. Such cultural values translate directly into a DevOps organization.</p> <p>Providing empowerment and accountability to DevOps team members helps to build ownership among the team, and trust from their customers in the rest of the organization. It also helps to provide a physical environment that fosters collaboration, teamwork and continued learning. Co-working spaces and collaboration tools such as Slack are a good start. Attending external conferences to broaden perspectives and to bring new ideas back to the team is often beneficial. From there, brown bag lunch sessions where ideas and experiences can be shared, frequent post-mortems on implementations to hone best practices, and even internal mini-conferences where several departments come together for a day to discuss DevOps practices are all effective ways to build a strong DevOps culture.</p> <h1>Organization</h1> <p>Any good DevOps organization is two-sided; that is it has to work from the top down and from the bottom up at the same time.</p> <p>The top-down part is in the ability to  'see the system ' from a macro level, allowing for process understanding and insights from a business workflow perspective. This helps to identify the pain points and bottlenecks in the current process that can be optimized through the DevOps process.</p> <p>Once that 's accomplished, the bottom-up work begins. Identifying things such as inconsistencies in code deployment environments that cause delivery issues, elimination of manual and custom built deployment processes and quarantining inefficient and poorly written code until it can be redone or eliminated are all part of optimizing the time, quality, resources and success factors for deploying production systems on schedule. It 's also important here to continually audit the current processes with an eye toward eliminating the processes that are no longer required or useful but have been kept in place out of the fear of   'breaking something we don 't understand. ' If nobody understands it, then it shouldn 't be in production software.</p> <h1>Automation Tools</h1> <p>The final factor for DevOps success is to have the right toolset.</p> <p><b>Communication</b>: Any DevOps team requires the ability to quickly and directly communicate with other team members sans meetings. For this purpose, tools such Slack (real-time chat), Skype (video chat), and Confluence (for storing persistent information) are pretty good options.</p> <p><b>Planning, Monitoring & Consistency</b>: For the team 's planning needs, a tool such as Trello that can provide Kanban board functionality is worth a look. For issue tracking and monitoring of any system 's overall health, tools such as Jira and NewRelic respectively provide some good functionality. Likewise, consistency is vital in a DevOps world, and using automation to ensure that all systems are configured as desired across different environments is a crucial best practice. For this, a tool such as Ansible is worth a review.</p> <p><b>Integration & Deployment</b>: For continuous integration of systems in development and as a way to tighten the feedback loop for developers to determine if the central build used for deployment to production is working as intended, the Jenkins toolset might be a good fit. And finally, when it comes making any deployment process as painless as possible, a tool such as Docker that can handle created containers for an application that includes all dependencies, reducing the complexity of deployment to multiple environments, is a solid way to go.</p> <p>The point of all of this is to create an environment culturally, technically and physically where DevOps can succeed, grow and thrive. Organizations that can create an effective and efficient DevOps environment have also created a competitive advantage for themselves.</p>",
    "tags": ["devops", "agile"],
    "nextMindShareId": "living-in-a-devops-world-part-one",
    "nextMindShareTitle": "Living In A Dev-Ops World (Part One)"
  },
  {
    "id": "living-in-a-devops-world-part-one",
    "title": "Living in a DevOps World (Part 1)",
    "authorName": "Dave Farinelli",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "dave-farinelli.jpg",
    "shortDescription": "The first step on the journey to effective DevOps is the recognition that the concept is the result of the rise of the Lean and Agile software development methodologies.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "06/05/2018",
    "readTimeInMinutes": 5,
    "publishName": "DevOps.com",
    "publishUrl": "https://devops.com/living-in-a-devops-world-part-1/",
    "content": "<p>The concept of DevOps has grown and evolved into a conceptual and working model for more effective and efficient software development and implementation. That said, there are some differences of opinion on the real-world value of any DevOps approach to date, and on the best way to create and implement a real-world DevOps environment. This two-part article will focus on what an agile DevOps approach is meant to address, and what it is not meant to address.</p><p>DevOps sits at the nexus of three essential business technology functions: software development, quality assurance and operations. A short and concise definition of DevOps proposed in 2015 seems as appropriate as any:</p> <q>DevOps is a set of practices intended to reduce the time between committing a change to a system and the change being placed into regular production while ensuring high quality.</q> <p>The definition was suggested in the book,  'DevOps: A Software Architect 's Perspective, ' and the authors have hit upon the essence of the practice. The key, of course, is how to put that concept into practice.</p> <p>Perhaps the first step on the journey to effective DevOps is the recognition that the concept is the result of the rise of the Lean and Agile software development methodologies. Those methodologies, among other things, emphasize the following:</p>  <p><ul><li>A focus on customer value.</li> <li>The elimination of waste.</li> <li>Reduced cycle time (accomplishing work faster, releasing faster).</li> <li>Shared learning.</li> <li>Avoiding batching (don 't do things until required).</li> <li>Theory of constraints (break things up, focus on individual issues).</li> <li>Continuous integration, testing and delivery.</li> <li>Faster time to market.</li>  <h1>DevOps in Practice</h1> <p>Adherence to the principles above meant that something had to be invented to accomplish them and that something was DevOps. Over time, an effective DevOps practice should address any number of business technology pain points. The following short list of those pain points and the DevOps response should prove instructive.</p> <h1>System Downtime</h1> <p>The developers ' curse since systems were developed, system outages are inevitable as long as systems are designed, tested and implemented even with increased automation by imperfect beings. DevOps acknowledges that by changing the focus from trying to create applications that never fail to designing systems that can recover quickly, thus decreasing aggregate systems outage time over the life cycle of any application or system.</p> <h1>Stagnation</h1> <p>This was a staple of traditional systems development and is most closely associated with the waterfall methodology for systems development. After requirements were created, the development team would be locked away for weeks, months or, in some cases, years before emerging with  'fully ' working software that inevitably no longer satisfied rapidly evolving business requirements. DevOps is designed to fit hand-in-glove with the Agile practice of short windows of incremental changes instead of long release cycles, putting working software in the hands of customers as quickly as possible.</p> <h1>Team Conflict</h1> <p>Having been borne from the cultural combination of Agile and Lean, DevOps has taken on the problem of functional silos that are often erected between development, operations and the business customers. It follows the methodological approaches of collaboration and teamwork first to understand what others know and to leverage the best of it to solve business problems more rapidly. There is also a cultural bent toward experimentation, continual learning and constant improvement. This leads to blameless post-mortems, where instead of finger pointing when something goes wrong there is collaborative discussion and learning to correct and prevent the problem from occurring again.</p> <h1>Knowledge Silos</h1> <p>Functional silos have led to compartmentalized knowledge. If the old game was that knowledge is power, the new game in the DevOps world is that knowledge is freely exchanged as an enabler to solving business problems. DevOps addresses the problem of information being lost in translation between the development and operations functions by eliminating the functional barricades and making knowledge sharing the highest form of collaboration.</p> <h1>Inefficiency</h1> Waiting for things to happen used to be a standard operating procedure in the pre-DevOps world. Project plans were created and managed to account for the time it might take for new code to be moved into a testing, quality or even production environment. This was a momentum killer for projects and at times a morale killer for developers waiting to see what changes they might need to make to their code set. The combined Agile and DevOps approach has rewritten the traditional approach to code migration, smoothing and eliminating wait times so that projects flow more seamlessly from start to finish. It also has the benefit of keeping business resources testers, approvers, etc. more engaged as a result of a constant flow of new functions and features to test and use. And lest this be viewed as simply a way to keep the technical stuff moving along, it 's important to remember that there is a financial aspect to this as well. Reducing speed to market with new functionality, reducing or eliminating idle hands be they technical or business and delighting customers with a steady stream of enhancements and features all go directly to an organization 's top and bottom lines.</p> <p>That, after all, is in many ways what the DevOps approach is all about. All of these critical areas become the means to accomplish it. Part two of this article will focus on some more of the benefits of a DevOps approach, and how to achieve them.</p>",
    "tags": ["devops", "agile"],
    "nextMindShareId": "how-to-integrate-systems-more-efficiently",
    "nextMindShareTitle": "How to Integrate Systems More Efficiently"
  },
  {
    "id": "how-to-integrate-systems-more-efficiently",
    "title": "How to Integrate Systems More Efficiently",
    "authorName": "Darius Cooper",
    "authorTitle": "Architect",
    "authorImageUrl": "darius-cooper.jpg",
    "shortDescription": "Any large software program has tentacles that stretch across the entire insurance organization. These dependencies are often undocumented and overlooked.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "06/01/2018",
    "readTimeInMinutes": 3,
    "publishName": "Digital Insurance",
    "publishUrl": "https://www.dig-in.com/opinion/how-to-integrate-systems-more-efficiently?brief=00000159-faf0-d111-af7d-fbfd9f0a0000",
    "content": "<p>As many insurers have discovered, the risks entailed in core system modernization are not what they used to be. There are still plenty of risks, of course, but in many cases these have less to do with new software implementation and more to do with systems integration.</p><p>Any large software program has tentacles that stretch across the entire insurance organization. These dependencies are often undocumented and overlooked until such time when a new system is rolled out and promptly begins to break other systems in unexpected ways. To correct and prevent this, the integration effort may require as much work as the new core system itself.</p><p>How then to achieve these integrations in the most efficient manner possible?</p><p>One byproduct of all the core system modernization efforts underway is that many specialized integration tools are now available. And, to take advantage of them, many insurers have established an 'integration team' that groups related technical specialists together.</p><p>This approach, however, can cut against the business view of the project. From a business standpoint, what the user sees on a front-end screen is the visual representation of one or more business processes or user stories, and this is where some integration is often required. But this requires great coordination and communication. To avoid this, the agile approach emphasizes treating the user-story as a cohesive unit.<p><p>But a good systems integration approach is to combine these strategies. This keeps the integration work close to the other work being done for the user story, while also bringing specialized knowledge to bear.</p><p>Let me explain in a little more detail.</p><p>When a team works independently on a project, the structure of the software they produce will often reflect that separation. This is sometimes called Conway 's Law: that software structure often reflects organizational structure.</p><p>Likewise, when the team working on a user-story does integration work, there is a risk that the integration software will also reflect that team 's orientation and assumptions.</p><p>Having a separate team develop the integration software can lead to more independently structured and universally applicable integration.</p><p>However, this approach also comes with a risk: The generic solutions created by a more independent team can fail to fully support the different user stories and require costly rewrites.</p><p>Instead of creating a separate integrations team, one way to avoid both types of problems is to treat the integration software as an extension of the business process software with an adaptable layer between one system and another. In effect, this approach creates an integration layer between the core system and the insurer 's other systems, even though the software is developed by the core team. This helps to mitigate the adverse effects of Conway's Law. And the dedicated integration team can provide a single point of contact with the rest of the organization, thus reducing the risk of any communications falling through the cracks.</p><p>In a follow-on article, I 'll explore further how the two approaches can be successfully wedded.</p>",
    "tags": ["modernization", "integration"],
    "nextMindShareId": "analogical-storytelling-approach-to-technical-communication-barriers",
    "nextMindShareTitle": "Analogical Storytelling Approach to Technical Communication Barriers"
  },
  {
    "id": "analogical-storytelling-approach-to-technical-communication-barriers",
    "title": "Analogical Storytelling Approach to Technical Communication Barriers",
    "authorName": "Mohammed Hussain",
    "authorTitle": "Senior Developer",
    "authorImageUrl": "mohammed-hussain.jpg",
    "shortDescription": "Data technologists are often challenged with effectively communicating complex technical information to the non-technical side of the business.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "05/07/2018",
    "readTimeInMinutes": 5,
    "publishName": "Becker's Health IT & CIO Report",
    "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/analogical-storytelling-approach-to-technical-communication-barriers.html",
    "content": "<p>Data technologists are often challenged with effectively communicating complex technical information to the non-technical side of the business to gain approval for the funding, the resources, the priorities, or all the above for any given data strategy.</p><p>This situation is common practice, but being able to conceptually explain the strategy is where good storytelling comes into play. To better elaborate, consider the following story.</p><p><i>Suppose you are in the year 300 BC and you need to create a way to collect, store and organize data. How do you do it? Well as a first step, you'll likely learn the language used to communicate and gather information, particularly in its written format. With this knowledge in hand, you 'll be ready to start writing. To accomplish this, you 'll acquire some papyrus or parchment and some ink (made from the local materials available) and begin recording information. When you run out of page space on one page, you'll start to create more pages. When the number of pages gets large enough that it becomes difficult to find a particular page quickly, you 'll give each page a mark or a number to make it easier and faster to access the information you want. And when the amount of marked or numbered pages becomes too cumbersome to balance on your lap or to spread out on a table, you 'll create a book by binding the pages together.</p><p>As you create and acquire books, you find that you have to start organizing books by subject to make it easier to store and retrieve. The math information goes in the math book, and the math book is placed with other math books. Likewise, the biology information is placed in the biology book, the geography in the geography book, and so on. When you have amassed too many books to put on the table or the floor, you 'll build a bookshelf to hold them all. When you run out of bookshelf space, you 'll build another bookshelf to hold all of your books. As the number of books you 've acquired increases, you begin to organize them by sub-topics within subjects to make them easier to locate and access.</p><p>Up to this point you 've done an effective job of organizing your two bookshelves worth of books - so far so good. But what happens when you have a hundred bookshelves full of books? As the number of similar subjects grows, organizing simply by subject and even sub-topic within the subject is difficult, so you decide to organize not only by subject but also by author. However, at one hundred bookcases and growing, it can still take a while to locate a specific book, so what do you do? You decide to number each of the one hundred bookshelves and create a separate filing system that tells you which book is on which shelf. To help further, you also label the individual shelves so that your filing system includes not only the bookshelf number but also the individual shelf any book is on. You could expand your filing further and maybe add a different filing system that tells you where books are by genre. Or maybe reading level. Or whatever other organization you can come up with so long as it makes the task of locating the information you want as efficient and effective as it can be. Congratulations are now in order, as you 've become one of the creators The Library of Alexandria c. 300 BC and your data is stored and easy to find.</i></p><p>If you followed that story you now have a basic understanding of a simple data strategy along with the rationale for why it was required, and some of the insights surrounding the intuition behind why it was built the way it was built.</p><p>To connect this back to modern software, think of the Book as the data entity or record, the Bookshelf as the data structure representing and storing the data (an array or a list maybe), the collection of bookshelves as representing your Database, and the filing system as the Database Index. We can expand this analogy even further to say that in your library you can check out a book so that someone cannot change it while you're reading it, or that two people cannot change it at the same time. And, as your library grows it will require regular updates. For example, if you add a new book to your library, your filing system needs to be updated to record all of the particulars of the new book. That will protect against the complications of running out of shelf space and having to move all of the books the filing system will remain accurate.</p><p>In my experience, a quality storytelling approach has proven to be an effective way to convey the technical ideas involved and to get some interesting discussions going as a byproduct. By telling a non-technical but relatable story, a technologist can help explain what data is and how it's generated, as well as what it means to create a data strategy and why it's both necessary and important.</p><p>And while it might not come naturally to many technologists, the happy fact is that stories and storytelling is a part of who we all are, the technical and non-technical alike. It's a part of human history and the human experience, uniquely so in fact, and as such provides an instantly recognizable format. Of course, if it were as simple as it sounded everybody would do it. The truth is that like anything else it requires practice and persistence, but once mastered, storytelling can provide huge dividends in communications and relationship building. The other requirement is finding the right story for the right technical concepts presented, which also requires practice and persistence.</p><p>The introductory story takes between five and ten minutes to tell. The overall idea is that the story represents a relatively succinct and relatable way for understanding something very complex essentially giving non-technical listeners the conceptual building blocks to help understand complex problems. This method of formulating a creative analogy for a technical problem is foundational to any successful consulting career. It is a shockingly successful way of getting buy-in from business users by helping them understand a topic to the level that a new developer would, for example, without the explicit understanding of the underlying technical details.</p><p>The art of analogical storytelling accomplishes a few things: first and foremost it captures the audience 's attention - everyone likes to hear a story. Second, it uses a common and easily understood format that can be visualized by each listener, increasing the possibility of retention and understanding. And finally, it can be seamlessly connected back to its technical counterpart.</p><p>Good stories are worth their weight in gold, and like any other skill, it is something that requires practice and forethought. So the next time you're struggling to explain something to someone, try telling a story.</p>",
    "tags": ["technical communication", "data"],
    "nextMindShareId": "new-life-insurance-delivering-a-next-gen-experience-to-the-next-gen-applicant",
    "nextMindShareTitle": "New Life Insurance: Delivering a Next-Gen Experience to the Next-Gen Applicant"
  },
  {
    "id": "new-life-insurance-delivering-a-next-gen-experience-to-the-next-gen-applicant",
    "title": "New Life Insurance: Delivering a Next-Gen Experience to the Next-Gen Applicant",
    "authorName": "Samir Ahmed",
    "authorTitle": "Principal",
    "authorImageUrl": "samir-ahmed.jpg",
    "shortDescription": "Delivering the complete next-gen customer experience requires assembling simple technology building blocks into a sophisticated and well-engineered solution.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "04/11/2018",
    "readTimeInMinutes": 7,
    "publishName": "Insurance-Canada",
    "publishUrl": "https://www.insurance-canada.ca/2018/04/11/xby2-next-gen-experience/",
    "content": "<p>The life insurance industry has been abuzz with improving the customer experience for the past few years, particularly in the new business and underwriting processes. It might seem like a tired topic to revisit in 2018, but the number of carriers that have yet to (fully) implement capabilities such as end-to-end electronic application, end-to-end eSignature, straight-through policy issuance, and eDelivery of policy, just to name a few, indicates otherwise. What is behind the low adoption rates for such capabilities? And more importantly, what can be done to drive higher adoption?</p><h1>Current Applicant Experience</h1> <p>The current life insurance applicant is likely to be a Gen X or Millennial - and will soon be a Post-Millenial. Her buying experience leaves her frustrated and dissatisfied. As she does whenever she is faced with any new purchase, she starts on the internet. She quickly finds a website that offers life insurance quotes. This is where her experience first starts to deviate from her expectations. Instead of providing a quote, the website facilitates a phone call with an agent. Disappointed, the applicant provides her phone number and indicates a convenient call time. The agent calls and offers to visit her at her house and walk her through everything. The applicant is a bit taken aback. She wasn 't expecting to have to work with someone to get a life insurance quote, let alone invite that person into her home, or go to his or her office. In her view of the world, this is not how things get done.</p><p>When the agent arrives at her home, he starts by asking her to fill out an insurance  'Needs Assessment ' questionnaire that is seven pages long. It all looks simple and straight-forward, yet, as she begins filling it out, she notices that the questions become progressively more intrusive. It starts with demographic information, such as her name, address, and occupation. That is followed by detailed questions about her monthly budget, including all her income and all her expenses broken down by category. Finally, it rounds out the assessment by asking about assets, liabilities, financial goals, and expectations for final expenses, debts and income replacement. Using the collected information, the agent prepares a few proposals, walks the applicant through them while answering her questions along the way. He also gives her a 41-page packet containing the insurance application and several associated forms. He asks the applicant to review the proposals and to fill out the application packet based on the plan she likes. He offers to return in a few days to take care of signatures and payment and to collect the paperwork for submission to the carrier.</p><p>Her experience continues in this manner, inclusive of documentation follow-ups, family medical histories of which she knows little, a visit from a nurse, and it culminates with the agent telling her that the 41-page application packet is ready for evaluation and that she 'll hear something in the next 90 days or so. To the applicant, the 21st century this is not.</p><h1>The Next-Gen Experience</h1><p>These sentiments held by the biggest pool of potential life insurance buyers are well known in the life insurance industry; it 's not a surprise to anyone.</p><p>It 's equally well known that what these applicants desire instead is an experience that flows like this:</p><p><ul><li>24/7 self-service on devices of their choosing with a seamless transition from one (e.g., mobile app) to another (e.g., desktop web browser);</li><li>interactive questionnaires presenting a few questions at a time and tailored based on answers already provided;</li><li>being asked the breadth of information needed during the application process (not as follow-ups during the evaluation process);</li><li>comparison of products, coverages, premiums, etc.;</li><li>review of the final application packet before signing it;</li><li>electronic signature;</li><li>electronic payment at the time of signature;</li><li>an immediate decision, with an explanation in cases when the application requires further evaluation, followed by regular notification of its status; and</li><li>electronically delivered documents, including the issued policy.</li></ul><p>When both the source of frustration and the pathway to delight are known, why the low adoption rate on capabilities that matter most to this newest generation of consumers? The short answer is that while there is a simple to understand and implement technology solution for each element of the desired experience, delivering the complete experience requires assembling simple technology building blocks into a sophisticated and well-engineered solution. That might sound simple, but it is not easy and is what bedevils the industry.</p><p>The technology components that need to be assembled consist of the following:</p><p><ul><li>A modern user interface development framework that supports web, tablet and mobile access.</li><li>A reflexive question engine that can determine what questions to ask based on answers already provided.</li><BLOCKQUOTE><li>It requires codification of all application evaluation rules, including New Business, Compliance and Underwriting</li></BLOCKQUOTE><li>A document generation system to present electronically completed application packets for review.</li><li>eSignature and ePayment</li><li>A system integration platform that facilitates:</li><BLOCKQUOTE><li>Real-time communication with information sources, e.g., MIB, Rx history, MVR, etc.</li><li>Real-time appointment scheduling with evidence providers, e.g., paramedical exam, labs, tele-interview, etc.</li></BLOCKQUOTE><li>An underwriting rules engine, to codify underwriting rules, and provide real-time risk assessment with stratification by statistical confidence intervals.</li><li>ePolicy and eDelivery</li><li>Policyholder portal for receiving application status, viewing the issued policy and securely communicating with the agent and the insurer.</li><li>Agent portal to view the status of submitted applications, and securely communicate with applicants and the insurer.</li></ul></p> <h1>Conceptual Solution</h1><p>The following diagram illustrates a logical assembly of the necessary technology components into a conceptual future state for a typical life insurer:</p><img src='/assets/mind-share-img/samir-image-1.jpg' /> <p>Such a conceptual solution might seem daunting. Fortunately, the software architecture discipline provides a proven approach for accomplishing all of the above and more, which is to conceptualize the target state, acknowledge the current state, identify gaps between the two, outline a roadmap for closing the gaps, and then chip away at the solution one capability at a time. Care must be taken to ensure that each building block that adds functionality and capability on the back-end also enhances the front-end experience of next-generation customers. Given proper prioritization of resources and budgets, all of this can be accomplished in two to three years.</p><p>Think big, start small, and move fast is the call of the hour. It 's not rocket science, but it does take considerable focus and persistence - something the industry has been demanding of its applicants for decades.</p>",
    "tags": ["customer experience", "application"],
    "nextMindShareId": "the-purpose-driven-project-approach",
    "nextMindShareTitle": "The Purpose-Driven Project Approach"
  },
  {
    "id": "the-purpose-driven-project-approach",
    "title": "The Purpose-Driven Project Approach",
    "authorName": "Jason Brown",
    "authorTitle": "Developer",
    "authorImageUrl": "jason-brown.jpg",
    "shortDescription": "One of the issues with the current technology tsunami is that it can be difficult for those working on IT initiatives to gain a fuller understanding of a project.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "04/02/2018",
    "readTimeInMinutes": 7,
    "publishName": "Becker's Health IT & CIO Report",
    "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/the-purpose-driven-project-approach.html",
    "content": "<p>Modern Information Technology work is all about outcomes.</p><p>Market pressures, competitive differentiators, customer service expectations and new capabilities are just a few of the reasons that insurers are sprinting to enable new technologies and processes that support their strategic goals and objectives. For people consulting and working in IT divisions that usually means long hours, shifting priorities and making decisions as quickly as possible with the information available. That 's all well and good, but a recent experience has suggested an alternative approach.</p><p>One of the issues with the current technology tsunami is that it can be difficult for those working on IT initiatives to gain a fuller understanding of why a particular technology project is being put together the way it is, as opposed to just having a rudimentary understanding of what the technology is supposed to be doing. That's an important distinction and one that deserves further consideration, as this article will attempt to do.</p><p>For most developers, architects, data administrators and others on IT projects, it can be very easy to fall into the trap of only understanding how something is supposed to work in a rush to get new capabilities into production. One sure sign of this can be the introduction of new tools and frameworks as part of a new project that is duplicative of the tools and frameworks already in use. This leads to technology tunnel vision, with IT resources having an even narrower understanding of a new project, focusing only on their piece of the technology pie for the effort.</p><p>Again, this is understandable given the short-term pressures of delivering, but it can be detrimental over the longer term in the context of developing well-rounded IT people who understand more than what something is supposed to do. The problems with this kind of approach are many and well documented, but a short list includes the likelihood of repeating patterns and mistakes from previous projects, IT people who are too narrowly skilled and focused on only what they 've done in the past, and an unwillingness to be open to new technologies and approaches that might be beneficial to any insurer. And a good deal of this can be attributed to focusing on the execution-driven issues (what and how), rather than the purpose-driven (why) business technology issues.</p><p>That said, there are a couple of ways to begin to break this cycle, but both will take a company 's commitment to altering at least some of its IT project approach. One is to allow the time on a new project to focus on the why, or purpose-driven questions: why that architecture, why that platform, why that technology stack, why that database approach, etc. And when time doesn 't allow such an approach (which might be almost always), an alternative and perhaps more practical approach is to allow the purpose-driven questions (as opposed to the execution-driven questions) to be considered on subsequent project rework, maintenance and upgrade cycles. There 's a risk to that, but there 's also a risk of being late to the market with new technology capabilities, so those tradeoffs must be considered.</p><p>Of course being part of a team considering the purpose-driven questions when implementing a project from the ground-up can provide valuable experience for IT developers and others. Among them are:</p><p><ul><li>Ingraining the importance of considering the purpose-driven questions before starting on any subsequent IT project</li><li>Providing time to evaluate technology and process patterns for the best fit for any particular project</li><li>Offering time to dissect previous decisions made about architecture and database approaches</li> <li>Giving IT people an opportunity to discover and discuss the things they don't know about technology or project</li></ul></p><p>As suggested earlier, recent experience on a project provided a valuable opportunity for asking such purpose-driven questions. The application had the same architecture and technology stack as the one from an immediately prior assignment. This provided a good opportunity to look at another application using the same technology stack, and evaluate more closely the why, instead of merely accepting the architecture and copying it blindly in the new project. Essentially the prior project could be used as a reference architecture comparison point leading to asking why it was designed as it was and why a specific approach was chosen for the project.</p><p>This proved invaluable from an educational and a qualitative perspective and forced the kind of thinking and re-evaluations to occur that often don't happen in the first go-round of IT projects. In this latest project, the opportunity to consider the purpose-driven questions proved essential for professional development and skills growth, and the ability to carry the lessons forward into future projects.</p><p>And from a more practical perspective, having more than just the architect understand why certain decisions have been made is a good hedge against sudden staff changes and critical knowledge walking out the door. When that happens, the use of ineffective patterns in the race just to complete the project at whatever the cost can quickly dilute the deliverables for any project. That said, the experience of being able to ask purpose-driven questions allowed for the reconsideration of the architectural approach rather than just accepting what was already in place. Asking the fundamental why questions also opened the doors to better problem-solving skills.</p><p>After all, architecture or technology that works well for one project may not be the right thing for the next project. It may seem obvious in hindsight, but seeing and understanding the patterns and architectures - along with the reasoning - behind something can easily and quickly get lost if there isn 't enough time to carefully think about an approach and ask questions about why things were done the way they were done. For this project, some of those why questions were things like:</p><p><ul><li>Why was this application designed the way it was?</li> <li>Is this application solving the problem it was created to address, and why is it or is it not doing it well enough?</li><li>Why was the architecture deployed chosen over others?</li><li>Is there a better approach or why should this application remain in place?</li></ul></p><p>Determinedly answering these questions has a direct impact on the overall quality of the project and its deliverables. In all, the purpose-driven approach can lead to benefits such as:</p><p><ul><li><b>Technology Agility</b>: knowing when and when not to use specific patterns</li><li><b>Maintainability</b>: hedging against IT staff changes by spreading knowledge</li><li><b>Upgradability</b>: designing and implementing with upgrades in mind</li><li><b>Integration</b>: understanding ahead of time prevents problems later</li><li><b>Skills Development</b>: overall IT staff gets better</li></ul></p><p>These and other benefits seem like as good as reasons as any to ask and answer the why questions. And while that might take a little more time up front, it will inevitably take several times longer to answer them after the project is in production.</p>",
    "tags": ["purpose-driven", "approach"],
    "nextMindShareId": "using-analytics-for-healthier-encounters",
    "nextMindShareTitle": "Using Analytics For Healthier Encounters"
  },
  {
    "id": "using-analytics-for-healthier-encounters",
    "title": "Using Analytics For Healthier Encounters",
    "authorName": "Oleg Issers",
    "authorTitle": "Architect",
    "authorImageUrl": "oleg-issers.jpg",
    "shortDescription": "To ensure data tells a comprehensive story, insurers are focusing on their claims analytics data, and particularly the idea of a Healthcare Encounter.",
    "isFeatured": false,
    "industry": "healthcare",
    "publishDate": "03/23/2018",
    "readTimeInMinutes": 7,
    "publishName": "Health IT Outcomes",
    "publishUrl": "https://www.healthitoutcomes.com/doc/using-analytics-for-healthier-encounters-0001",
    "content": "<p>It 's no secret that a new business model has been emerging for the healthcare industry over the last several years. From its early days at the end of the previous century, the Managed Care model has slowly but surely been making its way into every nook and cranny of an industry desperately in need of reinventing itself. The reasons for this are many, but at its core, the promise of a healthcare delivery system organized to manage cost, utilization and the quality of healthcare services is just too promising to ignore.</p><p>The Managed Care process has also introduced a very powerful concept to the industry: that of shared risk and shared rewards between healthcare providers and healthcare insurers. But while conceptually compelling, the actual implementation of the model in the real world of day-to-day healthcare in this country has proved daunting to say the least.</p><p>One of the key success factors for the successful implementation of the Managed Care model - and specifically for the shared risk and rewards approach - is the utilization of high-quality data and the analytics it can produce. Without the data to accurately verify patient, provider and payer performance the whole model becomes sub-optimized. To make sure that the data does, in fact, tell a comprehensive story, many providers and insurers are focusing on improving their claims analytics data, and particularly the idea of a Healthcare Encounter.</p><p>Simply put, a Healthcare Encounter is an interaction between a person (patient) and a healthcare provider. In the context of claims analytics as an example, providers are categorized as a professional (an individual medical practitioner) or a facility (a hospital, outpatient clinic, laboratory, etc.). Further, professional and facility providers submit different types of claims to payers (health insurance companies) for reimbursement. In the context of claims then, a Healthcare Encounter can be defined as a grouping of related claims for the same person, provider, payer and date range.</p><p>An example set of business rules for a Healthcare Encounter can help illustrate this approach:</p><img src='./assets/mind-share-img/oleg-image-1.png' /></p>Illustration: Grouping of Facility and Professional Claims to build an Inpatient Encounter</p><h1>Facility Inpatient Encounters</h1><p>These typically span multiple days. Encounter date range is built by grouping related Facility Claims (same Person, Facility Provider, Payer, Diagnosis, and adjacent or overlapping date ranges). Any Professional Claims for the same Person within the date range are linked to the Inpatient Encounter; there may be claims from multiple Professional Providers (Practitioners) linked to the same Facility Encounter.</p><p><ul><li>Skilled Nursing Facility or Nursing Home Encounter</li><li>Hospice Encounter</li><li>Home Health Encounter</li><li>Acute Inpatient Hospital Encounter</li><li>Inpatient Mental Health Encounter</li><li>Inpatient Rehabilitation Encounter</li></ul></p><h1>Facility Person-Day Encounter</h1><p>These follow the same business logic as Facility Inpatient but only span a single day - no merging of Claims with adjacent dates. An assumption is made that any Professional medical services the Person received on the same day are related to the Person-Day encounter.</p><p><ul><li>Observation Encounter</li><li>Emergency Room Encounter</li><li>Urgent Care Encounter</li><li>Outpatient Surgery Encounter</li></ul></p><h1>Facility Outpatient Encounter</h1><p>Facility Claims are grouped by Person, Facility Provider, Payer and Service Date. No merging of dates or linkage of Professional Claims.</p><p><ul><li>Dialysis</li><li>Physical Therapy</li><li>Occupational Therapy</li><li>Radiology</li><li>Lab/Pathology</li></ul></p><h1>Professional/Ambulatory Encounter Types</h1><p>Professional Claims are grouped by Person, Professional Provider (Practitioner), Payer and Service Date.</p><p><ul><li>Administered Drugs, including Chemo Drugs</li><li>Allergy Encounter</li><li>Cardiovascular Encounter</li><li>Surgery Professional Encounter</li><li>Among others</li></ul></p><p>The concept of a Healthcare Encounter is a powerful one. It starts to address the issue that many large providers encounter when accepting patient healthcare insurance from a multitude of insurers. The providers need a more efficient way to identify multiple insurance payers for the same patient.</p><p>One way to think about the concept of Healthcare Encounters is to imagine a  'day-in-the-life ' of a patient who has multiple outpatient appointments that might include a consultation, a procedure, blood work, physical therapy, etc., all from potentially different facilities and payers. The concept of the Healthcare Encounter pulls all these disparate encounters together into a more meaningful way to manage healthcare claims. Additionally, this concept goes right to the heart of the Managed Care model 's requirement for the kinds of analytics that can truly produce the quality risk measures needed to accurately and effectively assess provider performance.</p><p>There are, however, still obstacles to achieving the promise of the Healthcare Encounter concept. First and foremost, overall industry cooperation when it comes to data and analytics needs to improve. As of now, there is not a common and accepted set of data quality standards across the healthcare industry. Such standards are a critical part of the ability to create the kinds of advanced analytics required to support the dreams of the Managed Care model - inclusive of the Healthcare Encounter concept - in the industry. While progress has been made on optimizing and standardizing data code sets in the industry, there is still much work to be done.</p><p>The healthcare industry is fast approaching a critical inflection point. There is wide consensus that the Managed Care model remains the future of the industry, leading to better overall health, service, and satisfaction for patients, and better overall process, efficiency, and profitability for providers and insurers. The key to really achieving all of that is data analytics for verification and continual improvement. In this area, much work needs to be done.</p>",
    "tags": ["analytics", "encounter"],
    "nextMindShareId": "enterprise-architecture-in-an-agile-world",
    "nextMindShareTitle": "Enterprise Architecture In An Agile World"
  },
  {
    "id": "enterprise-architecture-in-an-agile-world",
    "title": "Enterprise Architecture In An Agile World",
    "authorName": "Rob McCurley",
    "authorTitle": "Architect",
    "authorImageUrl": "rob-mccurley.jpg",
    "shortDescription": "It has become common to view enterprise architecture as something that time has passed by; however, this view is misguided and potentially risky.",
    "isFeatured": false,
    "industry": "insurance",
    "publishDate": "03/14/2018",
    "readTimeInMinutes": 5,
    "publishName": "ITA Pro",
    "publishUrl": "http://www.emagazine.itapro.org/Home/Article/Enterprise-Architecture-in-an-Agile-World/2100",
    "content": "<p>It has become a common refrain over the past few years to view the practice of enterprise architecture (EA) as something that time has passed by, much like using email or making actual phone calls on a smartphone. The ascent of agile methodologies and practices has seemingly relegated architectural concepts to the dustbin of history.</p><p>This article will argue that the suggestions that EA 's time has past are misguided at best, and potentially risky at worst to companies that view it in that context. That said, the sad truth is that EA has been misunderstood and misinterpreted over many years, and has earned its less than stellar reputation through poorly organized, designed and executed architectural efforts.</p><p>In many insurers, EA has mostly failed to live up to its potential and promises. The reasons cited for this failure, familiar and in many cases justified, include:</p><p><ul><li>EA is too methodical and process-heavy when agility is needed</li><li>EA focuses on technology perfection rather than business practicality</li><li>EA focuses on governance rather than business technology enablement</li><li>Architects live in  'ivory towers ' and avoid issues of day-to-day IT</li></ul></p><p>If these reasons are valid - and they mostly are - then it 's hard to escape the conclusion that EA is not only unnecessary, but it may even be viewed as a negative proposition. If you dig deeper, however, one finds that these are not failures of EA per se, but instead, they are failures of practitioners to adhere to the principles and concepts of EA itself.</p><p>The purpose of EA is to enhance the interplay between technologies and business processes to better support and enable business needs and goals. When architects move away from this fundamental value proposition, they lose relevance to IT and among business leaders. Architects can regain relevance by refocusing their practices around two principles.</p><p>The first principle of EA is the linkage of architecture and business. Architects often act as if their problem domain is technology. It 's not. Architects, like everyone else in the company, are tasked with making the business more successful. Architecture teams currently misaligned with the business goals can realign themselves by making a few key adjustments:</p><p><ul><li>Understand the Business: Architects must make time to learn about the non-technical aspects of their company's business. They should spend time observing business processes in action, and discuss business challenges and opportunities with business management at every level.</li><li>Communicate in Business Terms: Architects sometimes project an uber-nerd persona. This can alienate business colleagues and even senior IT staff. Architects need to take their understanding of the business and communicate regarding solutions to business problems - reducing costs, lowering headcount, enhancing business processes, enabling customers, etc. They should use instructive analogies and stories. With self-awareness and coaching, architects can learn to communicate more effectively.</li><li>Measure Architecture in Business Terms: More evolved architecture teams maintain metrics about their practice. But often these metrics will seem artificial or useless to outsiders. Architects should examine metrics they collect and affirm that they provide business-relevant information. Not that every parameter must relate directly to the corporate bottom line. Metrics about technology are fine as long as they are framed to give insight into how it supports the company.</li></ul></p><p>Shifting to a business orientation doesn 't necessarily force a wholesale overhaul of an architecture practice. The key activities of analysis, modeling and technology governance can continue, but they need to be reassessed through the lens of business success. Consider pruning activities that are hard to justify through this perspective.</p><p>The second principle the modern architect should focus on is to embrace agility. Agility is used to mean different things, but overall it reflects the fact that everything moves at a much faster pace than it did a few decades ago.</p><p><ul><li>Support Business Agility: Accept that different parts of any insurer 's business run at different speeds. When speed to market is important, architectural standards can be relaxed. Prioritize modernization efforts around agility.</li><li>Support IT Agility: Don 't resist agile development methodologies - adopt a lighter touch to integrate with them instead. Be selective about the artifacts that the architecture team produces - create only enough and in enough detail to get the job done.</li><li>Get in the Trenches: Develop, hire and otherwise obtain architects that don't mind getting their hands dirty with the tough day-to-day work of communicating, planning, developing, adjusting and communicating again. It's challenging to have an agile architecture that responds quickly to changing business and technology requirements if you're not plugged into the business functional areas and the people around the company that drive the requirements. This is essential and has benefits on multiple levels, not the least of which is demonstrating to the rest of the company that architects add value to the business efforts, have a deep understanding of the business needs and can effectively wed those to the most effective technology approaches.</li></ul></p> <p>Finally, in a world where the insurance industry is changing in dramatic and fundamental ways, EA is, in fact, more necessary than ever. The macro trends of digital transformation, customer centricity, mobile first and the demographics of next-generation customers have driven nearly all insurers to reconsider their strategic and operational models. As a result, almost all insurers are in the midst of some combination of core systems transformations, creating or enhancing mobile platforms, and partnering with the Insurtech community to bring innovation and creativity into their companies.</p><p>This change requires insurers to take a fresh look at how they can best leverage the concepts and principles of EA for the long-term benefits it can provide. And, the good news is that none of this is pie-in-the-sky stuff. Instead, it's a common sense approach to reestablishing the practice of EA in an increasingly agile and digital world.</p>",
    "tags": ["enterprise architecture", "agile methodology"],
    "nextMindShareId": "how-providers-can-get-better-results-from-data-efforts",
    "nextMindShareTitle": "How Providers Can Get Better Results From Data Efforts"
  },
  {
    "id": "when-it-comes-to-large-projects-think-architecture-first",
    "title": "When it Comes to Large Projects, Think Architecture First",
    "authorName": "Oleg Sadykhov",
    "authorTitle": "Principal",
    "authorImageUrl": "oleg.png",
    "shortDescription": "Large information technology programs are often multiyear initiatives with large teams and large budgets, but this approach does not guarantee success.",
    "industry": "insurance",
    "publishDate": "01/25/2017",
    "readTimeInMinutes": 10,
    "publishName": "Insurance Canada",
    "publishUrl": "https://archive.insurance-canada.ca/ebusiness/canada/2017/Xby2-Project-Architecture-1701.php",
    "content": "<p>Large information technology programs are often multi-year initiatives with large teams - more than 20 to 30 people, often much larger - and budgets in eight figures. Examples include an implementation of a core system such as policy administration, claims or billing, or a large data-analytics project. The problem with large initiatives is that delivering them in a reasonable amount of time requires large teams, and large teams present large challenges such as:</p><p><ul><li><b>Communication:</b> Smaller groups can communicate more efficiently and directly, but large teams require more communication and documentation that increase project overhead.</li><li><b>Inefficient use of resources</b>: Team members either wait for one another to accomplish tasks they depend on or inadvertently break each other's functionality, which introduces rework.</li><li><b>Uneven skills</b>: More-capable people can take on larger, more complex tasks, but it's very difficult to assemble a team of all-stars.</li><li><b>Ramp-up and knowledge transfer:</b> Building the initial team, addressing turnover and other team configuration changes create the need for new people to quickly get up to speed.</li></ul></p><p>On large projects all of this becomes even more challenging due to the size and complexity of the solutions. Because issues with large teams are well known, many modern software development methodologies (especially agile) stipulate that teams should be small. For example, in Scrum, an agile methodology, teams are typically cross-functional groups of seven or so. In an ideal world, large initiatives would be broken down to create a set of small teams that works almost independently and efficiently, and ultimately assembles their work products into the overall solution. Of course, most everybody tries to break large teams into sub-teams (also called work streams, tracks and the like), but very rarely is it done well, where a variety of challenges, including communication and inefficient use of resources, are truly addressed.</p><p>This is where architecture comes in - and it's not talking about the selection of Java versus .NET, or what integration technology to employ, or what types of servers and networks to use - though these topics are all important. Instead, it's about architecture as decomposition: how big systems must be broken down into smaller pieces that will lend themselves well to independent work by teams. These pieces must be autonomous and loosely coupled. The task is not easy, but it's important since without a good breakdown of systems into parts (variously called subsystems, modules, services or bounded contexts; subsystems for purposes of this article) there will be no good breakdown of large teams into efficient and independent sub-teams. It's natural for sub-teams to form around subsystems. The stakes are even higher than just the team breakdown. If the system is not well-architected, it will result in a complex and inflexible solution. Cost overruns and major failures are inevitable.</p><p>Every project team that has dealt with big system implementations has hit the 'wall.' Suddenly, development tasks that used to take a week start to take a month and it's not clear why. This is a direct result of increasing complexity and insufficient architecture. If the parts of the overall solution overlap, then the teams responsible for these components will end up performing similar tasks. This leads to duplication of effort, inconsistent solutions and similar defects that show up in various parts of the system that seem elusive and hard to eliminate. Project teams will under perform if the architecture is poor. Inefficient communication paths between teams will start dragging the initiative down. Teams will be mired in coordination and dependency nightmares and spend their valuable time adjusting to the work performed by others.</p><h1>Creating Good Architecture</h1><p>Let's take a simplified example to illustrate the challenges. Assume that the project is the implementation of a policy administration system where insurance policies are stored and maintained. Further assume that customer information is also stored and updated by the same system. In order to break our system down into subsystems, a logical place to start might be to separate the functionality related to policies from the functionality related to customers. In this way we've created a policy subsystem that will deal with policy information, and a customer subsystem that will deal with customers. This is an example of a good architecture that has subsystems that are autonomous and loosely coupled. This creates the ability to make and release changes to these subsystems independently of one another. In this example, policy and customer subsystems should now be able to grow and evolve independently of each other.</p><p>A typical system consists of user interface components, business logic components and data. When breaking the system down into subsystems, consideration is given to each of the layers listed. In order to achieve the independence of the subsystems, all of the layers must be dealt with - UI, business and data, including their corresponding business components. Separation of the business components is probably the easiest task (though still not easy) and as such it's the only thing that is typically attempted. Take for example the many implementations of Service Oriented Architecture over the past several years, where there is an incorrect belief that good architecture results from exposing business components such as Web services, which implies loose coupling and autonomy.</p><p>But if both the customer and the policy subsystems continue to share the same underlying database, then any work done on customer functionality subsystem can inadvertently break the policy subsystem. In this configuration, when one subsystem is released, the others must be tested as well. And when the database is down, then both subsystems are down. So where is the autonomy? The same can be said of the UI. It's natural for the same screen to present information about both policies and customers, but the way such screens are often built further entangles the two subsystems. The challenge of architecture is to keep the criteria of autonomy and the loose coupling of subsystems very clearly in mind, and tackle the difficult issues such as data separation and the disentanglement of user interfaces to accomplish it.</p><h1>Data Replication</h1><p>So what's the problem with breaking the data down and letting the subsystems own their data in order to avoid one large database that supports it all? The problem is that the policy subsystem legitimately needs customer information, and the customer subsystem may also need to know information about the policies the customer owns. Clearly, the data needs to be shared, and there are a couple of choices for accomplishing that:</p><p><ul><li><b>Store the data in one place and provide access to it.</b> In this scenario the customer data is truly owned and only accessible by the customer subsystem. To share data, the customer subsystem can either expose the data via Web services, or provide UI screens or widgets for others to use in order to access the data.</li><li><b>Allow the partial replication of the data.</b> In this scenario, the replication of some of the customer data elements gets created so the policy subsystem can store it in its own database.</li></ul></p><p>The idea of storing data in one place initially looks advantageous, but after a deeper look, it presents several challenges. In many data intensive situations, centralized data access presents a performance challenge. Many off-the-shelf packages expect the data they need to be stored locally in the application database (this is called a replication scenario). Some of the popular enterprise subsystems will be difficult to scale, since everyone will need the same data. They also will be hard to change without affecting everyone. Additionally, downtime of one subsystem will lead to the downtime of others that depend on it. The bottom line is that data replication is an important tool to achieve the goals of autonomy and loose coupling of subsystems.</p><p>There is no hard-and-fast rule to say when data should be centralized and when replication makes sense. To decide, think about the amount of data in play, how much data processing is done by the subsystem that owns the data, how impactful the downtime of one subsystem will be on another, and so on. There is no hard-and-fast rule to say when data should be centralized and when replication makes sense. Of course, the moment one starts down the path of data replication is the moment one needs a good framework in order to implement data ownership and the corresponding rules required for the subsystems. One such framework is master data management, which is a tried-and-true way to track owners of data and the rules that should be followed.</p><h1>CQRS Aids Architecture</h1><p>Another relatively recent framework that helps one think about separation between subsystems, data ownership, and much more is called the Command and Query Responsibility Separation. The idea is fairly simple: It advocates separation of data that supports modifications of the system from data that supports inquiries. According to CQRS, systems should have two parts - one for data modifications and the other for data inquiries - each with their own databases. Even without fully following CQRS, the concepts introduced by CQRS are helpful when working on and discussing architecture concepts such as:</p><p><ul><li><b>Commands:</b> Requests to modify data (that is, to create a new customer). Their names should be in the present tense and sound like commands. Their processing involves data validation, execution of business rules and changing the data.</li><li><b>Events:</b> Notifications about data changes (such as, new customer created). Names of events should use past tense to indicate that they're just a notification of a change that's already occurred. Events notify interested parties that a data change has been approved and recorded. They're typically used to trigger other processing or to update decentralized data replicas.</li><li><b>Queries:</b> Read-only requests to access the for specific data record details. Here's how the CQRS approach can be applied to our example of policy and customer subsystems. Policy and customer subsystems will 'own' their data and be responsible for processing commands to change their data. Upon successful changes of data, they will publish events notifying the other subsystems. To accomplish this, a new subsystem is created - the operational data store (ODS) - that will combine data from policy and customer components in a way that is optimized for inquiries. The ODS will change its data in response to events published by the policy and customer subsystems. Therefore, to build a solution that involves interactions with the policy and customer subsystems, the ODS will be used to fulfill inquiries, and when data changes are made commands will be sent to the respective subsystems (policy and customer) to process them appropriately.</li></ul></p><p>In summary, software architects should consider newer approaches such as CQRS when working through the challenges of the decomposition of their systems. The goal is to achieve autonomy and loose coupling of any subsystems. If successful, it can pave the way for small, independent, and efficient teams that have a much better chance to deliver large and complex enterprise initiatives successfully.</p>",
    "tags": ["insurance", "architecture"],
    "nextMindShareId": "the-key-to-small-project-management-keep-it-simple",
    "nextMindShareTitle": "The Key to (Small) Project Management: Keep it Simple",
    "isFeatured": true
  },
  {
    "id": "the-key-to-small-project-management-keep-it-simple",
    "title": "The Key to (Small) Project Management: Keep it Simple",
    "authorName": "Jeff Sallans",
    "authorTitle": "Senior Consultant",
    "authorImageUrl": "jeff-sallans.jpg",
    "shortDescription": "Can a methodology be slimmed down to something that fits the needs of a smaller project, but still provide enough structure to effectively manage it?",
    "industry": "insurance",
    "publishDate": "02/02/2017",
    "readTimeInMinutes": 7,
    "publishName": "Insurance Innovation Reporter",
    "publishUrl": "http://iireporter.com/the-key-to-small-project-management-keep-it-simple/",
    "content": "<q>There are ways to apply some of the best practices from larger project efforts to smaller projects, while not burdening smaller projects with unnecessary bureaucratic overhead.</q><p>Today one sees many articles about standard practices and patterns for managing projects. But what if it 's a small project or an ad hoc initiative that wasn 't necessarily planned for? Can a standard methodology be slimmed down to something that fits the needs of a smaller project, but still provides enough structure to effectively manage it?</p><p>When working on any sizable project, decisions are typically made for development and testing in the interest of resource and time trade-offs. On such projects, the questions are something like:  'Should we delay this project release to add this feature? ' or  'We are pretty confident in this code, so should we decrease testing time in this area? '  When it comes to smaller projects, why shouldn 't these questions be applied to methodology and practices? The key is to identify what practices provide the most benefit, and more importantly, what practice can be simplified to use on a smaller project.</p><p>A comparison of two larger and smaller projects might prove instructive. For this comparison, both are full-stack web development projects. One is a large insurance quoting project, and the other a small e-commerce project. The biggest difference between the two is their scale and timeline. To set the scene, here is a quick overview of the two project structures:</p><p><b>Large Team Structure - Team of 18 - 2-Year Timeline:</b></p><p><ul><li>(2) Project Leads</li><li>(2) Business Analysts (provides stakeholder sign-off)</li><li>(4) Quality Analysts</li><li>(2) Tech Leads</li><li>(6-8) Developers</li></ul></p><p><b>Small Team Structure - Team of nine - 6-Month Timeline:</b></p><p><ul><li>(3) Stakeholders</li><br>Responsibilities include Business Analyst (provides stakeholder sign-off)</br><br>Responsibilities include Quality Analyst</br><li>(2) Part-time Project Leads</li><br>Responsibilities include Tech Lead</br><li>(2-4) Developers</li></ul></p> <h1>Estimated project durations:</h1><p>Large project man-hours = (18 team members) x (24 months) = 432 working months</p><p>Small project man-hours = (8 team members) x (6 months) = 42 working months</p><p>The large project is approximately 10 times larger than the smaller project. Given this, how might the smaller team, working on the smaller project, simplify the project communication practices for effectiveness and efficiency? Here are some suggestions:</p><h1>Daily Standup: General Team Communication</h1><p>The purpose of a daily standup is to spread awareness about the team 's work to keep everyone productive. In the larger project, the standup was between eight people and usually lasted 20 minutes. The biggest benefit of this process is coordinating between testers and developers.</p><p>For the smaller project, the process was simplified by using email to report any standup details. However, this proved to be ineffective because emails couldn 't duplicate the standup 's quick back and forth discussion between team members. The compromise for the smaller project was to use a longer in-person meeting (four people and 30 minutes), which kept team members with multiple roles on the same page.</p><h1>Design Reviews/Code Reviews - Developer-to-Developer Communication</h1><p>Design and code reviews are typically done between fellow developers. A design review occurs when a plan or structure is reviewed before it is implemented. This is important to help create well-organized code and to reduce time when making improvements in the future. A code review occurs when the code written is reviewed before testing to help catch bugs, and to help with code consistency.</p><p>For both large and small projects, design reviews are very important, especially at the beginning of the project because there are more opportunities to see the benefits of more easily extending existing functionality. Code reviews can be simplified for smaller projects because less communication is needed between developers when each person writes large sections of code. In many small projects, a single developer will write all the code for a certain feature. This removes a lot of developer coordination, and the code inherently gains the same benefits of reduced bugs and code consistency, while reducing the number of code reviews.</p><h1>Business Engagement - Developer-to-Business Analyst Communication</h1><p>Developer and business analyst interaction is crucial to a project, big or small. For those unfamiliar with the term business analyst, it refers to someone that understands the behavior and actions that the program will be responsible for executing. A useful way to think about it is that the business analyst knows the destination, and the developer builds the road to get there. Projects often veer off course when the two fall out of sync.</p><p>For larger projects, this is usually addressed by adding a person to the development team dedicated to the business analyst role. For many smaller projects, however, that is not a practical option. More often than not, most members of small project teams must wear multiple hats and serve multiple roles. To simplify things for this purpose, it 's often helpful to use a Google Document or something similar to act as a proxy for someone who would normally play this role full time on a larger project.</p><p>In practice, developers would add a question to the document when they encountered a question while writing the code. Every other day, a business member of the team would respond to the questions posted. If a developer 's question changed or was otherwise resolved prior to a response, they would update the Google Document, thus avoiding unnecessary work for the business members of the team.</p><p>This method works quite well for quick questions about schema or the legacy systems but can be lacking when discussing new features. In those cases, a conference call is probably more appropriate.</p><h1>Bug Reports: Developer-to-Quality Analyst Communication</h1><p>For small project teams, everything is magnified. Things have to happen faster with fewer people, so finding the right tools that support the small project team structure is critical. As with developer-to-analyst communications, Google Docs or something similar can be quite helpful in the QA process.</p><p>For larger projects, JIRA or something similar is a great tool for managing the QA process, but smaller project teams don 't have that kind of tool luxury. That 's why Google Spreadsheet fits this small project situation better than anything bigger and bulkier.</p><p>In the end, having everything on one spreadsheet speeds-up data entry and offers a more holistic view of the data for a small project team, that 's worth its weight in gold.</p><p>The lesson to be drawn here is that there are ways to apply some of the best practices from larger project efforts to smaller projects, while not burdening smaller projects with unnecessary bureaucratic overhead. The best practice of project communications can be adopted for this purpose, and those same principles can be applied to other project management practices.</p><p>The key is looking at the current roles and communication practices of a smaller project team and asking these questions: Do the project team roles line up with the project team 's communication practices? Or, is there a practice that seems unnecessary? If so, modify or eliminate it. Remember the benefit of a small project and a smaller project team is the ability to quickly change practices to suit the needs of the team. Use this to your advantage when simplifying your small project team processes to strike the balance that makes the most sense in order to meet the ultimate goal getting the project done.</p>",
    "tags": ["insurance", "project management"],
    "nextMindShareId": "how-neglecting-non-functional-requirements-makes-systems-non-functional",
    "nextMindShareTitle": "How Neglecting Non-Functional Requirements Makes Systems Non-Functional"
  },
  {
    "id": "how-neglecting-non-functional-requirements-makes-systems-non-functional",
    "title": "How Neglecting Non-Functional Requirements Makes Systems Non-Functional",
    "authorName": "Matt Flores",
    "authorTitle": "Architect",
    "authorImageUrl": "matt-flores.jpg",
    "shortDescription": "It's more often than not that the non-functional capabilities of a system determine its value and lifespan, and whether or not the company views it as a success.",
    "industry": "insurance",
    "publishDate": "03/17/2017",
    "readTimeInMinutes": 7,
    "publishName": "Insurance Innovation Reporter",
    "publishUrl": "http://iireporter.com/how-neglecting-non-functional-requirements-makes-systems-non-functional/",
    "content": "<q>Over the long term, it 's more often than not that the non-functional capabilities of a system that determine its value and lifespan, and whether or not the company views it as a success, or just another sub-optimal delivery from IT.</q><p>Whether partnering with a custom solutions provider, creating a home-brew application, or purchasing an out-of-the-box product, most companies will follow a standard approach for evaluating their needs for a new or updated system.  Commonly, the first step is to identify the core functional requirements that need to be satisfied. First and foremost, the solution must meet the base business and process needs. However, this is only half of the first step. These core functional requirements typically will only address how something works but not how well it works. To completely identify requirements includes considering the non-functional requirements (NFR) of the system. It can be argued that this second half of requirements is as important as the first, if not more so in some cases.</p><p>Non-functional requirements differ from functional requirements in several important ways. Functional requirements describe how a system will work: what screens will users see, how the screens transition from one to the next, what data can be entered, how and when will the data be saved and retrieved, and so on. These are commonly seen through the lens of the primary users of the system, as they must have these requirements met in order to do their jobs.</p><h1>Functionality and Quality</h1><p>By contrast, non-functional requirements describe the quality of a system. This simple but broad description covers a number of attributes including performing well under normal <i>and</i> extreme usage patterns (performance and scalability), quickly adapting the application to changing or varying business rules (configurability), addressing productivity for novice and expert users (usability), and preventing unauthorized access or transfer of data (security). These sets of requirements can be seen and experienced by many stakeholders beyond just the primary end-users. Viewing it this way, it is clear that outlining and fulfilling only the functional requirements could lead to a half-baked solution.</p><p>It is not uncommon or unexpected for a customer to be focused primarily on the functional requirements, as the visible and tangible assets can create the perception of a high-quality system. Often, non-functional requirements can be prioritized lower or treated as afterthoughts constrained by time, budget, and resources. This is a mistake.  Non-functional requirements have a significant impact not only on the architecture and design of a system, but also on the long-term total cost of ownership (TCO).</p><p>To illustrate this, consider Amazon.com and the architecture, infrastructure, and hardware required to maintain a system serving over 150 million unique users per month.  Even when stripping the site down to the most basic features of e-commerce (search, add to cart, and checkout), having such an enormous user base demands constant up time, a highly responsive system, and an ordering process simple enough for your grandmother to send you a birthday present.  Hiccups in any of these areas could be costly.  In 2013, merely 30 minutes of down time cost Amazon nearly $2 million.  This is, of course, is an extreme example, and the engineers at Amazon have created a remarkably robust architecture to prevent disruptions, but the site would not function as well as it does without having considered the needs of the company beyond its front-facing capabilities.</p><p>Given that non-functional requirements are not a new concept, one would hope that they have become so ubiquitous in software development that major applications should never experience such issues.  Unfortunately that is not the case.  In 2016, the launch of the highly anticipated mobile game Pokemon Go attracted over 50 million players in the first month after release.  The game gave players the chance to capture and power up fantasy creatures Pokemon in an augmented reality platform, an exciting experience for players familiar with the video game series. This massive userbase overwhelmed the game 's servers often preventing users from logging in or reliably playing the game in the first few weeks. Additionally, features within the game, such as tracking nearby Pokemon, were slowly disabled and removed over time due to the volume of network traffic between the client and the servers. While features met their functional requirements well, it appears that non-functional requirements were not given adequate weight. Overall, this led to a poor user experience until the game stabilized.</p><h1>The Bigger Picture</h1><p>The importance non-functional requirements can be demonstrated to be more than stretch goals or afterthoughts by connecting their value to important business goals. Projected growth, potential features, geographic expansion, or the execution of a company 's long-term strategy can be hindered by the dreaded  'system constraints ' born from neglecting NFRs.</p><p>Given these observations, what are some considerations for specific non-functional requirements? Let 's review two of the aforementioned non-functional requirements as a way to better understand see how they connect to business goals, and identify key areas of focus.</p><p>A thoughtful product owner will consider how much and how quickly a system 's user base will grow. Building a system only for the present or average load can limit growth and significantly limit business goals. Scalability non-functional requirements are as much a business issue as they are an IT issue. Though a solution can be quickly developed, tested, and deployed without considering how a system might function under increased load, addressing scalability concerns may be more difficult to solve on a live system than one still on the drawing board. When considering how much a system may need to scale, a product owner must consider when the system is under the most load, including what time of day and what time of year.  Perhaps most activity happens early in the morning or just after lunch. Maybe usage spikes significantly on weekends. Some industries, like retail, need to consider increased transactions leading up to major holidays. In any case, the system must be architected to handle peak loads as well as day-to-day traffic.</p><p>How might systems handle spikes in demand? A straight forward tactic is to scale-up existing hardware with improved components thus increasing overall processing power. A system can also scale-out by adding additional hardware and servers to serve more concurrent users. However, a scalability bottleneck can occur in any layer of a system 's architecture. Aside from the application running on its main hardware, less obvious areas to review include available network bandwidth, database responsiveness, and dependencies on external services and integrations.</p><p>Aside from hardware concerns, a system must be easily usable by its core audience to be a success.  Usability as a non-functional requirement category entails too many parts to detail here. However, a key component that cannot be understated or overlooked is an understanding of what types of users will access the system. What about users who rarely access the application? Can such users stay familiar with the flow and nuances of the screens from session to session at that usage frequency? Are on-screen tips and a wizard-supported workflow helpful to guide users through a process? When accommodating novice or infrequent users, the application must have an intuitive and consistent layout to be accessible.</p><p>Conversely, an expert user who works on the system daily may find this level of hand holding more annoying than helpful. For power users, the layout of screens can focus on completing as many tasks as possible in an efficient way.  This could mean that instead of a set of screens to enter one set of data, one screen with a different layout could allow for multiple entries at once with less on-screen help. Neither design here changes what data is stored from a user, thereby changing no functionally required fields, but the non-functional usability requirements must be assessed before the workflow is built to maximize the user experience.</p><h1>Not Just How, But How Well</h1><p>As solution providers increasingly deliver more functional, more stable, and more flexible software systems, the project leadership must consider not only how the product will work, but how well it will work.  To separate and neglect non-functional requirements or to prioritize them far below or after functional requirements is to tacitly approve building an inherently flawed and incomplete system. To meet the business ' current and long-term goals, the product owners, architects, business analysts, and technical leaders must take a holistic approach to defining new features and updates.  Over the long term, it 's more often than not that the non-functional capabilities of a system that determine its value and lifespan, and whether or not the company views it as a success, or just another sub-optimal delivery from IT.</p>",
    "tags": ["insurance", "requirements"],
    "nextMindShareId": "architecting-an-agile-enterprise",
    "nextMindShareTitle": "Architecting an Agile Enterprise"
  },
  {
    "id": "architecting-an-agile-enterprise",
    "title": "Architecting an Agile Enterprise",
    "authorName": "Samir Ahmed",
    "authorTitle": "Principal",
    "authorImageUrl": "samir-ahmed.jpg",
    "shortDescription": "In the insurance industry the need to innovate is dire. The common thread weaving through the industry is the role of technology; it is expected to lead the charge.",
    "industry": "insurance",
    "publishDate": "04/10/2017",
    "readTimeInMinutes": 7,
    "publishName": "Insurance Technology Association",
    "publishUrl": "http://buyersguide.itapro.org/Article?area=&articleID=289&channel=Solution%20Partner%20Views",
    "content": "<p>In the insurance industry, be it life, health or P&C, the need to innovate is dire. Disruption is the word of the hour. Discussions around innovations in core systems, business processes, omnichannel experience for producers and policyholders, and digitization (e.g. e-application, e-signature, e-delivery, etc.) abound.</p><p>The common thread weaving through all of these discussions is the role of technology: It is expected to lead the charge. And why shouldn't it? Technologies such as social networking, mobile computing, big data, neural networks, machine learning, and artificial intelligence, are all expected to disrupt the marketplace, and pay big dividends to those willing to make the bet. <i>Venture Scanner's Insurance Technology Market Overview and Innovation Quadrant for Q1 of 2017</i> shows them tracking 1,050 start-up companies in 14 categories across 54 countries with a combined $17 billion in funding.</p><p>For carriers, often viewed as evolving slower than a speeding microorganism, the challenge is figuring out how to participate in the market disruption that presumably awaits their fate. According to the venerable Harvard Business Review (HBR), the answer lies in being agile. In the May 2016 edition, in an article titled 'Embracing Agile,' the authors posit that the same agile innovation methods that have worked wonders for software development teams over the past 25 to 30 years are ripe for the picking as innovation methods of choice in a broad range of industries and business functions. In other words, agile approaches are no longer just for skunkworks projects; they are the way to innovate.</p><p>With HBR's recommendation understood, the challenge for carriers is figuring out how to architect their enterprises to be (more) agile. To frame this portion of the discussion in the proper context, let's define a few key terms.</p><p><ul><li>By enterprise, we mean the three-legged stool of people, process, and technology that supports any given business function, and more specifically in our current discussion, any given innovation project.</li><li>Next, by agile, we are not referring to any of the specific agile software development methodologies such as Extreme Programming (XP), Scrum, Kanban, and their ilk. Rather, we're referring to the underlying values that make a given approach or methodology agile. In a manner of speaking, we're discussing not just 'doing agile,' but also 'being agile.'</li><li>Finally, by architecture, we`re referring to the structure or structures of a system, which comprise elemental building blocks of the system, the externally visible properties of those building blocks, and the relationships among them. This is an adaptation and generalization of the definition of software architecture, as defined by Bass, Clements, and Kazman in the 2nd edition of Software Architecture in Practice, published by Addison-Wesley in 2003.</li></ul></p><p>Since we`re borrowing a methods leaf from the software development community, we should address an issue many agile software developers would take with mixing in architecture. Agile software developers have an aversion to heavy and seemingly endless documentation of a system, which, in traditional methods, would be the outcome of an activity they would call  'big design up front ' or BDUF.</p><p>Instead, they prefer an emergent design that`s the result of following the principles of  'keep[ing] it simple ' (KISS) and  'you aren`t going [to] need it ' (YAGNI), the latter a reference to attempts to build product features and capabilities that are not required currently, but are being anticipated as being required at some future point.</p><p>Such disdain for architecture by agile software developers is misplaced. The reality is that in viewing architecture as defined herein, we find that every system has an architecture; it's only a matter of whether that architecture is intentional or accidental. Even the 'emergent design' that agile software developers gravitate towards is the outcome of operating in some context that is pre-defined.</p><p>To borrow an analogy from construction, when building a house, one can certainly start out by building a single story structure, but if one anticipates needing to build a second story at any point, it had better be factored into the foundation. Not doing so will require tearing down the structure and rebuilding with a stronger foundation. This is an example of a feature that will not emerge. On the other hand, adding an additional room to the first story could certainly be an emergent feature, issues of physical space notwithstanding.</p><p>As to living the values of agility in order to be agile, in 2001, a group of individuals branding themselves as the Agile Alliance promulgated an Agile Manifesto as a way to codify the values of agility they adhered to. While these were in the context of software development, the premise of HBR's recommendation is that agile methods are broadly applicable, so it's worth discussing.</p><p>The manifesto states, '\u2026 we have come to value: individuals and interactions over processes and tools; working software over comprehensive documentation; customer collaboration over contract negotiation; [and] responding to change over following a plan. That is, while there is value in the items on the right, we value the items on the left more.' Let's imagine how these values might be seen in action on a project team.</p><p>In assembling a team, valuing individuals and interactions over processes and tools can manifest itself in several forms. First, team members should not be assigned to the project by managers. Instead, positions on the team should be filled much the same as any open position is filled in a company: through a process of applying for the position, interviewing candidates, and reaching a joint commitment to join the team between the project leads and the candidates. In doing so, the bar on required skills should be kept high, and not compromised due to the presence of other factors. The skills being sought should be a versatile set of skills, enabling team members to perform a variety of tasks. They should not be a narrow and specialized skill-set, constraining team members only to perform tasks within their specialized area of skill. The team should look more like a military special operations team, rather than a complete army.</p><p>Second, in valuing a working product more than a specification of it, we should measure our progress regularly and often. The only measure of progress should be demonstrable features and capabilities. Descriptions and status reports will not suffice. The purpose of this is to see the progress in action directly against the desired final outcome. Measuring frequently enables the team to detect issues early, allowing the team to address them sooner rather than later.</p><p>Third, valuing collaboration more than contract negotiation can have many manifestations as well. For instance, teams should be collocated, with access to spaces for face-to-face interactions and collaboration. Since most organizations have teams comprising individuals spread across different locations, this isn't always possible. In such cases, the team should invest in real-time collaboration tools such as web and video conferencing, shared editing and drafting tools, instant messaging, etc. Investing in such tools even for completely collocated teams has benefit. As basic as these might sound, many carriers have not yet made these investments.</p><p>Another manifestation would be to have the innovative project be a full-time focus, at least for key team members. Having competing commitments and differing availability between team members prevents spontaneous collaboration, requiring interactions to be formalized, which slows things down at best, and prevents progress at worst. Finally, the team ought to be empowered to make decisions on the spot during the collaborative interactions. When such empowerment is lacking, decision point has to be taken back to the true power of authority, thus undermining the effectiveness of the collaborative process.</p><p>Finally, in valuing responding to change more than following a plan, I'm reminded of a quote from President Eisenhower: 'In preparing for battle I have always found that plans are useless, but planning is indispensable.' A manifestation of this is engaging in the planning process regularly, and frequently. Such checkpoints allow the team to adapt based on feedback it`s receiving. They also create an opportunity for the team to benefit from new information that might have become available. Thus, at every checkpoint, the plan going forward can be refined.</p><p>The aforementioned are just a few practical examples from real-life experiences in executing agile projects that are manifestations of living the values of agility. While the practices are examples of 'doing agile' it's important to tie them back to values of 'being agile,' without which they end up being mindless habits whose benefit start diminishing over time. As these values spread to more areas of any organization, the enterprise will inevitably become more agile in the best possible sense.</p>",
    "tags": ["architecture", "agile"],
    "nextMindShareId": "life-insurance-applications-a-better-way",
    "nextMindShareTitle": "Life Insurance Applications: A Better Way"
  },
  {
    "id": "life-insurance-applications-a-better-way",
    "title": "Life Insurance Applications: A Better Way",
    "authorName": "Yunus Burhani",
    "authorTitle": "Senior Architect",
    "authorImageUrl": "yunus-burhani.jpg",
    "shortDescription": "Life insurers are racing to become more innovative as a way to improve overall customer experience; specifically through the initial application process.",
    "industry": "insurance",
    "publishDate": "05/11/2017",
    "readTimeInMinutes": 5,
    "publishName": "Insurance-Canada",
    "publishUrl": "https://archive.insurance-canada.ca/poladmin/canada/2017/xby2-better-life-applications-1705.php",
    "content": "<p>Like the rest of the insurance industry, life insurers are racing to become more innovative as a way to improve the overall customer experience. While it's fair to say that life insurers are lagging behind the health and property and casualty verticals when it comes to innovation and customer engagement, there still are inroads being made. That said, there's room for a lot more improvement. One example of this, and one that touches a potential customer directly, is the initial application process for life insurance.</p><p>Over the years life insurers have tried many approaches to enhance the buying experience for customers, but with little success. However, life insurance is still unique enough in terms of underwriting information that it requires potential customers to engage with an agent in order to find the right product for them. For better or worse, it is not as easy as buying auto insurance online - a process that is for the most part automated across the personal lines space and is widely accepted by consumers as the process for acquiring that insurance. However, a typical life insurance company sells a number of complex products, each with its own pros and cons that need to be carefully evaluated on an individual customer basis. In many respects it is more akin to investing money in financial sector products, which often requires engaging a broker, a certified financial planner, or some other financial expert.</p><p>That's why the life insurance application process - and transitioning that to the underwriting process - continues to be an issue for most life insurance companies. Some of the issues that typically arise during the process are:</p><p><ul><li>The application is not fully filled in by an agent.</li><li>The insurer's web application is too strictly edited for required information so agents often try to avoid filling them out online and submits paper application.</li><li>Not all the necessary underwriting requirements could be fulfilled because some application forms are not completely filled out.</li><li>The scheduling of evidence vendors are not set at the time of the application, so it becomes difficult to reach out to a potential customer afterward.</li><li>Applications are not signed electronically so insurers and consumers have to wait for wet signatures to occur.</li></ul></p><p>These seem like relatively simple problems to overcome, but despite the perception they persist in the life insurance industry. So in a world growing increasingly accustomed to instant information and response gratification, life insurers run the risk of being perceived as backwards from a customer service and technology perspective, and with some justification.</p><p>For example, while the quoting process for life insurance is pretty straightforward and does not require much data entry from a potential customer, once the potential customer agrees to the quote, the application process typically takes a long time and requires an extensive amount of data entry. Depending on the number of potentially insured's seeking coverage, the list of questions could easily go for an hour or two. This requires the customer(s) to take time out of their busy schedules in order to visit an agent's office, often for an extended period of time. Further, some of the questions on the application require the agent to physically see potential insured's, thus eliminating the option to have the conversation over the phone. A further complication can occur when some of the questions asked are HIPAA sensitive in nature, sometimes leading to awkward situations for potential insured's when they are required to answer such questions in front of others.</p><p>Given all of that, the question is what can be done about this cumbersome application process? One possible approach would be a hybrid one that would allow the agent and the potential customer to each complete a part of the application. In this approach, an agent can initiate the quoting process over the phone, since it does not require lot of data to be entered. Once the customer agrees with the quote the agent can schedule a thirty-minute office interview with the customer to narrow down the possibility of life, health and annuity insurance products and better explain the best options for customer. During that same visit agent would initiate the application process electronically, and once they completed their portion send it electronically to the customer to complete the rest.</p><p>As part of this self-serve approach, an evidence vendor scheduling system can be integrated allowing customers to schedule their exams appointments online. In the case where the insurer and/or local regulation requires the agent to be present when the customer signs, this suggested approach still makes it easier as the agent does not have to spend the time asking all the involved parties medical questions individually. The parties could be given tablets or use their cell phone to answer those questions in a lobby just like what occurs in a doctors office today. Once the signature is executed - either in person or electronically - the executed application can be forwarded to the insurer to initiate the underwriting process.</p><p>Such an approach would significantly streamline the application and underwriting processes for the life industry by providing customers a convenient way to apply for a policy without having to sit in front of an agent for a long time answering questions about their medical and financial histories. This application process also has the potential to speed up the new business and underwriting process for insurers, since they do not have to deal with incomplete applications, or applications coming in without a signature. They would also know that all the underwriting requirements have been ordered during the application process, and as a result would not have to bother with calling customers to schedule an appointment, or sending application information to evidence vendors so they can call the potential customers. Finally, since this streamlined application process does not allow agents to fill in all of the customer information (almost always erroneously), it will reduce the amount of paper applications received by insurers greatly, which will eliminate the burden on the New Business department to key in paper applications before any underwriting starts.</p><p>There's an old saying that first impressions - good or bad ones - tend to stick with people. In today's rapidly changing insurance environment, life insurers should be motivated to put their best customer service foot forward by making it as easy and simple as possible for potential customers to navigate the application and underwriting process. Any life insurer who fails to do so could find themselves losing potential new customers to their competitors.</p>",
    "tags": ["application", "customer experience"],
    "nextMindShareId": "the-bigger-they-are-the-tougher-the-talent",
    "nextMindShareTitle": "The Bigger They Are, The Tougher The Talent"
  },
  {
    "id": "the-bigger-they-are-the-tougher-the-talent",
    "title": "The Bigger They Are, The Tougher The Talent",
    "authorName": "David Packer",
    "authorTitle": "Principal",
    "authorImageUrl": "david-packer.jpg",
    "shortDescription": "Finding and keeping talented people for critical business technology programs and projects remains a difficult challenge across the insurance industry.",
    "industry": "insurance",
    "publishDate": "06/01/2017",
    "readTimeInMinutes": 7,
    "publishName": "HIT Leaders & News",
    "publishUrl": "https://us.hitleaders.news/the-bigger-they-are-the-tougher-the-talent/",
    "content": "<p>Finding and keeping talented people for critical business technology programs and projects remains a difficult challenge across the insurance industry. When it comes to large, transformational type efforts, the talent problem gets even more difficult.</p><p>The projects can be multiyear, the risk to the organization can be high, the technology and process issues and problems can be daunting, and it can be difficult to keep talented people engaged on longer projects when they are in demand for other projects in other parts of the organization.</p><p>That said, these large, transformational projects are now more common in the industry as insurers continue their efforts to modernize and innovate. Unfortunately, the effort required for finding the right talent is compromised in the name of quickly staffing these large projects. The compromised strategy often involves blindly staffing from vendor  'strategic partners, ' professional services firms and staff augmentation firms, often leading to increased cost and poor quality solutions.</p><p>So is there a better way to go about staffing large projects with the right talent?</p><p>In fact, there is a better approach, and it is based on a set of general principles that organizations often shortcut when staffing up. These principles are:</p><p><ul><li><b>Quality over quantity.</b> The caliber of the people involved is the single most critical element to project success. Be patient and take the time needed to staff based on the quality of the people versus meeting arbitrary staffing goals that lead to filling the team quickly and compromising on quality.</li><li><b>Project success trumps team member development.</b> The health and welfare of the project should not be compromised in the interest of giving less qualified employees  'some experience doing this kind of stuff. ' Large, high-risk projects cannot afford to be training grounds.</li><li><b>Recruiting and staffing is a continuous effort, not one and done.</b> Attrition, role rotation and performance issues will ensure that recruiting and staffing will be an ongoing process, so just plan for that eventuality up front. It makes for a healthier team environment anyway.</li><li><b>Recruiting well takes time and resources; don 't underestimate it.</b> For example, let 's assume that it might take two interviews to qualify every candidate and that for every five candidates there is one candidate that is selected. If the project team needs 50 people, that equates to 500 interviews to staff a 50-person team, not to mention the effort that goes into the other essential aspects of recruiting, such as candidate screening, coordinating scheduling, interviewing and decision making. The overall project plan should account for a full-time resource to handle this process, and for several other team leads spending 20-30 percent of their allocated time on the interviewing process, at least at the outset of the project.</li><li><b>Have an onboarding plan.</b> It is important to recognize onboarding as an essential part of the recruiting and staffing effort, and not an afterthought. Proper onboarding takes time and effort, but it goes a long way in increasing the overall effectiveness of individuals on the project and of the project team overall.</li></ul></p><p>So let 's break this down a bit. Just as it is with the technology and process portions of the project, recruiting for the overall project should have its plan, complete with ownership and accountability, for its success. The recruiting plan should consider the resources, time, effort and expense needed to staff the project initially and to rotate and replace team members continuously.</p><p>A good place to start is at the top - start with the project leadership team before thinking about appropriate technical, project management or process skills. It 's critical that team leads with strong skill sets are in place before the rest of the team is recruited, as the team leads will be vital to the rest of the recruiting process.</p><p>Next, create actual job descriptions based on the project 's needs. The descriptions should clearly define the skills and capabilities required for each role.</p><p>Beyond the technical, it is imperative to consider the softer skills of candidates - communication, leadership, working with others, etc. While there is a need and place for specialist skill sets for particular focus areas, on balance it is a good practice to seek out more flexible people who are well-rounded and can adapt and grow as the project evolves.</p><p>Once the team leads are in place, and the descriptions are created, it is time to create the rest of the recruiting team. The recruiting team will be responsible for the day-to-day screening, interviewing and placement for the rest of the project team.</p><p>For a 50-person project team, an initial recruiting team of one or two people working full time on this task should suffice. However, this is a long-term task, so the recruiting team will persist throughout the lifecycle of the project. Plan accordingly.</p><p>As in most worthwhile pursuits, time is the key element. Finding the right talent and resources takes time, so take that into account when planning.</p><p>In the example previously given, staffing a team of 50 people can take a lot of time when being picky about who is selected. It will likely take hundreds, if not thousands, of person hours to find the right 50-75 (alternates for when attrition begins) for the project team.</p><p>The same is true for the on-boarding process. With a larger project team, it is often more efficient and effective to onboard groups rather than individuals. A good approach is to set a regularly scheduled on-boarding day on a monthly basis for incoming team members. That session should include a general orientation of the project, processes and the overall team, and include any necessary training.</p><p>Training should also include specific tools the team is using for collaboration and communication. And as talented as people may be, it will also take them time to ramp up to their specific roles in the project, so that needs to be built into the overall onboarding timeline.</p><p>Additionally, finding the optimal mix of in-house and outside talent for any large project is often a delicate dance. Team chemistry versus overall team talent and competency should be given some consideration.</p><p>However, training and  'growing ' in-house resources should not come at the cost of the project. Similarly, satisfying contractual obligations with staff augmentation and consulting firms should not sway team member selection. Rather, the focus needs to stay on those resources that bring the most talent to the table and whose presence will improve the chances for project success the most.</p><p>In this case, one great developer is more valuable than 10 average developers. And if organizational politics begin to get in the way of assembling the very best team for the effort, be sure to tie project responsibilities and accountabilities to those making the political waves - that usually calms the waters.</p><p>The end goal in all of this is to put the best team together for that large project. Doing so immediately reduces the project and the organizational risks and increases the likelihood of project success, while allowing talented people to do what they do best.</p>",
    "tags": ["talent", "recruiting"],
    "nextMindShareId": "healthcare-data-sharing-is-only-as-good-as-your-data-standards",
    "nextMindShareTitle": "Healthcare Data Sharing is Only as Good as Your Data Standards"
  },
  {
    "id": "healthcare-data-sharing-is-only-as-good-as-your-data-standards",
    "title": "Healthcare Data Sharing is Only as Good as Your Data Standards",
    "authorName": "Saifuddin Bhagat",
    "authorTitle": "Senior Consultant",
    "authorImageUrl": "saif-bhagat.jpg",
    "shortDescription": "In this era of insurance industry innovation and rapid agile software development, many people blanch at the constriction that standards often represent.",
    "industry": "healthcare",
    "publishDate": "06/08/2017",
    "readTimeInMinutes": 7,
    "publishName": "Becker's Health IT & CIO Review",
    "publishUrl": "https://www.beckershospitalreview.com/healthcare-information-technology/healthcare-data-sharing-is-only-as-good-as-your-data-standards.html",
    "content": "<p>Standards are a funny thing. In this era of insurance industry innovation, Insurtech startups, and rapid agile software development and distribution, many people blanch at the constriction that standards often represent.</p><p>While there is a well-established standards approach for medical treatment coding, healthcare data standards are noticeably lacking. That is important because standards are the essential ingredient for optimizing advanced analytics and the information value derived from a well-designed standards structure.</p><p>The healthcare insurance vertical, like most other verticals in the insurance industry, has struggled with pulling together the vast treasure troves of provider and patient information in their many and sundry data stores. Many reasons for this are familiar to all: older systems with poor data editing capabilities; proprietary data stores that require specific data formats but are incompatible with other systems; and operational data stores that create data quality degradation to name a few. However, many of these problems have a root cause based on the fact that most healthcare organizations do not have strong data standards as a part of their overall business and technology ecosystem.</p><p>An example of this issue using a couple of pretty common healthcare data elements should prove illustrative: the difference between insurance eligibility and attribution enrollment. With a solid set of data standards in place, the first order of business is defining the data elements once and only once for use by all interested systems:</p><p><ul><li>Eligibility is health insurance coverage that one has for a certain or fixed period</li><li>Enrollment is signing up for coverage or program for a certain or fixed period, or derivation of a relationship for a certain or fixed period. A good way to think about enrollment is as a relationship, much like a marriage or friendship as it exists over some period.</li></ul></p><p>While it might seem mundane, these standards-driven definition distinctions are important for creating analytical insights and actionable value from this data. Because one of the foundations of analytical insight is data sharing across systems and platforms, it is critical to know when a particular data element was created and when the data contained therein was last updated. The same holds true for any subsequent subsets of data. This means that there may be a number of  'date ' elements that are defined in a particular way. For instance, a  'date ' definition for when a file was created, versus a  'date ' definition for when that file is updated, and a subsequent file is created. While this sounds elementary, more often than these initial data standard markers are not present in healthcare technology systems. Here are a couple of examples why that is a problem:</p><p><ul><li>Eligibility data needs to be shared when building an analytical claims population file that includes the subset of the claims population a given set of claims represents, and if the date spans are based on either service dates as opposed to paid dates. That is important as there is often a lag between when a patient is seen by a doctor, and when that service is billed.</li><li>For enrollment, an attribution enrollment analytical file needs first to include the date described time span of the data necessary to derive the attribution and for what period the attribution is valid. An example of this is a patient seeing a doctor for the first time then making an attribution of the patient-doctor relationship valid for some period that includes the past, present, and future.</li></ul></p><p>In both cases, a standards-based definition of the various date elements required is crucial for analytical data quality and insight. So how does one define data standards that optimize healthcare data sharing?</p><p>To start, it is important to fix any communication problems among the various parties involved. For instance, providers usually hire an external vendor or buy a COTS product for analytical needs. However, the claims data needed for provider data analysis is usually provided by the payers. In this scenario, there are often two contracts - one between the payer and the provider for the claim, and the other between the provider and the analytical service vendor for the reports. More often than not, the two contracts have different communication protocols, different points of contact, and different delivery timelines. As a result, the provider often plays the role of the middleman, exchanging questions and answers between the payer and the analytics vendor. Many times the communication happens during the beginning of the contract but then fades into an ad hoc basis only. Adding to the problem, there is often no service level agreement in place for questions and verifications between the parties.</p><img src='./assets/mind-share-img/Saif_Picture_1.jpg' /><p>To correct this common situation and make sure all of the parties are working synchronously, there needs to be some upfront collaboration between all the potential stakeholders before the creation and execution of the various contracts. The purpose of this collaboration is to ensure that each party understands the business goals and the analytical needs of the provider - both tactical and strategic. This collaboration will help the team understand what data is available; what will be provided to whom; and what will be required and expected to meet the data goals. This collaboration should then continue at regular intervals during the entire course of the contracts. This will provide a platform to ask questions and inform the group of any immediate or future changes to the data or process. There should be a medium set (i.e. Jira, MS Excel, or some other collaboration toolset) for asking questions and sharing relevant data.</p><img src='./assets/mind-share-img/Saif_Picture_2.jpg' /><p>Once these high-level agreements are in place, it is time to get into the details of what will allow the parties involved to work in an efficient and effective manner on behalf of the provider. For example, all medical claims cannot be provided in one file. There is a clear distinguishing feature between facility, professional and Rx claims. These claims should be provided in separate files, but if they are provided in one file, there should be a clear indication of that. Also, since claims files are usually created on a monthly basis from the source, the data is often extracted based on service date. This results in an incomplete file because not all claims in that service month might have made it into the system. Hence, the easiest and safest choice is to extract data based on paid dates. This will always be a complete set with no duplication from month to month.</p><p>Furthermore, a primary file should always have a supporting file (e.g., Provider_Rx_022016.txt should also have Provider_Rx_022016_support.txt). The support file has information such as when the file was created and its corresponding eligibility file. Similarly, an attribution file will have look-back dates, validity date, etc. Another possible option would be that all files come bundled in a zip format with a summary file. Of course, it is understandable that for privacy purposes claims related to STD, mental illness, or drug abuse are not shared. However, not sharing such data might lead to incorrect analytical output. A solution for this is to effectively mask the member information and provide the rest so that it can be included in the analytics. Finally, it is important to create a mandatory and industry specific list of columns populated with relevant data before sharing. A good example of this is that a claims file should always contain the service date, paid date, member id, POS, ICD code, servicing provider, etc. An eligibility file should contain an effective date, end date, member id, subscriber id, coverage info, etc.</p><p>All of the above can be thought of as part of a data standards approach that will lead to efficient and effective data sharing. Once these elements and standards are in place, providers should be able to create insightful and actionable data analytics that provide more effective patient care and more efficient financial management.</p>",
    "tags": ["data", "standards"],
    "nextMindShareId": "why-clinical-document-architecture-doesnt-solve-data-quality-issues",
    "nextMindShareTitle": "Why Clinical Document Architecture Doesn't Solve Data Quality Issues"
  }
]
